{
  "hash": "37e64b860ff33f914292fb742af578dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Anomaly Detection in Network Traffic Using NSL-KDD Dataset\"\nsubtitle: \"Proposal\"\nauthor: \n  - name: \"Team 7 - Mehran Tajbakhsh\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"  This project investigates the detection of anomalous network behavior using both statistical and machine learning techniques.<br>\nThe study leverages the **NSL-KDD dataset**, a refined version of the **KDD Cup 1999** intrusion detection benchmark, to evaluate\ndifferent approaches for distinguishing between normal and malicious network traffic. <br>\n<br>\nThe project compares two methodological categories:<br>\n\n- **Unsupervised anomaly detection**, where models learn only from normal traffic and must identify outliers without attack labels.<br>\n- **Supervised intrusion detection**, where labeled attack data are used to train classifiers to recognize known attack patterns.<br>\n<br>\n       \nThe actual NSL-KDD dataset is fairly large, containing nearly **4 million connection records**.<br>\nTo make the analysis computationally feasible while preserving the dataset’s diversity, this project will use a **randomly selected 10% subset** of the full data. This reduced sample maintains representative proportions of normal and attack classes, ensuring statistical validity while allowing for more efficient experimentation and model tuning. <br>\n<br>\nThe project will also address class imbalance through resampling and weighting techniques, apply standardized preprocessing (encoding, scaling, and feature selection), and evaluate each model’s performance using threshold-based anomaly scoring and classification metrics.<br>  \n  The ultimate goal is to develop a **robust, interpretable intrusion detection framework** suitable for **enterprise and IoT network environments**.\n\"\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\ncode-annotations: hover\nexecute:\n  warning: false\njupyter: python3\n---\n\n\n\n## Dataset\n\n\n\nThe NSL-KDD dataset is an enhanced version of the KDD Cup 1999 intrusion detection benchmark. <bt>\nIt removes redundant records and provides a more balanced sample distribution for fair model evaluation. <br>\nEach record represents a network connection with 41 features describing various characteristics such as protocol type, service, traffic flags, and packet statistics.<br>\nThe target label identifies whether a connection is normal or belongs to one of four attack families: <br>\n- **DoS (Denial of Service):** Overwhelms system resources (e.g., neptune, smurf, teardrop) <br>\n- **Probe:** Scans for vulnerabilities or open ports (e.g., satan, nmap, ipsweep) <br>\n- **R2L (Remote to Local):** Attempts to gain access remotely (e.g., guess_passwd, imap) <br>\n- **U2R (User to Root):** Escalates privileges locally (e.g., rootkit, buffer_overflow) <br>\n\n::: {#dataset-summary .cell message='false' execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nIn the context of intrusion detection, the dataset encompasses four main families of \nattack behaviors: Denial of Service (DoS), Probe, Remote to Local (R2L),\nand User to Root (U2R): \n\n- normal: 19.69% (n = 97278)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n```\n:::\n:::\n\n\n**Why this dataset?** <br> It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders).\n\n## Methodology Overview\n\nThis project will implement two complementary detection approaches to compare how well statistical and learning-based methods can detect network intrusions. <br>\n\n| **Approach Type** | **Algorithms** | **Training Data** | **Evaluation Strategy** |\n|--------------------|----------------|-------------------|--------------------------|\n| **Unsupervised Anomaly Detection** | Z-Score, Elliptic Envelope, Local Outlier Factor (LOF), DBSCAN | Trained only on **normal traffic** (no attack labels) | Evaluate anomaly scores using percentile-based thresholding (e.g., top 5% as anomalies); compute precision, recall, and false positive rate against true labels |\n| **Supervised / Semi-Supervised Intrusion Detection** | Random Forest, Support Vector Machine (SVM), Isolation Forest, Deep Autoencoder | Trained on **labeled normal and attack traffic** | Evaluate using stratified 80/20 train-test split with **5-fold cross-validation**; compare Precision, Recall, F1-score, and ROC-AUC |\n\n:::{.center}\n*Table 1. Comparison of Supervised vs. Unsupervised Detection Approaches.*\n:::\n\n## Threshold Definition for Unsupervised Models:\nFor models like LOF and DBSCAN, anomaly scores will be transformed into binary outcomes using a threshold determined by the 95th percentile of the score distribution on training data. Models will be compared based on precision, recall, and false positive rate against true attack labels.\n\n## Data Preprocessing and Feature Handling\n\n- **Feature Encoding:** Categorical features (`protocol_type`, `service`, and `flag`) will be transformed using **One-Hot Encoding** to convert them into numerical representations suitable for machine learning models.<br>\n\n- **Scaling:** All numeric features will be standardized using **StandardScaler** to normalize feature magnitudes and support distance-based models such as SVM, LOF, and DBSCAN.<br>\n\n- **Dimensionality Reduction:** **Principal Component Analysis (PCA)** will be applied to reduce feature dimensionality, minimize noise, and visualize decision boundaries in lower dimensions.<br>\n\n- **Latent-Space Visualization:** Beyond PCA, t-SNE and UMAP will be applied to project high-dimensional data into 2D latent spaces. These visualizations will reveal natural clustering between normal and attack traffic, and illustrate how different models separate anomalous connections.<br>\n\n- **Feature Selection:** Correlation filtering and low-variance thresholding will be used to eliminate redundant and non-informative predictors.<br>\n\n- **Train/Test Split:** A **stratified 80/20 split** will ensure consistent class ratios between training and testing datasets, preserving representation of all attack families.<br>\n\n- **Cross-Validation:** For supervised learning models, **5-fold cross-validation** will be applied to ensure stable and generalizable results.<br>\n\n- **Class Imbalance Handling:** Severe imbalance in **R2L** and **U2R** attack classes will be mitigated using **SMOTE (Synthetic Minority Oversampling Technique)** and **class-weight adjustments** during model training.<br>\n\n## Model Interpretability\n\nTo ensure transparency and explainability in the results, this project will include interpretability analysis for both supervised and unsupervised models:<br>\n\n- **Tree-Based Models (Random Forest):** Feature importance plots will be used to identify the most influential network attributes contributing to intrusion detection decisions.<br>\n\n- **SVM and Deep Autoencoders:** The **SHAP (SHapley Additive exPlanations)** framework will be applied to quantify how each input feature affects individual model predictions, highlighting both global and local interpretability.<br>\n\n- **Unsupervised Models (LOF, DBSCAN, Elliptic Envelope):** Model behavior will be interpreted through visualization of **PCA-based decision boundaries**, **cluster separations**, and **anomaly score distributions**.<br>\n\n- **Autoencoder Reconstruction Analysis:** For deep learning models, reconstruction error plots will be examined to understand how effectively normal vs. anomalous traffic patterns are captured and separated.<br>\n\n- **Combined Insights:** Comparing interpretability outputs across models will help evaluate the trade-off between detection accuracy and explainability—an important factor in real-world IDS deployment.<br>\n\n- **Interpretability Trade-Offs:** Comparative analysis will examine the balance between model transparency and detection power. While deep or ensemble methods may offer superior accuracy, their complexity can obscure decision rationale. Simpler statistical models, though less accurate, provide clearer interpretive insights valuable for security analysts. This trade-off will be explicitly discussed in the results and conclusions.<br>\n\n## Evaluation Setup \n\nThe evaluation process will assess the performance, robustness, and reliability of all models through a consistent and structured framework:<br>\n\n- **Performance Metrics:** Model effectiveness will be quantified using **Precision**, **Recall**, **F1-Score**, **ROC-AUC**, and **Detection Rate** to capture both classification accuracy and anomaly detection strength.<br>\n\n- **Validation Strategy:** Results will be validated through a combination of **stratified 80/20 train–test split** and **5-fold cross-validation** to ensure generalization and mitigate overfitting.<br>\n\n- **Family-Wise and Rare Attack Evaluation:** Each attack family (DoS, Probe, R2L, and U2R) will be evaluated independently, with special attention to rare classes (R2L and U2R). Detection rates, false negatives, and confusion patterns for these low-frequency attacks will reveal how well unsupervised and supervised approaches generalize to scarce attack behaviors.<br>\n\n- **Threshold Optimization (Unsupervised Models):** Anomaly score thresholds (e.g., top 5% or 95th percentile) will be tuned on the training data to balance false positives and false negatives.<br>\n\n- **Baseline Comparison:** Model results will be benchmarked against **published NSL-KDD baselines** and recent IDS studies to contextualize performance improvements.<br>\n\n- **Visualization:** ROC curves, confusion matrices, and precision–recall plots will be used to visually compare detection quality and identify performance trade-offs among methods.<br>\n\n## Questions\n\n1. How effectively can unsupervised anomaly detection methods identify malicious network traffic when trained only on normal data? <br>\n\n2. How does the performance of supervised machine learning–based intrusion detection compare with statistical anomaly detection methods in terms of detection accuracy, precision, recall, and interpretability?\n\n## Real-World Application Context\n\nThis project models an **enterprise network intrusion detection system (IDS)**.<br>\nIn real-world settings, such systems continuously monitor inbound and outbound connections to detect unusual traffic patterns.\nFindings from this analysis can be directly extended to:<br>\n- **Enterprise network monitoring**<br>\n- **Cloud service intrusion prevention**<br>\n- **IoT network anomaly detection**<br>\n- **Streaming data analysis** using similar architectures (e.g., online learning or real-time inference)<br>\n\n## Dataset Limitations and Justification\n\nAlthough the **NSL-KDD dataset** is dated and does not reflect modern encrypted or high-volume network traffic, it remains widely used in academic research because:<br>\n\n- It contains **well-structured, labeled attack categories**, making it ideal for benchmarking and comparing different algorithms.<br>\n\n- It enables **controlled, reproducible comparisons** between models without introducing data privacy or confidentiality issues.<br>\n\n- It provides **clear ground truth labels**, which are often unavailable in modern or proprietary network traffic datasets.<br>\n\nThe goal is to develop and compare generalizable anomaly detection methods, which can later be adapted to more recent or proprietary datasets.<br>\n\n## Reproducibility and Version Control\n\nTo ensure that all results are consistent and reproducible across runs and environments, this project follows strict versioning and organization practices:<br>\n\n- **Random Seeds:** All random processes (e.g., data splits, model initialization, SMOTE) will use **fixed random seeds** to ensure reproducibility.<br>\n\n- **Repository Organization:** The project repository will follow a clear, modular directory structure to separate data, notebooks, and sources.<br>\n\n```text\nproject-root/\n│\n├── data/                # Raw and processed datasets\n├── notebooks/           # Jupyter notebooks for analysis and modeling\n├── src/                 # Preprocessing and utility code\n├── requirements.txt     # Python dependencies for environment setup\n└── README.md            # Project overview and usage instructions\n```\n- Experiment tracking will be implemented using **versioned notebooks and GitHub commits**, ensuring reproducibility.<br>\n- Dependencies and environment configuration will be captured in a requirements.txt file.<br>\n\n- All visualizations, experiment logs, and performance metrics will be versioned and stored in the repository for transparent result verification.<br>\n\n## Analysis plan\n\n**Week 1 –** Problem Definition & Data Preparation\nThe first week focuses on defining the objectives of anomaly detection within the NSL-KDD dataset. The dataset will be loaded, inspected, and preprocessed to ensure analytical readiness. Tasks include handling missing values, encoding categorical features, and scaling numerical variables.\nDeliverables: Data preparation notebook and summary report.\n\n**Week 2 –** Exploratory Data Analysis (EDA)\nThis phase aims to understand data distributions, feature correlations, and patterns of normal versus attack traffic. Visualization techniques will be applied to highlight key discriminative features and potential sources of anomaly.\nDeliverables: EDA notebook with visual insights and descriptive summaries.\n\n**Week 3 –** Statistical Anomaly Detection Methods\nIn this stage, classical statistical methods for anomaly detection will be implemented, including Z-Score, Elliptical Envelope, Local Outlier Factor (LOF), and DBSCAN. Their performance in identifying abnormal network behaviors will be analyzed and compared.\nDeliverables: Statistical anomaly detection notebook and comparative summary.\n\n**Week 4 –** Machine Learning–Based Methods\nModern learning-based detectors will be explored using Isolation Forest, One-Class SVM, and Deep Autoencoders. Models will be tuned for optimal performance, and detection boundaries will be visualized to interpret decision behavior.\nDeliverables: ML anomaly detection notebook and evaluation metrics.\n\n**Week 5 –** Model Evaluation and Comparison\nAll implemented methods will be assessed using standard performance metrics such as Precision, Recall, F1-score, ROC-AUC, and Detection Rate. Comparative analyses and ablation studies will be performed to evaluate robustness and generalization.\nDeliverables: Evaluation notebook and performance report.\n\n**Week 6 –** Final Analysis & Reporting\nThe final week synthesizes results into a comprehensive assessment of the effectiveness of different anomaly detection techniques. The findings will discuss practical implications, advantages, and limitations of each approach in real-world intrusion detection scenarios.\nAdditionally, a **reflection phase** will visualize anomaly scores and latent embeddings (via PCA, t-SNE, UMAP) to assess separability between normal and attack samples. The final report will include a **comparative interpretability reflection**, discussing how the transparency–accuracy balance affects IDS deployment decisions.<br>\nDeliverables: Final project report and presentation slides.\n\n",
    "supporting": [
      "proposal_files"
    ],
    "filters": [],
    "includes": {}
  }
}