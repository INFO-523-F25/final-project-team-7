{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999c1c1f",
   "metadata": {},
   "source": [
    "# Week 05 — Model Evaluation & Comparison\n",
    "\n",
    "This notebook compares all models from Weeks 03–04 on the NSL-KDD dataset test split. It focuses on:\n",
    "\n",
    "- Metric table (Precision, Recall, F1, ROC-AUC, PR-AUC)\n",
    "- Family-wise deep-dive (especially **R2L** and **U2R**)\n",
    "- Robustness checks (thresholds & seeds)\n",
    "- Calibration of supervised models\n",
    "- Runtime / complexity\n",
    "- Final ranked shortlist with trade-offs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919321f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, project root, and paths.\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*covariance matrix associated to your dataset is not full rank.*\"\n",
    ")\n",
    "\n",
    "import os, sys, time\n",
    "from pathlib import Path\n",
    "\n",
    "this_dir = Path.cwd()\n",
    "project_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Sklearn metrics & helpers\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    RocCurveDisplay,\n",
    "    PrecisionRecallDisplay,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Project utilities (follow the same pattern as previous weeks)\n",
    "from src.utils import Paths, set_global_seed\n",
    "\n",
    "# Optional: custom helpers if they exist in your repo.\n",
    "try:\n",
    "    from src.eval import to_binary  # used in Week 04\n",
    "except ImportError:\n",
    "    to_binary = None\n",
    "\n",
    "set_global_seed(42)\n",
    "paths = Paths().ensure()\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"Using paths:\", paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed train/test arrays from Week 01.\n",
    "X_train_path = paths.data_proc / \"X_train.npy\"\n",
    "X_test_path  = paths.data_proc / \"X_test.npy\"\n",
    "y_train_path = paths.data_proc / \"y_train.npy\"\n",
    "y_test_path  = paths.data_proc / \"y_test.npy\"\n",
    "\n",
    "X_train = np.load(X_train_path)\n",
    "X_test  = np.load(X_test_path)\n",
    "y_train = np.load(y_train_path)\n",
    "y_test  = np.load(y_test_path)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "print(\"y_train classes:\", Counter(y_train))\n",
    "print(\"y_test  classes:\", Counter(y_test))\n",
    "\n",
    "# Derive a binary label vector (normal vs attack) if a helper exists.\n",
    "# This is mainly for unsupervised anomaly detectors.\n",
    "if to_binary is not None:\n",
    "    # Adjust `normal_label` to match your encoding, e.g. 'normal' or 'Normal'.\n",
    "    normal_label = \"normal\"\n",
    "    y_train_bin = to_binary(y_train, normal_label)\n",
    "    y_test_bin  = to_binary(y_test, normal_label)\n",
    "    print(\"Binary train counts:\", Counter(y_train_bin))\n",
    "    print(\"Binary test  counts:\", Counter(y_test_bin))\n",
    "else:\n",
    "    y_train_bin = None\n",
    "    y_test_bin = None\n",
    "    print(\"NOTE: src.eval.to_binary not found – binary labels not created automatically.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d27ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for unified metric computation and plotting.\n",
    "\n",
    "def multiclass_metrics(y_true, y_pred, y_proba=None, labels=None, average=\"macro\"):\n",
    "    \"\"\"Compute macro metrics + per-class metrics for a multiclass classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true, y_pred : array-like\n",
    "        True and predicted labels (families).\n",
    "    y_proba : array-like of shape (n_samples, n_classes), optional\n",
    "        Predicted probabilities for each class (for ROC/PR).\n",
    "    labels : list, optional\n",
    "        Label order. If None, inferred from y_true.\n",
    "    average : str\n",
    "        Averaging scheme for global metrics.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_true)\n",
    "    \n",
    "    prec, rec, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=None, zero_division=0\n",
    "    )\n",
    "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=average, zero_division=0\n",
    "    )\n",
    "    \n",
    "    per_class_df = pd.DataFrame({\n",
    "        \"family\": labels,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"support\": support,\n",
    "    })\n",
    "    \n",
    "    metrics = {\n",
    "        \"precision_macro\": macro_prec,\n",
    "        \"recall_macro\": macro_rec,\n",
    "        \"f1_macro\": macro_f1,\n",
    "    }\n",
    "    \n",
    "    # Multi-class ROC-AUC (OvR) and PR-AUC if probabilities are available.\n",
    "    if y_proba is not None:\n",
    "        # Binarize labels for OvR using a simple label -> index mapping.\n",
    "        label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
    "        y_true_idx = np.array([label_to_idx[lab] for lab in y_true])\n",
    "        Y_true_ovr = np.eye(len(labels))[y_true_idx]\n",
    "        \n",
    "        try:\n",
    "            roc_macro_ovr = roc_auc_score(Y_true_ovr, y_proba, average=\"macro\", multi_class=\"ovr\")\n",
    "        except Exception:\n",
    "            roc_macro_ovr = np.nan\n",
    "        \n",
    "        try:\n",
    "            pr_macro_ovr = average_precision_score(Y_true_ovr, y_proba, average=\"macro\")\n",
    "        except Exception:\n",
    "            pr_macro_ovr = np.nan\n",
    "        \n",
    "        metrics[\"roc_auc_ovr_macro\"] = roc_macro_ovr\n",
    "        metrics[\"pr_auc_ovr_macro\"] = pr_macro_ovr\n",
    "    else:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = np.nan\n",
    "        metrics[\"pr_auc_ovr_macro\"] = np.nan\n",
    "    \n",
    "    return metrics, per_class_df\n",
    "\n",
    "\n",
    "def binary_threshold_metrics(y_true_bin, scores, thresholds):\n",
    "    \"\"\"Sweep thresholds over anomaly scores for binary detectors.\n",
    "    \n",
    "    Assumes higher `scores` = more anomalous (1 = attack, 0 = normal).\n",
    "    Returns a DataFrame with precision/recall/F1 for each threshold.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for thr in thresholds:\n",
    "        y_pred_bin = (scores >= thr).astype(int)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        rows.append({\n",
    "            \"threshold\": thr,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_confusion(cm, labels, title):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n",
    "                xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a790d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised models from Week 04.\n",
    "# \n",
    "# Strategy:\n",
    "# - Prefer factory functions from src.models (same as Week 04).\n",
    "# - If not available, fall back to simple sklearn baselines.\n",
    "#\n",
    "# NOTE: If you already saved trained models in `paths.models`, you can\n",
    "#       skip training here and just load them with joblib instead.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "try:\n",
    "    from src.models import (\n",
    "        make_rf_classifier_grid,\n",
    "        make_svm_rbf_grid,\n",
    "    )\n",
    "    HAVE_FACTORY = True\n",
    "except ImportError:\n",
    "    HAVE_FACTORY = False\n",
    "\n",
    "supervised_models = {}\n",
    "fit_times = {}\n",
    "predict_times = {}\n",
    "probas_cache = {}\n",
    "preds_cache = {}\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "\n",
    "if HAVE_FACTORY:\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    print(\"Using src.models factory functions + GridSearchCV (similar to Week 04).\")\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_spec = make_rf_classifier_grid()\n",
    "    rf_gs = GridSearchCV(\n",
    "        estimator=rf_spec.model,\n",
    "        param_grid=rf_spec.param_grid,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    rf_gs.fit(X_train, y_train)\n",
    "    fit_times[\"RandomForest (GS)\"] = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    y_pred_rf = rf_gs.predict(X_test)\n",
    "    predict_times[\"RandomForest (GS)\"] = time.perf_counter() - t0\n",
    "    y_proba_rf = rf_gs.predict_proba(X_test)\n",
    "    \n",
    "    supervised_models[\"RandomForest (GS)\"] = rf_gs.best_estimator_\n",
    "    preds_cache[\"RandomForest (GS)\"] = y_pred_rf\n",
    "    probas_cache[\"RandomForest (GS)\"] = y_proba_rf\n",
    "    \n",
    "    # SVM (RBF)\n",
    "    svm_spec = make_svm_rbf_grid()\n",
    "    svm_gs = GridSearchCV(\n",
    "        estimator=svm_spec.model,\n",
    "        param_grid=svm_spec.param_grid,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    svm_gs.fit(X_train, y_train)\n",
    "    fit_times[\"SVM RBF (GS)\"] = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    y_pred_svm = svm_gs.predict(X_test)\n",
    "    predict_times[\"SVM RBF (GS)\"] = time.perf_counter() - t0\n",
    "    \n",
    "    # Some SVMs may be configured without probability; enable if needed.\n",
    "    if hasattr(svm_gs.best_estimator_, \"predict_proba\"):\n",
    "        y_proba_svm = svm_gs.best_estimator_.predict_proba(X_test)\n",
    "    else:\n",
    "        y_proba_svm = None\n",
    "    \n",
    "    supervised_models[\"SVM RBF (GS)\"] = svm_gs.best_estimator_\n",
    "    preds_cache[\"SVM RBF (GS)\"] = y_pred_svm\n",
    "    probas_cache[\"SVM RBF (GS)\"] = y_proba_svm\n",
    "\n",
    "else:\n",
    "    print(\"src.models factories not found – using simple sklearn baselines.\")\n",
    "    \n",
    "    # Random Forest baseline\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    rf.fit(X_train, y_train)\n",
    "    fit_times[\"RandomForest\"] = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    predict_times[\"RandomForest\"] = time.perf_counter() - t0\n",
    "    y_proba_rf = rf.predict_proba(X_test)\n",
    "    \n",
    "    supervised_models[\"RandomForest\"] = rf\n",
    "    preds_cache[\"RandomForest\"] = y_pred_rf\n",
    "    probas_cache[\"RandomForest\"] = y_proba_rf\n",
    "    \n",
    "    # SVM RBF baseline\n",
    "    svm = SVC(\n",
    "        kernel=\"rbf\",\n",
    "        C=3.0,\n",
    "        gamma=\"scale\",\n",
    "        probability=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    svm.fit(X_train, y_train)\n",
    "    fit_times[\"SVM RBF\"] = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    predict_times[\"SVM RBF\"] = time.perf_counter() - t0\n",
    "    y_proba_svm = svm.predict_proba(X_test)\n",
    "    \n",
    "    supervised_models[\"SVM RBF\"] = svm\n",
    "    preds_cache[\"SVM RBF\"] = y_pred_svm\n",
    "    probas_cache[\"SVM RBF\"] = y_proba_svm\n",
    "    \n",
    "    # Multinomial Logistic Regression baseline\n",
    "    lr = LogisticRegression(\n",
    "        multi_class=\"multinomial\",\n",
    "        max_iter=500,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    lr.fit(X_train, y_train)\n",
    "    fit_times[\"Logistic Regression\"] = time.perf_counter() - t0\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "    predict_times[\"Logistic Regression\"] = time.perf_counter() - t0\n",
    "    y_proba_lr = lr.predict_proba(X_test)\n",
    "    \n",
    "    supervised_models[\"Logistic Regression\"] = lr\n",
    "    preds_cache[\"Logistic Regression\"] = y_pred_lr\n",
    "    probas_cache[\"Logistic Regression\"] = y_proba_lr\n",
    "\n",
    "print(\"Supervised models:\", list(supervised_models.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffffdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised anomaly detectors from Week 03.\n",
    "#\n",
    "# We assume Week 03 defined helpers such as:\n",
    "#   - run_zscore(df_bin, ...)\n",
    "#   - run_elliptic_envelope(df_bin, ...)\n",
    "#   - run_mahalanobis(df_bin, alpha=0.99, robust=True)\n",
    "#\n",
    "# that return a result object with attributes:\n",
    "#   - name\n",
    "#   - y_true (binary)\n",
    "#   - y_pred (binary)\n",
    "#   - scores (float anomaly scores, higher = more anomalous)\n",
    "#   - confusion (2x2 confusion matrix)\n",
    "#\n",
    "# If your interface is different, adjust this cell accordingly.\n",
    "\n",
    "unsup_results = {}\n",
    "\n",
    "try:\n",
    "    from src.unsupervised import run_zscore, run_elliptic_envelope, run_mahalanobis\n",
    "    HAVE_UNSUP = True\n",
    "except ImportError:\n",
    "    HAVE_UNSUP = False\n",
    "\n",
    "if HAVE_UNSUP and y_test_bin is not None:\n",
    "    # Rebuild a binary DataFrame similar to Week 03 if needed.\n",
    "    # Here we only have the processed X/y; adapt if Week 03 used raw features.\n",
    "    df_test_bin = pd.DataFrame(X_test)\n",
    "    df_test_bin[\"label_bin\"] = y_test_bin\n",
    "    \n",
    "    # Z-score\n",
    "    z_res = run_zscore(df_test_bin, contamination=0.1)\n",
    "    unsup_results[\"Z-score\"] = z_res\n",
    "    \n",
    "    # Elliptic Envelope\n",
    "    ell_res = run_elliptic_envelope(df_test_bin, contamination=0.1, random_state=0)\n",
    "    unsup_results[\"EllipticEnvelope\"] = ell_res\n",
    "    \n",
    "    # Mahalanobis (robust covariance + chi-square threshold)\n",
    "    mahal_res = run_mahalanobis(df_test_bin, alpha=0.99, robust=True)\n",
    "    unsup_results[\"Mahalanobis (robust, alpha=0.99)\"] = mahal_res\n",
    "    \n",
    "    print(\"Unsupervised results:\", list(unsup_results.keys()))\n",
    "else:\n",
    "    print(\"Unsupervised helpers not available or no binary labels – skipping Week 03 models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global metric table: Precision, Recall, F1 (macro), ROC-AUC (OvR), PR-AUC.\n",
    "rows = []\n",
    "\n",
    "# Supervised models (multiclass)\n",
    "labels = np.unique(y_test)\n",
    "\n",
    "for name, model in supervised_models.items():\n",
    "    y_pred = preds_cache[name]\n",
    "    y_proba = probas_cache.get(name, None)\n",
    "    metrics, _ = multiclass_metrics(y_test, y_pred, y_proba=y_proba, labels=labels)\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"type\": \"supervised\",\n",
    "        **metrics,\n",
    "    })\n",
    "\n",
    "# Unsupervised models (binary); treat `1` = anomaly/attack.\n",
    "if unsup_results and y_test_bin is not None:\n",
    "    for name, res in unsup_results.items():\n",
    "        y_true_b = res.y_true if hasattr(res, \"y_true\") else y_test_bin\n",
    "        y_pred_b = res.y_pred if hasattr(res, \"y_pred\") else None\n",
    "        scores   = res.scores if hasattr(res, \"scores\") else None\n",
    "        \n",
    "        if y_pred_b is None and scores is not None:\n",
    "            # Default threshold at 0 (or median score) if only scores exist.\n",
    "            thr = np.median(scores)\n",
    "            y_pred_b = (scores >= thr).astype(int)\n",
    "        \n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_b, y_pred_b, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        \n",
    "        # ROC-AUC / PR-AUC if scores exist.\n",
    "        if scores is not None:\n",
    "            try:\n",
    "                roc_auc = roc_auc_score(y_true_b, scores)\n",
    "            except Exception:\n",
    "                roc_auc = np.nan\n",
    "            try:\n",
    "                pr_auc = average_precision_score(y_true_b, scores)\n",
    "            except Exception:\n",
    "                pr_auc = np.nan\n",
    "        else:\n",
    "            roc_auc = np.nan\n",
    "            pr_auc = np.nan\n",
    "        \n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "            \"type\": \"unsupervised\",\n",
    "            \"precision_macro\": prec,\n",
    "            \"recall_macro\": rec,\n",
    "            \"f1_macro\": f1,\n",
    "            \"roc_auc_ovr_macro\": roc_auc,\n",
    "            \"pr_auc_ovr_macro\": pr_auc,\n",
    "        })\n",
    "\n",
    "metric_table = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "metric_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family-wise deep-dive for the best supervised model.\n",
    "#\n",
    "# Use the top model by macro F1 (among supervised models)\n",
    "# and inspect per-family performance and confusion matrix.\n",
    "\n",
    "sup_only = metric_table[metric_table[\"type\"] == \"supervised\"]\n",
    "best_name = sup_only.sort_values(\"f1_macro\", ascending=False)[\"model\"].iloc[0]\n",
    "print(\"Best supervised model by macro F1:\", best_name)\n",
    "\n",
    "best_pred = preds_cache[best_name]\n",
    "best_proba = probas_cache.get(best_name, None)\n",
    "\n",
    "labels = np.unique(y_test)\n",
    "metrics_best, per_class_best = multiclass_metrics(\n",
    "    y_test, best_pred, y_proba=best_proba, labels=labels\n",
    ")\n",
    "display(per_class_best.sort_values(\"f1\"))\n",
    "\n",
    "cm_best = confusion_matrix(y_test, best_pred, labels=labels)\n",
    "fig, ax = plot_confusion(cm_best, labels, f\"Confusion matrix – {best_name}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38880f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2L and U2R error analysis with example rows.\n",
    "#\n",
    "# This assumes your `y_test` uses family labels such as 'R2L' and 'U2R'.\n",
    "# Adjust the family names below if your encoding is different.\n",
    "\n",
    "r2l_label = \"R2L\"\n",
    "u2r_label = \"U2R\"\n",
    "\n",
    "mask_r2l = (y_test == r2l_label)\n",
    "mask_u2r = (y_test == u2r_label)\n",
    "\n",
    "print(\"R2L support:\", mask_r2l.sum())\n",
    "print(\"U2R support:\", mask_u2r.sum())\n",
    "\n",
    "# Indices where R2L / U2R were misclassified by the best model.\n",
    "mis_r2l_idx = np.where(mask_r2l & (best_pred != r2l_label))[0]\n",
    "mis_u2r_idx = np.where(mask_u2r & (best_pred != u2r_label))[0]\n",
    "\n",
    "print(\"Misclassified R2L:\", len(mis_r2l_idx))\n",
    "print(\"Misclassified U2R:\", len(mis_u2r_idx))\n",
    "\n",
    "# Reconstruct a feature DataFrame using the preprocessor if available,\n",
    "# otherwise fall back to a numeric DataFrame.\n",
    "try:\n",
    "    import joblib\n",
    "    pre_path = paths.data_proc / \"preprocessor.joblib\"\n",
    "    pre = joblib.load(pre_path)\n",
    "    feature_names = pre.get_feature_names_out()\n",
    "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "except Exception as e:\n",
    "    print(\"Could not load preprocessor or feature names:\", e)\n",
    "    X_test_df = pd.DataFrame(X_test)\n",
    "    X_test_df[\"family\"] = y_test\n",
    "\n",
    "# Show a few misclassified examples for each rare family.\n",
    "def show_examples(indices, family_name, n=5):\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No misclassifications found for {family_name}.\")\n",
    "        return\n",
    "    idx_sel = indices[:n]\n",
    "    ex = X_test_df.iloc[idx_sel].copy()\n",
    "    ex[\"y_true\"] = y_test[idx_sel]\n",
    "    ex[\"y_pred\"] = best_pred[idx_sel]\n",
    "    display(ex)\n",
    "\n",
    "print(\"\\nExample misclassified R2L samples:\")\n",
    "show_examples(mis_r2l_idx, \"R2L\")\n",
    "\n",
    "print(\"\\nExample misclassified U2R samples:\")\n",
    "show_examples(mis_u2r_idx, \"U2R\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dcb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness: sensitivity to threshold for unsupervised models.\n",
    "#\n",
    "# For each unsupervised detector with a `scores` attribute, sweep\n",
    "# thresholds and plot F1 vs threshold.\n",
    "\n",
    "if unsup_results and y_test_bin is not None:\n",
    "    thresholds = np.linspace(0.1, 0.9, 9)  # generic range; adjust to your score scale\n",
    "    \n",
    "    for name, res in unsup_results.items():\n",
    "        if not hasattr(res, \"scores\"):\n",
    "            print(f\"[{name}] no scores attribute – skipping threshold sweep.\")\n",
    "            continue\n",
    "        \n",
    "        scores = res.scores\n",
    "        # Normalize scores to [0, 1] for a more stable threshold range, if needed.\n",
    "        s_min, s_max = np.min(scores), np.max(scores)\n",
    "        if s_max > s_min:\n",
    "            scores_norm = (scores - s_min) / (s_max - s_min)\n",
    "        else:\n",
    "            scores_norm = scores\n",
    "        \n",
    "        df_thr = binary_threshold_metrics(res.y_true, scores_norm, thresholds)\n",
    "        display(df_thr)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(df_thr[\"threshold\"], df_thr[\"f1\"], marker=\"o\")\n",
    "        ax.set_xlabel(\"Threshold (on normalized score)\")\n",
    "        ax.set_ylabel(\"F1 (binary)\")\n",
    "        ax.set_title(f\"Threshold sensitivity – {name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No unsupervised results (or binary labels) available – skipping threshold robustness.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ad7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness: stability across random seeds for supervised models.\n",
    "#\n",
    "# We will re-train each supervised model on a subset of seeds and track\n",
    "# the variation in macro F1 on a validation fold (not the test set).\n",
    "\n",
    "seed_results = []\n",
    "\n",
    "# Simple single-fold split (train/val) using StratifiedKFold.\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = next(iter(skf.split(X_train, y_train)))\n",
    "X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "seed_list = [1, 2, 3, 4, 5]\n",
    "\n",
    "for base_name, model in supervised_models.items():\n",
    "    for seed in seed_list:\n",
    "        # Rebuild a fresh instance with the new seed when possible.\n",
    "        if hasattr(model, \"get_params\"):\n",
    "            params = model.get_params()\n",
    "            if \"random_state\" in params:\n",
    "                params[\"random_state\"] = seed\n",
    "            cls = model.__class__\n",
    "            model_seed = cls(**params)\n",
    "        else:\n",
    "            # Fallback: re-use the fitted model (not ideal, but avoids errors).\n",
    "            model_seed = model\n",
    "        \n",
    "        model_seed.fit(X_tr, y_tr)\n",
    "        y_val_pred = model_seed.predict(X_val)\n",
    "        f1_macro, _, _, _ = precision_recall_fscore_support(\n",
    "            y_val, y_val_pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        \n",
    "        seed_results.append({\n",
    "            \"model\": base_name,\n",
    "            \"seed\": seed,\n",
    "            \"f1_macro_val\": f1_macro,\n",
    "        })\n",
    "\n",
    "seed_df = pd.DataFrame(seed_results)\n",
    "seed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration: reliability curves for supervised probabilities.\n",
    "#\n",
    "# We will plot reliability curves for the top-k supervised models\n",
    "# that have `predict_proba`. For multi-class, we focus on the\n",
    "# \"attack vs normal\" binary view if `to_binary` exists; otherwise,\n",
    "# we plot per-class curves for the most frequent family.\n",
    "\n",
    "top_k = 2\n",
    "\n",
    "sup_sorted = sup_only.sort_values(\"f1_macro\", ascending=False)[\"model\"].tolist()\n",
    "top_models = [m for m in sup_sorted if hasattr(supervised_models[m], \"predict_proba\")][:top_k]\n",
    "\n",
    "if not top_models:\n",
    "    print(\"No probabilistic supervised models available for calibration.\")\n",
    "else:\n",
    "    for name in top_models:\n",
    "        model = supervised_models[name]\n",
    "        y_proba = probas_cache[name]\n",
    "        \n",
    "        if to_binary is not None and y_test_bin is not None and y_proba is not None:\n",
    "            # Collapse to binary attack vs normal by summing columns.\n",
    "            # Assume column order matches labels; locate normal column.\n",
    "            labels = model.classes_\n",
    "            normal_label = \"normal\"\n",
    "            if normal_label in labels:\n",
    "                normal_idx = list(labels).index(normal_label)\n",
    "                p_normal = y_proba[:, normal_idx]\n",
    "                p_attack = 1.0 - p_normal\n",
    "                prob_pos = p_attack\n",
    "                y_true_bin = y_test_bin\n",
    "                \n",
    "                fig, ax = plt.subplots()\n",
    "                CalibrationDisplay.from_predictions(\n",
    "                    y_true_bin, prob_pos, n_bins=10, name=name, ax=ax\n",
    "                )\n",
    "                ax.set_title(f\"Reliability curve (attack vs normal) – {name}\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"{name}: normal label not found in classes – skipping binary calibration.\")\n",
    "        else:\n",
    "            # Fallback: pick the most common class and plot a one-vs-rest curve.\n",
    "            if y_proba is None:\n",
    "                print(f\"{name}: no predict_proba – skipping calibration.\")\n",
    "                continue\n",
    "            \n",
    "            labels = supervised_models[name].classes_\n",
    "            _, counts = np.unique(y_test, return_counts=True)\n",
    "            majority_idx = int(np.argmax(counts))\n",
    "            majority_label = labels[majority_idx]\n",
    "            prob_pos = y_proba[:, majority_idx]\n",
    "            y_true_bin = (y_test == majority_label).astype(int)\n",
    "            \n",
    "            fig, ax = plt.subplots()\n",
    "            CalibrationDisplay.from_predictions(\n",
    "                y_true_bin, prob_pos, n_bins=10, name=name, ax=ax\n",
    "            )\n",
    "            ax.set_title(f\"Reliability curve (one-vs-rest: {majority_label}) – {name}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime / complexity summary.\n",
    "#\n",
    "# We reported fit and predict times earlier; here we assemble them into\n",
    "# a compact table. For a rough memory estimate, we approximate model size\n",
    "# via pickle byte size.\n",
    "\n",
    "import pickle\n",
    "\n",
    "model_sizes = {}\n",
    "for name, model in supervised_models.items():\n",
    "    try:\n",
    "        model_bytes = pickle.dumps(model)\n",
    "        model_sizes[name] = len(model_bytes)\n",
    "    except Exception:\n",
    "        model_sizes[name] = np.nan\n",
    "\n",
    "runtime_rows = []\n",
    "for name in supervised_models.keys():\n",
    "    runtime_rows.append({\n",
    "        \"model\": name,\n",
    "        \"fit_time_sec\": fit_times.get(name, np.nan),\n",
    "        \"predict_time_sec\": predict_times.get(name, np.nan),\n",
    "        \"model_bytes\": model_sizes.get(name, np.nan),\n",
    "    })\n",
    "\n",
    "runtime_table = pd.DataFrame(runtime_rows).sort_values(\"fit_time_sec\")\n",
    "runtime_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea973899",
   "metadata": {},
   "source": [
    "## Final summary and ranked shortlist\n",
    "\n",
    "Use the `metric_table` and `runtime_table` to summarize trade-offs:\n",
    "\n",
    "- **Detection quality**: macro F1, per-family F1 (especially R2L and U2R)\n",
    "- **Robustness**: stability across seeds and threshold sensitivity\n",
    "- **Calibration**: how well probabilities match observed frequencies\n",
    "- **Complexity**: training time, prediction time, and model size\n",
    "\n",
    "In the cell below, we create a compact summary table and a ranked shortlist.\n",
    "Feel free to add narrative comments for your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbcc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metric and runtime information into one summary table.\n",
    "summary = metric_table.merge(\n",
    "    runtime_table,\n",
    "    how=\"left\",\n",
    "    left_on=\"model\",\n",
    "    right_on=\"model\",\n",
    ")\n",
    "\n",
    "summary_sorted = summary.sort_values([\"type\", \"f1_macro\"], ascending=[True, False])\n",
    "summary_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5dc40",
   "metadata": {},
   "source": [
    "### Ranked shortlist (example structure)\n",
    "\n",
    "You can base your written discussion on something like:\n",
    "\n",
    "1. **[Model A]** — best overall macro F1 and strong R2L/U2R detection; moderate training time.\n",
    "2. **[Model B]** — slightly lower F1 but much faster and smaller; well-calibrated probabilities.\n",
    "3. **[Model C]** — strong on majority classes but weak on rare families; useful as a baseline.\n",
    "\n",
    "Replace with the actual models and observations from your `summary_sorted` table and plots.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
