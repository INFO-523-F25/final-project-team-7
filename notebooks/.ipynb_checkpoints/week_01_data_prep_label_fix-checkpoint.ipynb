{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e32dd8",
   "metadata": {},
   "source": [
    "# Week 1 â€” Data Preparation (NSL-KDD)\n",
    "\n",
    "This notebook loads, validates, encodes, scales, splits, and saves the NSL-KDD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c8d80f5-19a9-4202-9ec3-14d82663586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def ensure_project_root():\n",
    "    # Start from the current working directory of the notebook.\n",
    "    here = Path.cwd().resolve()\n",
    "    # Walk up until we find a folder that contains src/__init__.py.\n",
    "    for parent in [here] + list(here.parents):\n",
    "        src_dir = parent / \"src\"\n",
    "        if src_dir.is_dir() and (src_dir / \"__init__.py\").exists():\n",
    "            if str(parent) not in sys.path:\n",
    "                sys.path.insert(0, str(parent))\n",
    "            return parent\n",
    "    raise RuntimeError(\"Could not find a project root containing src/__init__.py.\")\n",
    "\n",
    "_ = ensure_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "728a9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.utils import set_all_seeds, RANDOM_STATE, DATA_RAW, DATA_PROCESSED, ensure_dir\n",
    "from src.io import load_nsl_kdd_raw, save_numpy, save_joblib\n",
    "from src.prep import fit_transform_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235f73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility.\n",
    "set_all_seeds(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f39d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw file: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n",
      "Processed output: C:\\Users\\mehra\\Final_Project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Define data paths.\n",
    "raw_file = DATA_RAW / 'NSL-KDD.raw'\n",
    "processed_dir = ensure_dir(DATA_PROCESSED)\n",
    "\n",
    "print('Raw file:', raw_file)\n",
    "print('Processed output:', processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b86cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (494021, 42)\n",
      "Verified schema successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset.\n",
    "df = load_nsl_kdd_raw(raw_file)\n",
    "print('Shape:', df.shape)\n",
    "for c in ['protocol_type', 'service', 'flag', 'label']:\n",
    "    assert c in df.columns, f'Missing column: {c}'\n",
    "print('Verified schema successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ea60d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numeric values: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert numeric columns safely.\n",
    "categorical = ['protocol_type', 'service', 'flag']\n",
    "numeric = [c for c in df.columns if c not in categorical + ['label']]\n",
    "for c in numeric:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "print('Missing numeric values:', int(df[numeric].isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b7aa4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "smurf.          280790\n",
      "neptune.        107201\n",
      "normal.          97278\n",
      "back.             2203\n",
      "satan.            1589\n",
      "ipsweep.          1247\n",
      "portsweep.        1040\n",
      "warezclient.      1020\n",
      "teardrop.          979\n",
      "pod.               264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect label distribution.\n",
    "print(df['label'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb14cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (395216, 115) Test shape: (98805, 115)\n",
      "Train classes: {'other': 395216}\n",
      "Test classes: {'other': 98805}\n"
     ]
    }
   ],
   "source": [
    "# Split data, fit preprocessor, and transform.\n",
    "X_train, X_test, y_train, y_test, arts = fit_transform_split(df, random_state=RANDOM_STATE)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "print('Train classes:', {k:int(v) for k,v in pd.Series(y_train).value_counts().items()})\n",
    "print('Test classes:',  {k:int(v) for k,v in pd.Series(y_test).value_counts().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fd2027c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed artifacts saved to: C:\\Users\\mehra\\Final_Project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Save processed arrays and preprocessor.\n",
    "save_numpy(X_train, processed_dir / 'X_train.npy')\n",
    "save_numpy(X_test, processed_dir / 'X_test.npy')\n",
    "save_numpy(y_train, processed_dir / 'y_train.npy')\n",
    "save_numpy(y_test, processed_dir / 'y_test.npy')\n",
    "save_joblib(arts.preprocessor, processed_dir / 'preprocessor.joblib')\n",
    "\n",
    "print('All processed artifacts saved to:', processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9053098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n",
      "  - C:\\Users\\mehra\\Final_Project\\data\\processed\\X_train.npy\n",
      "  - C:\\Users\\mehra\\Final_Project\\data\\processed\\X_test.npy\n",
      "  - C:\\Users\\mehra\\Final_Project\\data\\processed\\y_train.npy\n",
      "  - C:\\Users\\mehra\\Final_Project\\data\\processed\\y_test.npy\n",
      "  - C:\\Users\\mehra\\Final_Project\\data\\processed\\preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# Summarize completion.\n",
    "print('Data preparation complete.')\n",
    "for f in ['X_train.npy','X_test.npy','y_train.npy','y_test.npy','preprocessor.joblib']:\n",
    "    print('  -', processed_dir / f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729bdf9",
   "metadata": {},
   "source": [
    "## Label Normalization and Robust Saving (Week 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Normalize labels and save processed arrays robustly ---\n",
    "import numpy as np, json\n",
    "from pathlib import Path\n",
    "from src.utils import ensure_dir\n",
    "\n",
    "# Binary label mapping: normal -> 0, others -> 1\n",
    "def to_binary_anomaly(y):\n",
    "    y = np.asarray(y)\n",
    "    try:\n",
    "        u = np.unique(y)\n",
    "        if set(u.tolist()).issubset({0, 1}) or set(u.astype(int).tolist()).issubset({0, 1}):\n",
    "            return y.astype(np.int8, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    y_str = np.char.lower(y.astype(str))\n",
    "    y_bin = np.ones_like(y_str, dtype=np.int8)\n",
    "    y_bin[(y_str == \"normal\") | (y_str == \"0\")] = 0\n",
    "    return y_bin\n",
    "\n",
    "# Apply normalization\n",
    "y_train_bin = to_binary_anomaly(y_train)\n",
    "y_test_bin  = to_binary_anomaly(y_test)\n",
    "\n",
    "# Sanity counts\n",
    "def counts(name, arr):\n",
    "    u, c = np.unique(arr, return_counts=True)\n",
    "    print(f\"{name} counts:\", dict(zip(u.tolist(), c.tolist())))\n",
    "\n",
    "counts(\"y_train_bin\", y_train_bin)\n",
    "counts(\"y_test_bin\",  y_test_bin)\n",
    "\n",
    "# Define processed dir\n",
    "try:\n",
    "    from src.utils import DATA_PROCESSED\n",
    "    PROCESSED_DIR = Path(DATA_PROCESSED)\n",
    "except Exception:\n",
    "    PROCESSED_DIR = Path(r\"C:\\Users\\mehra\\Final_Project\\data\\processed\")\n",
    "\n",
    "ensure_dir(PROCESSED_DIR)\n",
    "\n",
    "# Convert + save as numeric\n",
    "np.save(str(PROCESSED_DIR / \"X_train.npy\"), np.asarray(X_train, dtype=np.float32, order=\"C\"))\n",
    "np.save(str(PROCESSED_DIR / \"X_test.npy\"),  np.asarray(X_test,  dtype=np.float32, order=\"C\"))\n",
    "np.save(str(PROCESSED_DIR / \"y_train.npy\"), y_train_bin)\n",
    "np.save(str(PROCESSED_DIR / \"y_test.npy\"),  y_test_bin)\n",
    "\n",
    "# Save label mapping\n",
    "with open(PROCESSED_DIR / \"label_mapping.json\", \"w\") as f:\n",
    "    json.dump({\"scheme\": \"binary\", \"normal->0, else->1\": True}, f, indent=2)\n",
    "\n",
    "print(\"Saved processed arrays to:\", PROCESSED_DIR.resolve())\n",
    "for fpath in [\"X_train.npy\",\"X_test.npy\",\"y_train.npy\",\"y_test.npy\"]:\n",
    "    p = PROCESSED_DIR / fpath\n",
    "    print(f\" - {fpath}: {'OK' if p.exists() else 'MISSING'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
