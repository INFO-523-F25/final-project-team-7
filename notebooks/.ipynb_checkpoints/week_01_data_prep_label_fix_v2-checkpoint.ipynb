{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e32dd8",
   "metadata": {},
   "source": [
    "# Week 1 â€” Data Preparation (NSL-KDD)\n",
    "\n",
    "This notebook loads, validates, encodes, scales, splits, and saves the NSL-KDD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8d80f5-19a9-4202-9ec3-14d82663586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def ensure_project_root():\n",
    "    # Start from the current working directory of the notebook.\n",
    "    here = Path.cwd().resolve()\n",
    "    # Walk up until we find a folder that contains src/__init__.py.\n",
    "    for parent in [here] + list(here.parents):\n",
    "        src_dir = parent / \"src\"\n",
    "        if src_dir.is_dir() and (src_dir / \"__init__.py\").exists():\n",
    "            if str(parent) not in sys.path:\n",
    "                sys.path.insert(0, str(parent))\n",
    "            return parent\n",
    "    raise RuntimeError(\"Could not find a project root containing src/__init__.py.\")\n",
    "\n",
    "_ = ensure_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728a9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules.\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from src.utils import set_all_seeds, RANDOM_STATE, DATA_RAW, DATA_PROCESSED, ensure_dir\n",
    "from src.io import load_nsl_kdd_raw, save_numpy, save_joblib\n",
    "from src.prep import fit_transform_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235f73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility.\n",
    "set_all_seeds(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f39d4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw file: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n",
      "Processed output: C:\\Users\\mehra\\Final_Project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Define data paths.\n",
    "raw_file = DATA_RAW / 'NSL-KDD.raw'\n",
    "processed_dir = ensure_dir(DATA_PROCESSED)\n",
    "\n",
    "print('Raw file:', raw_file)\n",
    "print('Processed output:', processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b86cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (494021, 42)\n",
      "Verified schema successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset.\n",
    "df = load_nsl_kdd_raw(raw_file)\n",
    "print('Shape:', df.shape)\n",
    "for c in ['protocol_type', 'service', 'flag', 'label']:\n",
    "    assert c in df.columns, f'Missing column: {c}'\n",
    "print('Verified schema successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea60d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing numeric values: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert numeric columns safely.\n",
    "categorical = ['protocol_type', 'service', 'flag']\n",
    "numeric = [c for c in df.columns if c not in categorical + ['label']]\n",
    "for c in numeric:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "print('Missing numeric values:', int(df[numeric].isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7aa4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "smurf.          280790\n",
      "neptune.        107201\n",
      "normal.          97278\n",
      "back.             2203\n",
      "satan.            1589\n",
      "ipsweep.          1247\n",
      "portsweep.        1040\n",
      "warezclient.      1020\n",
      "teardrop.          979\n",
      "pod.               264\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect label distribution.\n",
    "print(df['label'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb14cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (395216, 115) Test shape: (98805, 115)\n",
      "Train classes: {'other': 395216}\n",
      "Test classes: {'other': 98805}\n"
     ]
    }
   ],
   "source": [
    "# Split data, fit preprocessor, and transform.\n",
    "X_train, X_test, y_train, y_test, arts = fit_transform_split(df, random_state=RANDOM_STATE)\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "print('Train classes:', {k:int(v) for k,v in pd.Series(y_train).value_counts().items()})\n",
    "print('Test classes:',  {k:int(v) for k,v in pd.Series(y_test).value_counts().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd2027c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\mehra\\\\Final_Project\\\\data\\\\processed\\\\X_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save processed arrays and preprocessor.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m save_numpy(X_train, processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m save_numpy(X_test, processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m save_numpy(y_train, processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m save_numpy(y_test, processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Final_Project\\src\\io.py:50\u001b[0m, in \u001b[0;36msave_numpy\u001b[1;34m(arr, path)\u001b[0m\n\u001b[0;32m     48\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(path)\n\u001b[0;32m     49\u001b[0m ensure_dir(path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m---> 50\u001b[0m np\u001b[38;5;241m.\u001b[39msave(path, arr)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:542\u001b[0m, in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    541\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 542\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[0;32m    545\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\mehra\\\\Final_Project\\\\data\\\\processed\\\\X_test.npy'"
     ]
    }
   ],
   "source": [
    "# Save processed arrays and preprocessor.\n",
    "save_numpy(X_train, processed_dir / 'X_train.npy')\n",
    "save_numpy(X_test, processed_dir / 'X_test.npy')\n",
    "save_numpy(y_train, processed_dir / 'y_train.npy')\n",
    "save_numpy(y_test, processed_dir / 'y_test.npy')\n",
    "save_joblib(arts.preprocessor, processed_dir / 'preprocessor.joblib')\n",
    "\n",
    "print('All processed artifacts saved to:', processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize completion.\n",
    "print('Data preparation complete.')\n",
    "for f in ['X_train.npy','X_test.npy','y_train.npy','y_test.npy','preprocessor.joblib']:\n",
    "    print('  -', processed_dir / f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729bdf9",
   "metadata": {},
   "source": [
    "## Label Normalization and Robust Saving (Week 01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Normalize labels and save processed arrays robustly ---\n",
    "import numpy as np, json\n",
    "from pathlib import Path\n",
    "from src.utils import ensure_dir\n",
    "\n",
    "# Try to retrieve train/test arrays from globals, src.io helpers, or processed files\n",
    "def get_split_arrays(processed_dir: Path):\n",
    "    # 1) from globals\n",
    "    g = globals()\n",
    "    if all(k in g for k in (\"X_train\",\"X_test\",\"y_train\",\"y_test\")):\n",
    "        return g[\"X_train\"], g[\"X_test\"], g[\"y_train\"], g[\"y_test\"]\n",
    "\n",
    "    # 2) from src.io helpers\n",
    "    try:\n",
    "        import src.io as io_mod\n",
    "        candidates = [\n",
    "            (\"load_processed_arrays\", {}),\n",
    "            (\"load_processed_split\", {}),\n",
    "            (\"load_numpy_split\", {}),\n",
    "            (\"load_split\", {\"where\": \"processed\"}),\n",
    "            (\"load_arrays\", {\"subset\": \"processed\"}),\n",
    "        ]\n",
    "        for fn_name, kwargs in candidates:\n",
    "            if hasattr(io_mod, fn_name):\n",
    "                fn = getattr(io_mod, fn_name)\n",
    "                try:\n",
    "                    out = fn(**kwargs)\n",
    "                    if isinstance(out, dict):\n",
    "                        X_tr, X_te = out.get(\"X_train\"), out.get(\"X_test\")\n",
    "                        y_tr, y_te = out.get(\"y_train\"), out.get(\"y_test\")\n",
    "                    elif isinstance(out, (tuple, list)) and len(out) == 4:\n",
    "                        X_tr, X_te, y_tr, y_te = out\n",
    "                    else:\n",
    "                        X_tr = X_te = y_tr = y_te = None\n",
    "                    if X_tr is not None and X_te is not None and y_tr is not None and y_te is not None:\n",
    "                        return X_tr, X_te, y_tr, y_te\n",
    "                except Exception:\n",
    "                    pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) from processed_dir files (if exist)\n",
    "    Xt_p, Xv_p = processed_dir/\"X_train.npy\", processed_dir/\"X_test.npy\"\n",
    "    yt_p, yv_p = processed_dir/\"y_train.npy\", processed_dir/\"y_test.npy\"\n",
    "    if Xt_p.exists() and Xv_p.exists() and yt_p.exists() and yv_p.exists():\n",
    "        # safe load (handles object arrays)\n",
    "        def load_npy_safe(pth: Path):\n",
    "            try:\n",
    "                return np.load(pth)\n",
    "            except ValueError as e:\n",
    "                if \"allow_pickle\" in str(e):\n",
    "                    arr = np.load(pth, allow_pickle=True)\n",
    "                    if getattr(arr, \"dtype\", None) == object and arr.size == 1:\n",
    "                        arr = arr.item()\n",
    "                    try:\n",
    "                        import scipy.sparse as sp\n",
    "                        if sp.issparse(arr):\n",
    "                            arr = arr.toarray()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return np.asarray(arr)\n",
    "                else:\n",
    "                    raise\n",
    "        return load_npy_safe(Xt_p), load_npy_safe(Xv_p), load_npy_safe(yt_p), load_npy_safe(yv_p)\n",
    "\n",
    "    raise RuntimeError(\"Could not find X_train/X_test/y_train/y_test in memory, src.io, or processed_dir. \"\n",
    "                       \"Run the earlier Week 01 prep cells first.\")\n",
    "\n",
    "# Prefer DATA_PROCESSED from utils, else fallback to your absolute path\n",
    "try:\n",
    "    from src.utils import DATA_PROCESSED\n",
    "    PROCESSED_DIR = Path(DATA_PROCESSED)\n",
    "except Exception:\n",
    "    PROCESSED_DIR = Path(r\"C:\\Users\\mehra\\Final_Project\\data\\processed\")\n",
    "\n",
    "ensure_dir(PROCESSED_DIR)\n",
    "\n",
    "# Obtain arrays\n",
    "X_train, X_test, y_train, y_test = get_split_arrays(PROCESSED_DIR)\n",
    "\n",
    "# Binary label mapping: normal -> 0, others -> 1\n",
    "def to_binary_anomaly(y):\n",
    "    y = np.asarray(y)\n",
    "    try:\n",
    "        u = np.unique(y)\n",
    "        if set(u.tolist()).issubset({0, 1}) or set(u.astype(int).tolist()).issubset({0, 1}):\n",
    "            return y.astype(np.int8, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    y_str = np.char.lower(y.astype(str))\n",
    "    y_bin = np.ones_like(y_str, dtype=np.int8)\n",
    "    y_bin[(y_str == \"normal\") | (y_str == \"0\")] = 0\n",
    "    return y_bin\n",
    "\n",
    "# Apply normalization\n",
    "y_train_bin = to_binary_anomaly(y_train)\n",
    "y_test_bin  = to_binary_anomaly(y_test)\n",
    "\n",
    "# Sanity counts\n",
    "def counts(name, arr):\n",
    "    u, c = np.unique(arr, return_counts=True)\n",
    "    print(f\"{name} counts:\", dict(zip(u.tolist(), c.tolist())))\n",
    "\n",
    "counts(\"y_train_bin\", y_train_bin)\n",
    "counts(\"y_test_bin\",  y_test_bin)\n",
    "\n",
    "# Convert + save as numeric\n",
    "np.save(str(PROCESSED_DIR / \"X_train.npy\"), np.asarray(X_train, dtype=np.float32, order=\"C\"))\n",
    "np.save(str(PROCESSED_DIR / \"X_test.npy\"),  np.asarray(X_test,  dtype=np.float32, order=\"C\"))\n",
    "np.save(str(PROCESSED_DIR / \"y_train.npy\"), y_train_bin)\n",
    "np.save(str(PROCESSED_DIR / \"y_test.npy\"),  y_test_bin)\n",
    "\n",
    "# Save label mapping\n",
    "with open(PROCESSED_DIR / \"label_mapping.json\", \"w\") as f:\n",
    "    json.dump({\"scheme\": \"binary\", \"normal->0, else->1\": True}, f, indent=2)\n",
    "\n",
    "print(\"Saved processed arrays to:\", PROCESSED_DIR.resolve())\n",
    "for fpath in [\"X_train.npy\",\"X_test.npy\",\"y_train.npy\",\"y_test.npy\"]:\n",
    "    p = PROCESSED_DIR / fpath\n",
    "    print(f\" - {fpath}: {'OK' if p.exists() else 'MISSING'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
