{
  "hash": "c37249147d39fe7dfc78f11b344c5c00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Anomaly Detection in Network Traffic Using NSL-KDD Dataset\"\nsubtitle: \"INFO 523 - Fall 2025 - Final Project\"\nauthor: \"Mehran Tajbakhsh\"\ntitle-slide-attributes:\n  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg\n  data-background-size: stretch\n  data-background-opacity: \"0.7\"\n  data-slide-number: none\nformat:\n  revealjs:\n    theme:  ['data/customtheming.scss']\n  \neditor: visual\njupyter: python3\nexecute:\n  echo: false\n---\n\n## Introduction & Motivation {background-gradient=\"linear-gradient(to bottom, #283b95, 17b2c3)\" transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Rising **network complexity**\n- Expanding **attack surface**\n- Need for **adaptive detection**\n- Limits of **signature systems**\n- Value of **behavioral analytics**\n- Goal: improved **threat identification**\n\n::: aside \nThis project is motivated by the growing complexity of modern networks and the expanding attack surface that organizations face. Traditional signature-based detection struggles to keep pace with evolving and previously unseen threats. By shifting toward adaptive, behavior-based analytics, we can identify abnormal activity earlier and more accurately. The overall objective is to strengthen threat detection capabilities and improve security decision-making at scale.\n:::\n\n## Problem Statement {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Identify limits of current **anomaly detection** approaches\n- Assess ability to detect **malicious network behavior**\n- Compare **unsupervised**, **statistical**, and **supervised** methods\n- Evaluate differences in **detection effectiveness**\n- Understand variation in **precision**, **recall**, and false alerts\n- Determine most reliable model for **operational deployment**\n\n::: aside\nThe goal is to understand how different detection methods perform when identifying malicious activity in complex network environments. We evaluate traditional statistical models, modern unsupervised anomaly detectors, and supervised machine-learning approaches to determine their strengths and limitations. The focus is on how accurately these methods detect threats, how often they generate false alerts, and how interpretable their outputs are for analysts. Ultimately, the aim is to determine which approach provides the most reliable and operationally effective intrusion detection capability.\n:::\n\n## Background {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Expanding **cyberattack landscape**\n- Importance of **anomaly detection**\n- Mix of **statistical** and **ML** tools**\n- Challenge of **imbalanced attacks**\n- Need for **explainability**\n- Relevance to modern **SOCs**\n\n::: aside\nCyberattacks continue to grow in scale and sophistication, making traditional rule-based detection insufficient. Anomaly detection is increasingly important because it can surface unusual or emerging behaviors that signatures fail to capture. Our evaluation spans both statistical techniques and machine-learning models, each offering different strengths. A major challenge in this domain is the imbalance between normal traffic and rare attack events, which can distort model performance. Explainability is also essential so analysts can trust and validate detection outputs. These considerations directly impact how Security Operations Centers operate and prioritize alerts.\n:::\n\n## Dataset Overview {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- **NSL-KDD** standard dataset\n- 41 engineered **network features**\n- Balanced **training/test** sets\n- Multiple **attack categories**\n- Supports **binary** and **multi-class** tasks\n- Widely used for **IDS research**\n\n::: aside\nThe NSL-KDD dataset is a widely used benchmark for intrusion detection research and is designed to address some of the limitations in the original KDD’99 dataset, such as redundant records. It contains 41 engineered network features and multiple attack categories, enabling both binary and multi-class evaluation. For this project, we used the 10% version of the dataset, which provides a representative but computationally manageable subset of the full collection. The data comes from the University of New Brunswick’s Canadian Institute for Cybersecurity repository, which maintains standardized, research-grade network security datasets.\n:::\n\n## Methodology {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Standard pipeline: **EDA** → **preprocessing** → **modeling** → **evaluation**\n- Data **cleaning**, **encoding**, and **scaling**\n- Unsupervised: **Z-score**, **IQR**, **Elliptic**, **Mahalanobis**\n- Advanced: **Isolation Forest**, **LOF**, **Autoencoder**\n- Supervised: **RF**, **SVM**, **LogReg**\n- Evaluated using **accuracy**, **precision**, **recall**, **F1**, **AUC**\n\n::: aside\nOur workflow follows a standard machine learning pipeline beginning with exploratory data analysis, followed by preprocessing steps such as cleaning, encoding, and scaling. We implemented a range of models, starting with statistical and unsupervised anomaly detectors, then moving to more advanced techniques like Isolation Forest, Local Outlier Factor, and Autoencoders. Supervised models, including Random Forest, SVM, and Logistic Regression, were trained using the labeled portions of the dataset. All models were evaluated using consistent metrics—accuracy, precision, recall, F1, and AUC—to ensure a fair comparison across approaches.\n:::\n\n## Unsupervised Results: Statistical {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- **Z-score** detects extreme deviations\n- **IQR** captures distribution outliers\n- **Elliptic Envelope** robust covariance\n- **Mahalanobis** multivariate distance\n- High **false-positive** variability\n- Limited subtle **attack detection**\n\n::: aside\nThese traditional statistical methods provide fast and lightweight anomaly detection but have clear limitations. Z-score and IQR rely on simple distribution thresholds, so they mainly detect very large deviations and miss more nuanced behaviors. Elliptic Envelope and Mahalanobis distance incorporate covariance structure, offering a more sophisticated view of normal traffic, but they still struggle with the complexity and variability of real attack patterns. Across all of these methods, we observed high false-positive rates and inconsistent performance, especially when the anomalies were subtle or closely resembled normal traffic. This highlights the need for more advanced models in operational environments.\n:::\n\n## Unsupervised Results: Advanced {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- **Isolation Forest** strong separation\n- **LOF** local-density anomalies\n- **Autoencoder** learns latent structure\n- Outperforms statistical baselines\n- Lower recall for **stealth attacks**\n- Needs **threshold tuning**\n\n::: aside\nThese advanced unsupervised models deliver noticeably better performance than the simpler statistical methods. Isolation Forest is effective at separating anomalies by isolating rare behaviors, while LOF focuses on local density differences to detect unusual patterns in small regions of the feature space. The Autoencoder, which learns compressed representations of normal traffic, performed the strongest among the unsupervised approaches and was able to capture complex relationships between features. However, all these models still showed limitations, particularly with low-signal or stealthy attacks that closely resemble normal activity. Their performance is also highly sensitive to threshold settings, meaning operational tuning is essential for reliable detection.\n:::\n\n## Supervised Results: Models {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- **Random Forest** best performance\n- High **precision** and **recall**\n- **SVM-RBF** competitive results\n- Logistic baseline comparison\n- Clear attack **class separation**\n- Stable across **test folds**\n\n::: aside\nSupervised learning clearly provided the strongest results in this evaluation. Random Forest delivered the best overall performance with consistently high precision and recall, making it the most reliable model for distinguishing normal and malicious traffic. The SVM with an RBF kernel also performed competitively, especially on nonlinear patterns, while Logistic Regression served as a baseline for comparison. These models showed clear separation between attack classes and demonstrated stable behavior across multiple test folds, reinforcing their suitability for operational intrusion detection when labeled data is available.\n:::\n\n## Supervised Results: Explainability {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Random Forest feature **importance clarity**\n- Identify top **predictive fields**\n- SVM **decision boundary** insights\n- Enhanced analyst **trust**\n- Supports security **auditability**\n- Reduces black-box **risk**\n\n::: aside\nA key advantage of the supervised models—especially Random Forest and SVM—is their interpretability. Random Forest provides clear feature-importance rankings, helping analysts understand which network attributes contribute most to attack detection. SVM models offer insights into decision boundaries, clarifying how the classifier separates malicious and normal behavior. These interpretability tools improve analyst trust, support auditing and compliance requirements, and reduce the risks associated with deploying opaque, black-box detection systems. For an operational SOC, this level of transparency is essential for validating alerts and refining detection strategies.\n:::\n\n## Model Performance Summary {transition=\"fade-in slide-out\" transition-speed=\"slow\"}\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>precision_macro</th>\n      <th>recall_macro</th>\n      <th>f1_macro</th>\n      <th>roc_auc_ovr_macro</th>\n      <th>pr_auc_ovr_macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Random Forest (baseline)</td>\n      <td>0.962361</td>\n      <td>0.974293</td>\n      <td>0.967920</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random Forest + SMOTE</td>\n      <td>0.947867</td>\n      <td>0.975182</td>\n      <td>0.960155</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SVM-RBF + SMOTE</td>\n      <td>0.890610</td>\n      <td>0.930098</td>\n      <td>0.905367</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SVM-RBF (baseline)</td>\n      <td>0.642394</td>\n      <td>0.722858</td>\n      <td>0.501165</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Z-Score</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>OC-SVM (RBF)</td>\n      <td>0.779636</td>\n      <td>1.000000</td>\n      <td>0.876175</td>\n      <td>0.962972</td>\n      <td>0.779636</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Isolation Forest</td>\n      <td>0.769121</td>\n      <td>0.369132</td>\n      <td>0.498847</td>\n      <td>0.670050</td>\n      <td>0.414882</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Deep Autoencoder</td>\n      <td>0.906161</td>\n      <td>0.046605</td>\n      <td>0.088650</td>\n      <td>0.981710</td>\n      <td>0.878009</td>\n    </tr>\n  </tbody>\n</table>\n\n</div>\n\n\n## Models Comparison {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Supervised models **outperform** unsupervised\n- Unsupervised useful for **unknown** attacks\n- Supervised highest **accuracy**\n- Autoencoder best unsupervised\n- Statistical weakest coverage\n- RF strongest overall detector\n\n::: aside\nWhen comparing all modeling approaches, supervised methods clearly performed the best overall, offering the highest accuracy and the most reliable detection across attack categories. Unsupervised models still play an important role, particularly for detecting new or previously unseen attack patterns, but their precision and consistency are lower. Among the unsupervised models, the Autoencoder showed the strongest capability, while simple statistical techniques provided the weakest coverage and struggled with complex behaviors. Random Forest emerged as the strongest individual model in the entire evaluation, combining accuracy, stability, and interpretability.\n:::\n\n##  Discussion {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable} \n\n**How effectively can unsupervised anomaly detection identify malicious traffic?**\n\n- Trained only on **normal traffic**\n- Detects **high-variance anomalies**\n- Misses **subtle or stealth attacks**\n- Useful for **unknown threats**\n- Requires **threshold tuning**\n- Moderately effective overall\n\n::: aside\nUnsupervised anomaly detection methods can identify attacks that significantly deviate from normal behavior, especially high-variance or clearly abnormal patterns. However, they struggle with low-signal attacks that resemble legitimate traffic, leading to missed detections. Their strength lies in detecting unknown or emerging threats where labeled data is unavailable, but their performance is inconsistent without careful threshold calibration. Overall, unsupervised methods are moderately effective but insufficient as a standalone intrusion detection strategy.\n:::\n\n##  Discussion {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n\n**How do supervised ML models compare with statistical anomaly detectors?**\n\n- Supervised achieve **higher accuracy**\n- Stronger **precision** and **recall**\n- Better **class separation**\n- Statistical models detect only **simple outliers**\n- Supervised offer **clearer interpretability**\n- More reliable for **operational IDS**\n\n::: aside\nSupervised machine learning models significantly outperform statistical anomaly detection methods across all major evaluation metrics. They deliver higher accuracy, stronger precision and recall, and more consistent detection of diverse attack types. Statistical methods, such as Z-score, IQR, and Mahalanobis distance, catch only basic or extreme outliers and fail to capture complex attack behaviors. Supervised models also provide clearer interpretability through feature importance and decision boundary analysis, making them far easier for analysts to validate and operationalize. Overall, supervised learning offers far superior intrusion detection performance and reliability in real-world environments.\n:::\n\n## Discussion {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Unsupervised moderately **effective**\n- Detects only **high-variance** anomalies\n- Struggles with **low-signal** attacks\n- Supervised significantly **stronger**\n- Higher **precision** and **recall**\n- Better overall **interpretability**\n\n::: aside\nOverall, the results show that unsupervised methods are moderately effective but mainly detect anomalies that are large or highly unusual. They tend to miss low-signal or stealthy attacks that blend in with normal traffic. In contrast, supervised models performed significantly better across all key metrics, including precision and recall, and provided clearer interpretability for analysts. This makes them more dependable for operational use when labeled data is available. The comparison highlights that while unsupervised models add value for discovering unknown threats, supervised learning remains the stronger and more reliable foundation for intrusion detection.\n:::\n\n## Overall Trade-Off {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Supervised: high **accuracy**, low flexibility\n- Unsupervised: high **adaptability**, lower precision\n- Statistical: fast, **low insight**\n- Supervised requires **maintenance**\n- Unsupervised requires **threshold tuning**\n- Trade-off: precision vs **generalization**\n\n::: aside\nEach modeling approach comes with strengths and trade-offs. Supervised models deliver high accuracy and strong predictive power, but they depend on labeled data and require ongoing maintenance as network behavior evolves. Unsupervised models are more flexible and adaptable, making them valuable for detecting unfamiliar threats, but they generally produce lower precision and require careful threshold tuning. Statistical methods are fast and simple to deploy, yet they offer limited insight and struggle with complex attack patterns. Ultimately, the decision comes down to balancing precision against generalization, depending on the organization’s operational needs and risk tolerance.\n:::\n\n## Real-World Implications {transition=\"fade-in slide-out\" transition-speed=\"slow\" .scrollable}\n- Manage **false positives** proactively\n- Adjust **alert thresholds** regularly\n- Plan periodic **model retraining**\n- Monitor **concept drift** indicators\n- Combine supervised + unsupervised\n- Strengthen SOC **decision support**\n\n::: aside\nThese findings translate into several practical considerations for real-world intrusion detection. First, managing false positives is essential to prevent analyst fatigue and ensure SOC efficiency. Alert thresholds should be reviewed and adjusted regularly, especially for unsupervised models that are sensitive to behavioral shifts. Models must be retrained periodically to stay aligned with evolving network patterns and avoid degradation from concept drift. A hybrid strategy—combining supervised and unsupervised techniques—can offer both strong accuracy and adaptability. Altogether, these practices enhance SOC decision-making and support more resilient and proactive cybersecurity operations.\n:::\n\n",
    "supporting": [
      "presentation_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}