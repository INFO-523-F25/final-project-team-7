[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "The NSL-KDD dataset is an enhanced version of the KDD Cup 1999 intrusion detection benchmark.  It removes redundant records and provides a more balanced sample distribution for fair model evaluation.  Each record represents a network connection with 41 features describing various characteristics such as protocol type, service, traffic flags, and packet statistics. The target label identifies whether a connection is normal or belongs to one of four attack families:  - DoS (Denial of Service): Overwhelms system resources (e.g., neptune, smurf, teardrop)  - Probe: Scans for vulnerabilities or open ports (e.g., satan, nmap, ipsweep)  - R2L (Remote to Local): Attempts to gain access remotely (e.g., guess_passwd, imap)  - U2R (User to Root): Escalates privileges locally (e.g., rootkit, buffer_overflow) \n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nIn the context of intrusion detection, the dataset encompasses four main families of \nattack behaviors: Denial of Service (DoS), Probe, Remote to Local (R2L),\nand User to Root (U2R): \n\n- normal: 19.69% (n = 97278)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset?  It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "The NSL-KDD dataset is an enhanced version of the KDD Cup 1999 intrusion detection benchmark.  It removes redundant records and provides a more balanced sample distribution for fair model evaluation.  Each record represents a network connection with 41 features describing various characteristics such as protocol type, service, traffic flags, and packet statistics. The target label identifies whether a connection is normal or belongs to one of four attack families:  - DoS (Denial of Service): Overwhelms system resources (e.g., neptune, smurf, teardrop)  - Probe: Scans for vulnerabilities or open ports (e.g., satan, nmap, ipsweep)  - R2L (Remote to Local): Attempts to gain access remotely (e.g., guess_passwd, imap)  - U2R (User to Root): Escalates privileges locally (e.g., rootkit, buffer_overflow) \n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nIn the context of intrusion detection, the dataset encompasses four main families of \nattack behaviors: Denial of Service (DoS), Probe, Remote to Local (R2L),\nand User to Root (U2R): \n\n- normal: 19.69% (n = 97278)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset?  It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Questions",
    "text": "Questions\n\nHow effectively can unsupervised anomaly detection methods identify malicious network traffic when trained only on normal data? \nHow does the performance of supervised machine learning–based intrusion detection compare with statistical anomaly detection methods in terms of detection accuracy, precision, recall, and interpretability?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Analysis plan",
    "text": "Analysis plan\nWeek 1 – Problem Definition & Data Preparation The first week focuses on defining the objectives of anomaly detection within the NSL-KDD dataset. The dataset will be loaded, inspected, and preprocessed to ensure analytical readiness. Tasks include handling missing values, encoding categorical features, and scaling numerical variables. Deliverables: Data preparation notebook and summary report.\nWeek 2 – Exploratory Data Analysis (EDA) This phase aims to understand data distributions, feature correlations, and patterns of normal versus attack traffic. Visualization techniques will be applied to highlight key discriminative features and potential sources of anomaly. Deliverables: EDA notebook with visual insights and descriptive summaries.\nWeek 3 – Statistical Anomaly Detection Methods In this stage, classical statistical methods for anomaly detection will be implemented, including Z-Score, Elliptical Envelope, Local Outlier Factor (LOF), and DBSCAN. Their performance in identifying abnormal network behaviors will be analyzed and compared. Deliverables: Statistical anomaly detection notebook and comparative summary.\nWeek 4 – Machine Learning–Based Methods Modern learning-based detectors will be explored using Isolation Forest, One-Class SVM, and Deep Autoencoders. Models will be tuned for optimal performance, and detection boundaries will be visualized to interpret decision behavior. Deliverables: ML anomaly detection notebook and evaluation metrics.\nWeek 5 – Model Evaluation and Comparison All implemented methods will be assessed using standard performance metrics such as Precision, Recall, F1-score, ROC-AUC, and Detection Rate. Comparative analyses and ablation studies will be performed to evaluate robustness and generalization. Deliverables: Evaluation notebook and performance report.\nWeek 6 – Final Analysis & Reporting The final week synthesizes results into a comprehensive assessment of the effectiveness of different anomaly detection techniques. The findings will discuss practical implications, advantages, and limitations of each approach in real-world intrusion detection scenarios. Additionally, a reflection phase will visualize anomaly scores and latent embeddings (via PCA, t-SNE, UMAP) to assess separability between normal and attack samples. The final report will include a comparative interpretability reflection, discussing how the transparency–accuracy balance affects IDS deployment decisions. Deliverables: Final project report and presentation slides."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "Although the NSL-KDD dataset is dated and partly synthetic—constructed from simulated network environments rather than real enterprise traffic—it remains one of the most widely used and well-documented benchmarks for intrusion detection research.\nIts design introduces several limitations that will be carefully considered in this study:\n\nTemporal relevance: The dataset reflects network behaviors and attack types common in the late 1990s, before the advent of modern threats such as advanced persistent attacks, encrypted communication channels, and large-scale botnets.\n\nSynthetic generation: The data were produced under controlled lab conditions rather than from live network capture, which means background noise, user variability, and realistic packet timing patterns are underrepresented.\n\nFeature design: Certain attributes (e.g., service, flag) may no longer map directly to contemporary protocols or operating systems, reducing direct applicability to modern network telemetry.\n\nLabel completeness: All attacks are labeled, unlike in real-world network monitoring where labels are often incomplete or delayed.\n\nDespite these constraints, NSL-KDD remains a valuable controlled testbed for benchmarking anomaly and intrusion detection algorithms because:\n\nIt provides clearly defined attack families (DoS, Probe, R2L, U2R) that facilitate comparative and reproducible experiments.\n\nIt allows systematic evaluation of class imbalance, thresholding, and interpretability trade-offs across model types.\n\nIt serves as a pedagogically consistent foundation for developing methods that can later be generalized to newer datasets such as CIC-IDS2017, UNSW-NB15, or proprietary enterprise data.\n\nMitigation strategies include:\n\nApplying robust preprocessing, resampling, and dimensional reduction to reduce dataset biases.\n\nFraming results as algorithmic performance benchmarks, not as direct indicators of deployment-ready accuracy.\n\nReflecting on how each model’s strengths and weaknesses might translate to modern, encrypted, or real-time network environments.\n\n\n\n\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\n\nlike\nthis\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 29 Oct 2025   Prob (F-statistic):           5.84e-08\nTime:                        09:16:38   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Thu, 04 Dec 2025   Prob (F-statistic):           5.84e-08\nTime:                        16:02:27   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "presentation.html#footnotes",
    "href": "presentation.html#footnotes",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd add footnotes↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Mehran Tajbakhsh For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nMehran Tajbakhsh: Final-year MSDS student with an MS in Computer Science, specializing in cybersecurity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "This project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "This project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "proposal.html#dataset-summary",
    "href": "proposal.html#dataset-summary",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Summary",
    "text": "Dataset Summary\nThe NSL-KDD dataset is an improved successor to KDD Cup 1999, designed to mitigate key issues such as redundant records and extreme class imbalance. It comprises network connection records with 41 features (transport/application protocol, service, flag indicators, traffic statistics, etc.) and a label indicating normal or one of several attack types grouped into four categories: DoS, Probe, R2L, and U2R.\n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nFor security analytics, labels are grouped into families:\n\n- normal: 0.00% (n = 0)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset? It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#methodology-overview",
    "href": "proposal.html#methodology-overview",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Methodology Overview",
    "text": "Methodology Overview\nThis project will implement two complementary detection approaches to compare how well statistical and learning-based methods can detect network intrusions. \n\n\n\n\n\n\n\n\n\nApproach Type\nAlgorithms\nTraining Data\nEvaluation Strategy\n\n\n\n\nUnsupervised Anomaly Detection\nZ-Score, Elliptic Envelope, Local Outlier Factor (LOF), DBSCAN\nTrained only on normal traffic (no attack labels)\nEvaluate anomaly scores using percentile-based thresholding (e.g., top 5% as anomalies); compute precision, recall, and false positive rate against true labels\n\n\nSupervised / Semi-Supervised Intrusion Detection\nRandom Forest, Support Vector Machine (SVM), Isolation Forest, Deep Autoencoder\nTrained on labeled normal and attack traffic\nEvaluate using stratified 80/20 train-test split with 5-fold cross-validation; compare Precision, Recall, F1-score, and ROC-AUC\n\n\n\n\nTable 1. Comparison of Supervised vs. Unsupervised Detection Approaches."
  },
  {
    "objectID": "proposal.html#real-world-application-context",
    "href": "proposal.html#real-world-application-context",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Real-World Application Context",
    "text": "Real-World Application Context\nThis project models an enterprise network intrusion detection system (IDS). In real-world settings, such systems continuously monitor inbound and outbound connections to detect unusual traffic patterns. Findings from this analysis can be directly extended to: - Enterprise network monitoring - Cloud service intrusion prevention - IoT network anomaly detection - Streaming data analysis using similar architectures (e.g., online learning or real-time inference)"
  },
  {
    "objectID": "proposal.html#data-preprocessing-and-feature-handling",
    "href": "proposal.html#data-preprocessing-and-feature-handling",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Data Preprocessing and Feature Handling",
    "text": "Data Preprocessing and Feature Handling\n\nFeature Encoding: Categorical features (protocol_type, service, and flag) will be transformed using One-Hot Encoding to convert them into numerical representations suitable for machine learning models.\nScaling: All numeric features will be standardized using StandardScaler to normalize feature magnitudes and support distance-based models such as SVM, LOF, and DBSCAN.\nDimensionality Reduction: Principal Component Analysis (PCA) will be applied to reduce feature dimensionality, minimize noise, and visualize decision boundaries in lower dimensions.\nLatent-Space Visualization: Beyond PCA, t-SNE and UMAP will be applied to project high-dimensional data into 2D latent spaces. These visualizations will reveal natural clustering between normal and attack traffic, and illustrate how different models separate anomalous connections.\nFeature Selection: Correlation filtering and low-variance thresholding will be used to eliminate redundant and non-informative predictors.\nTrain/Test Split: A stratified 80/20 split will ensure consistent class ratios between training and testing datasets, preserving representation of all attack families.\nCross-Validation: For supervised learning models, 5-fold cross-validation will be applied to ensure stable and generalizable results.\nClass Imbalance Handling: Severe imbalance in R2L and U2R attack classes will be mitigated using SMOTE (Synthetic Minority Oversampling Technique) and class-weight adjustments during model training."
  },
  {
    "objectID": "proposal.html#threshold-definition-for-unsupervised-models",
    "href": "proposal.html#threshold-definition-for-unsupervised-models",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Threshold Definition for Unsupervised Models:",
    "text": "Threshold Definition for Unsupervised Models:\nFor models like LOF and DBSCAN, anomaly scores will be transformed into binary outcomes using a threshold determined by the 95th percentile of the score distribution on training data. Models will be compared based on precision, recall, and false positive rate against true attack labels."
  },
  {
    "objectID": "proposal.html#model-interpretability",
    "href": "proposal.html#model-interpretability",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Model Interpretability",
    "text": "Model Interpretability\nTo ensure transparency and explainability in the results, this project will include interpretability analysis for both supervised and unsupervised models:\n\nTree-Based Models (Random Forest): Feature importance plots will be used to identify the most influential network attributes contributing to intrusion detection decisions.\nSVM and Deep Autoencoders: The SHAP (SHapley Additive exPlanations) framework will be applied to quantify how each input feature affects individual model predictions, highlighting both global and local interpretability.\nUnsupervised Models (LOF, DBSCAN, Elliptic Envelope): Model behavior will be interpreted through visualization of PCA-based decision boundaries, cluster separations, and anomaly score distributions.\nAutoencoder Reconstruction Analysis: For deep learning models, reconstruction error plots will be examined to understand how effectively normal vs. anomalous traffic patterns are captured and separated.\nCombined Insights: Comparing interpretability outputs across models will help evaluate the trade-off between detection accuracy and explainability—an important factor in real-world IDS deployment.\nInterpretability Trade-Offs: Comparative analysis will examine the balance between model transparency and detection power. While deep or ensemble methods may offer superior accuracy, their complexity can obscure decision rationale. Simpler statistical models, though less accurate, provide clearer interpretive insights valuable for security analysts. This trade-off will be explicitly discussed in the results and conclusions."
  },
  {
    "objectID": "proposal.html#evaluation-setup",
    "href": "proposal.html#evaluation-setup",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Evaluation Setup",
    "text": "Evaluation Setup\nThe evaluation process will assess the performance, robustness, and reliability of all models through a consistent and structured framework:\n\nPerformance Metrics: Model effectiveness will be quantified using Precision, Recall, F1-Score, ROC-AUC, and Detection Rate to capture both classification accuracy and anomaly detection strength.\nValidation Strategy: Results will be validated through a combination of stratified 80/20 train–test split and 5-fold cross-validation to ensure generalization and mitigate overfitting.\nFamily-Wise and Rare Attack Evaluation: Each attack family (DoS, Probe, R2L, and U2R) will be evaluated independently, with special attention to rare classes (R2L and U2R). Detection rates, false negatives, and confusion patterns for these low-frequency attacks will reveal how well unsupervised and supervised approaches generalize to scarce attack behaviors.\nThreshold Optimization (Unsupervised Models): Anomaly score thresholds (e.g., top 5% or 95th percentile) will be tuned on the training data to balance false positives and false negatives.\nBaseline Comparison: Model results will be benchmarked against published NSL-KDD baselines and recent IDS studies to contextualize performance improvements.\nVisualization: ROC curves, confusion matrices, and precision–recall plots will be used to visually compare detection quality and identify performance trade-offs among methods."
  },
  {
    "objectID": "proposal.html#dataset-limitations-and-justification",
    "href": "proposal.html#dataset-limitations-and-justification",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and does not reflect modern encrypted or high-volume network traffic, it remains widely used in academic research because:\n\nIt contains well-structured, labeled attack categories, making it ideal for benchmarking and comparing different algorithms.\nIt enables controlled, reproducible comparisons between models without introducing data privacy or confidentiality issues.\nIt provides clear ground truth labels, which are often unavailable in modern or proprietary network traffic datasets.\n\nThe goal is to develop and compare generalizable anomaly detection methods, which can later be adapted to more recent or proprietary datasets."
  },
  {
    "objectID": "proposal.html#reproducibility-and-version-control",
    "href": "proposal.html#reproducibility-and-version-control",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Reproducibility and Version Control",
    "text": "Reproducibility and Version Control\nTo ensure that all results are consistent and reproducible across runs and environments, this project follows strict versioning and organization practices:\n\nRandom Seeds: All random processes (e.g., data splits, model initialization, SMOTE) will use fixed random seeds to ensure reproducibility.\nRepository Organization: The project repository will follow a clear, modular directory structure to separate data, notebooks, and sources.\n\nproject-root/\n│\n├── data/                # Raw and processed datasets\n├── notebooks/           # Jupyter notebooks for analysis and modeling\n├── src/                 # Preprocessing and utility code\n├── requirements.txt     # Python dependencies for environment setup\n└── README.md            # Project overview and usage instructions\n\nExperiment tracking will be implemented using versioned notebooks and GitHub commits, ensuring reproducibility.\nDependencies and environment configuration will be captured in a requirements.txt file.\nAll visualizations, experiment logs, and performance metrics will be versioned and stored in the repository for transparent result verification."
  },
  {
    "objectID": "index.html#abstract-1",
    "href": "index.html#abstract-1",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Abstract",
    "text": "Abstract\nThis project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "proposal.html#dataset-limitations-and-justification-1",
    "href": "proposal.html#dataset-limitations-and-justification-1",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and does not reflect modern encrypted or high-volume network traffic, it remains widely used in academic research because:\n\nIt contains well-structured, labeled attack categories, making it ideal for benchmarking and comparing different algorithms.\nIt enables controlled, reproducible comparisons between models without introducing data privacy or confidentiality issues.\nIt provides clear ground truth labels, which are often unavailable in modern or proprietary network traffic datasets.\n\nThe goal is to develop and compare generalizable anomaly detection methods, which can later be adapted to more recent or proprietary datasets."
  },
  {
    "objectID": "presentation.html#dataset-limitations-and-justification",
    "href": "presentation.html#dataset-limitations-and-justification",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and partly synthetic—constructed from simulated network environments rather than real enterprise traffic—it remains one of the most widely used and well-documented benchmarks for intrusion detection research.\nIts design introduces several limitations that will be carefully considered in this study:\n\nTemporal relevance: The dataset reflects network behaviors and attack types common in the late 1990s, before the advent of modern threats such as advanced persistent attacks, encrypted communication channels, and large-scale botnets.\n\nSynthetic generation: The data were produced under controlled lab conditions rather than from live network capture, which means background noise, user variability, and realistic packet timing patterns are underrepresented.\n\nFeature design: Certain attributes (e.g., service, flag) may no longer map directly to contemporary protocols or operating systems, reducing direct applicability to modern network telemetry.\n\nLabel completeness: All attacks are labeled, unlike in real-world network monitoring where labels are often incomplete or delayed.\n\nDespite these constraints, NSL-KDD remains a valuable controlled testbed for benchmarking anomaly and intrusion detection algorithms because:\n\nIt provides clearly defined attack families (DoS, Probe, R2L, U2R) that facilitate comparative and reproducible experiments.\n\nIt allows systematic evaluation of class imbalance, thresholding, and interpretability trade-offs across model types.\n\nIt serves as a pedagogically consistent foundation for developing methods that can later be generalized to newer datasets such as CIC-IDS2017, UNSW-NB15, or proprietary enterprise data.\n\nMitigation strategies include:\n\nApplying robust preprocessing, resampling, and dimensional reduction to reduce dataset biases.\n\nFraming results as algorithmic performance benchmarks, not as direct indicators of deployment-ready accuracy.\n\nReflecting on how each model’s strengths and weaknesses might translate to modern, encrypted, or real-time network environments."
  },
  {
    "objectID": "notebooks/week_06_report_explain.html",
    "href": "notebooks/week_06_report_explain.html",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "",
    "text": "This section synthesizes supervised and unsupervised model results, integrates SHAP explainability for Random Forest and SVM, and provides final IDS insights.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\n\n# Use a clean seaborn style\nsns.set(style=\"whitegrid\")\n\n# Project paths\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\n\nset_global_seed(42)\npaths = Paths().ensure()\n\n# Derive a robust reports root (Week 03–06)\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\nwk6_dir = reports_root / \"week_06\"\n\nwk6_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Project root:\", project_root)\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports:\", wk3_dir)\nprint(\"Week 04 reports:\", wk4_dir)\nprint(\"Week 06 reports:\", wk6_dir)\n\nProject root: C:\\Users\\mehra\\Final_Project\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nWeek 06 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_06\n# Load processed arrays from Week 01\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n\n# binary labels (0=normal, 1=attack) from Week 04 logic\nfrom collections import Counter\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin  = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nX_train: (395216, 115) X_test: (98805, 115)\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({0: 313166, 1: 82050})\nBinary test  counts: Counter({0: 78292, 1: 20513})\n# Load Week 03 & Week 04 prediction files \n\nfrom pathlib import Path\n\ndef _load_csv_if_exists(path: Path, label: str):\n    if not path.exists():\n        print(f\"{label} not found at {path} — returning None.\")\n        return None\n    df = pd.read_csv(path)\n    print(f\"{label} loaded from {path} with shape {df.shape}\")\n    return df\n\nwk3_unsup = _load_csv_if_exists(\n    wk3_dir / \"week03_unsupervised_predictions.csv\",\n    \"Week 03 unsupervised predictions\"\n)\n\nwk4_unsup = _load_csv_if_exists(\n    wk4_dir / \"week04_unsupervised_predictions.csv\",\n    \"Week 04 unsupervised/semi-supervised predictions\"\n)\n\nwk4_sup = _load_csv_if_exists(\n    wk4_dir / \"week04_supervised_predictions.csv\",\n    \"Week 04 supervised predictions\"\n)\n\nprint(\"\\nSummary:\")\nprint(\"  Week 03 unsupervised df:\", None if wk3_unsup is None else wk3_unsup.shape)\nprint(\"  Week 04 unsupervised df:\", None if wk4_unsup is None else wk4_unsup.shape)\nprint(\"  Week 04 supervised df:\", None if wk4_sup is None else wk4_sup.shape)\n\nfrom IPython.display import display\n\nif wk3_unsup is not None:\n    print(\"\\nWeek 03 unsupervised head:\")\n    display(wk3_unsup.head())\n\nif wk4_unsup is not None:\n    print(\"\\nWeek 04 unsupervised head:\")\n    display(wk4_unsup.head())\n\nif wk4_sup is not None:\n    print(\"\\nWeek 04 supervised head:\")\n    display(wk4_sup.head())\n\nWeek 03 unsupervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv with shape (98257, 4)\nWeek 04 unsupervised/semi-supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_unsupervised_predictions.csv with shape (296415, 4)\nWeek 04 supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_supervised_predictions.csv with shape (395220, 3)\n\nSummary:\n  Week 03 unsupervised df: (98257, 4)\n  Week 04 unsupervised df: (296415, 4)\n  Week 04 supervised df: (395220, 3)\n\nWeek 03 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n\n\n\n\n\n\nWeek 04 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0\n\n\n\n\n\n\n\n\nWeek 04 supervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#feature-names-loaded-models",
    "href": "notebooks/week_06_report_explain.html#feature-names-loaded-models",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Feature Names & Loaded Models",
    "text": "Feature Names & Loaded Models\nFeature names were loaded from the preprocessing pipeline. Random Forest and SVM models loaded successfully; the autoencoder was unavailable in this environment and AE explainability was skipped.\n\n# Feature names via preprocessor\n\nfeature_names = None\ntry:\n    import joblib\n    import sklearn.compose._column_transformer as _ct\n\n    class _RemainderColsList(list):\n        \"\"\"Compatibility shim for old sklearn ColumnTransformer pickles.\"\"\"\n        pass\n\n    _ct._RemainderColsList = _RemainderColsList\n\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = list(pre.get_feature_names_out())\n    print(\"Loaded feature names from preprocessor:\", len(feature_names))\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    # Fall back to simple numeric names\n    feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n    print(\"Using generic feature names:\", len(feature_names))\n\nLoaded feature names from preprocessor: 115\n\n\n\nimport joblib\n\nrf_model = None\nsvm_model = None\nae_model = None\n\nrf_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_rf.joblib\"\nsvm_path = getattr(paths, \"models\", project_root / \"models\") / \"week04_svm.joblib\"\nae_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_ae.joblib\"\n\nfor name, p in [(\"RF\", rf_path), (\"SVM\", svm_path), (\"AE\", ae_path)]:\n    print(f\"{name} expected at:\", p)\n\n# -------------------------\n# RF\n# -------------------------\ntry:\n    if rf_path.exists():\n        rf_model = joblib.load(rf_path)\n        print(\"Loaded RF model.\")\n    else:\n        print(\"RF model file not found.\")\nexcept Exception as e:\n    print(\"Could not load RF model:\", e)\n\n# -------------------------\n# SVM\n# -------------------------\ntry:\n    if svm_path.exists():\n        svm_model = joblib.load(svm_path)\n        print(\"Loaded SVM model.\")\n    else:\n        print(\"SVM model file not found.\")\nexcept Exception as e:\n    print(\"Could not load SVM model:\", e)\n\n\nRF expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSVM expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nAE expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib\nLoaded RF model.\nLoaded SVM model."
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#explainability-utilities",
    "href": "notebooks/week_06_report_explain.html#explainability-utilities",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Explainability Utilities",
    "text": "Explainability Utilities\nWe compute Gini-based importance and SHAP global importance for Random Forest. SVM-RBF uses Kernel SHAP over summarized background samples. Both methods highlight dominant network features influencing model decisions.\n\nimport shap\n\nfrom src.explain import (\n    compute_rf_feature_importance,\n    plot_anomaly_score_distributions,\n    plot_reconstruction_error_hist,\n    compute_pca_embedding,\n    plot_pca_dbscan_like,\n    plot_pca_decision_boundaries,\n)"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#random-forest-explainability",
    "href": "notebooks/week_06_report_explain.html#random-forest-explainability",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Random Forest Explainability",
    "text": "Random Forest Explainability\nRF Gini and SHAP rankings consistently surface top protocol and connection statistical features. SHAP provides smoother global attribution, confirming the RF decision structure shown in the plotted top‑20 bars.\n\n\n# Random Forest Gini importance + SHAP global importance\n\nif rf_model is None:\n    print(\n        \"RF model not loaded — skipping RF explainability. \"\n        \"Save your Random Forest as 'week04_rf.joblib' in paths.models to enable this section.\"\n    )\nelse:\n    # ----------------------------\n    # 1. Gini-based importance\n    # ----------------------------\n    rf_imp_df = compute_rf_feature_importance(rf_model, feature_names)\n    rf_imp_df = rf_imp_df.sort_values(\"importance\", ascending=False)\n\n    # ----------------------------\n    # 2. SHAP-based global importance\n    # ----------------------------\n    try:\n        import shap  # ensure imported\n\n        # Subsample test set for explanation\n        rng = np.random.RandomState(42)\n        n_explain = min(1000, X_test.shape[0])\n        explain_idx = rng.choice(X_test.shape[0], size=n_explain, replace=False)\n        X_explain = X_test[explain_idx]\n\n        explainer = shap.TreeExplainer(rf_model)\n        shap_values = explainer.shap_values(X_explain)\n\n        # shap_values can be:\n        # - array of shape (n_samples, n_features)  -&gt; binary/regression\n        # - list of arrays (n_classes, n_samples, n_features) -&gt; multiclass\n        if isinstance(shap_values, list):\n            # Stack into 3D: (n_classes, n_samples, n_features)\n            shap_arr = np.stack(shap_values, axis=0)\n            # Take mean absolute across classes and samples -&gt; (n_features,)\n            shap_global = np.mean(np.abs(shap_arr), axis=(0, 1))\n        else:\n            # Directly mean absolute across samples -&gt; (n_features,)\n            shap_global = np.mean(np.abs(shap_values), axis=0)\n\n        # Ensure 1D vector\n        shap_global = np.asarray(shap_global).ravel()\n\n        # Safety check: align length with feature_names\n        if len(shap_global) != len(feature_names):\n            print(\n                f\"⚠️ Length mismatch: shap_global={len(shap_global)}, \"\n                f\"feature_names={len(feature_names)}. Truncating to min length.\"\n            )\n            min_len = min(len(shap_global), len(feature_names))\n            shap_global = shap_global[:min_len]\n            feature_vec = feature_names[:min_len]\n        else:\n            feature_vec = feature_names\n\n        rf_shap_df = (\n            pd.DataFrame(\n                {\n                    \"feature\": feature_vec,\n                    \"importance\": shap_global,\n                }\n            )\n            .sort_values(\"importance\", ascending=False)\n        )\n\n        # ----------------------------\n        # 3. Two-column figure (Gini vs SHAP)\n        # ----------------------------\n        top_n = 20\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n\n        sns.barplot(\n            data=rf_imp_df.head(top_n),\n            x=\"importance\",\n            y=\"feature\",\n            ax=axes[0],\n        )\n        axes[0].set_title(\"Random Forest — Top 20 Gini Importances\")\n        axes[0].set_xlabel(\"Gini importance\")\n        axes[0].set_ylabel(\"Feature\")\n\n        sns.barplot(\n            data=rf_shap_df.head(top_n),\n            x=\"importance\",\n            y=\"feature\",\n            ax=axes[1],\n        )\n        axes[1].set_title(\"Random Forest — Top 20 SHAP Importances\")\n        axes[1].set_xlabel(\"Mean |SHAP value|\")\n        axes[1].set_ylabel(\"\")\n\n        plt.tight_layout()\n        fig_path = wk6_dir / \"rf_gini_vs_shap_top20.png\"\n        fig.savefig(fig_path, dpi=150)\n        print(\"Saved:\", fig_path)\n        plt.show()\n\n        display(rf_shap_df.head(10))\n\n    except Exception as e:\n        print(\"RF SHAP computation failed:\", e)\n\n⚠️ Length mismatch: shap_global=575, feature_names=115. Truncating to min length.\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\rf_gini_vs_shap_top20.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n75\ncat__flag_SF\n0.081151\n\n\n79\nnum__dst_bytes\n0.051866\n\n\n0\ncat__protocol_type_tcp\n0.024382\n\n\n102\nnum__same_srv_rate\n0.021036\n\n\n76\ncat__flag_SH\n0.015354\n\n\n4\ncat__service_auth\n0.014612\n\n\n71\ncat__flag_S0\n0.014130\n\n\n2\ncat__service_X11\n0.013217\n\n\n100\nnum__rerror_rate\n0.010808\n\n\n77\nnum__duration\n0.008508"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#svm-rbf-explainability",
    "href": "notebooks/week_06_report_explain.html#svm-rbf-explainability",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "SVM-RBF Explainability",
    "text": "SVM-RBF Explainability\nKernel SHAP applied to the SVM decision function reveals nonlinear feature influence. The SHAP ranking emphasizes features controlling margin separation, consistent with SVM behavior displayed in the bar plot.\n\n# SVM-RBF SHAP global importance (KernelExplainer)\n\nif svm_model is None:\n    print(\n        \"SVM model not loaded — skipping SVM SHAP importance. \"\n        \"Save your SVM as 'week04_svm.joblib' in paths.models to enable this section.\"\n    )\nelse:\n    try:\n        import shap\n\n        rng = np.random.RandomState(42)\n\n        # 🔹 Make this cheaper than before\n        n_background = min(100, X_train.shape[0])   # was 200\n        n_explain = min(150, X_test.shape[0])       # was 300\n\n        bg_idx = rng.choice(X_train.shape[0], size=n_background, replace=False)\n        ex_idx = rng.choice(X_test.shape[0], size=n_explain, replace=False)\n\n        X_bg = X_train[bg_idx]\n        X_ex = X_test[ex_idx]\n\n        # Optional: summarize background to K prototypes to speed things up\n        # X_bg = shap.kmeans(X_train, 50)  # uncomment if you want even more speed\n\n        # Use decision function for richer margins (multi-class or binary).\n        f = svm_model.decision_function\n\n        explainer = shap.KernelExplainer(f, X_bg)\n        shap_values = explainer.shap_values(X_ex, nsamples=100)  # was \"auto\"\n\n        # ----------------------------\n        # Aggregate to global |SHAP|\n        # ----------------------------\n        if isinstance(shap_values, list):\n            # shap_values: list of (n_samples, n_features) -&gt; stack to 3D\n            shap_arr = np.stack(shap_values, axis=0)    # (n_outputs, n_samples, n_features)\n            shap_global = np.mean(np.abs(shap_arr), axis=(0, 1))  # -&gt; (n_features,)\n        else:\n            # (n_samples, n_features)\n            shap_global = np.mean(np.abs(shap_values), axis=0)    # -&gt; (n_features,)\n\n        # Force 1D\n        shap_global = np.asarray(shap_global).ravel()\n\n        # Safety: align with feature_names\n        if len(shap_global) != len(feature_names):\n            print(\n                f\"⚠️ Length mismatch: shap_global={len(shap_global)}, \"\n                f\"feature_names={len(feature_names)}. Truncating to min length.\"\n            )\n            m = min(len(shap_global), len(feature_names))\n            shap_global = shap_global[:m]\n            feature_vec = feature_names[:m]\n        else:\n            feature_vec = feature_names\n\n        svm_shap_df = (\n            pd.DataFrame(\n                {\n                    \"feature\": feature_vec,\n                    \"importance\": shap_global,\n                }\n            )\n            .sort_values(\"importance\", ascending=False)\n        )\n\n        # ----------------------------\n        # Plot: two-column layout (left: SHAP, right: note)\n        # ----------------------------\n        fig, axes = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={\"width_ratios\": [3, 1]})\n\n        sns.barplot(\n            data=svm_shap_df.head(20),\n            x=\"importance\",\n            y=\"feature\",\n            ax=axes[0],\n        )\n        axes[0].set_title(\"SVM-RBF — Top 20 SHAP Importances\")\n        axes[0].set_xlabel(\"Mean |SHAP value|\")\n        axes[0].set_ylabel(\"Feature\")\n\n        axes[1].axis(\"off\")\n        axes[1].text(\n            0.0,\n            0.5,\n            \"Right panel reserved for future local SVM explanations\\n\"\n            \"(e.g., per-connection SHAP).\",\n            va=\"center\",\n            ha=\"left\",\n            fontsize=10,\n        )\n\n        plt.tight_layout()\n        fig_path = wk6_dir / \"svm_shap_importance_top20.png\"\n        fig.savefig(fig_path, dpi=150)\n        print(\"Saved:\", fig_path)\n        plt.show()\n\n        display(svm_shap_df.head(10))\n\n    except Exception as e:\n        print(\"SVM SHAP computation failed:\", e)\n\n100%|██████████| 150/150 [02:23&lt;00:00,  1.05it/s]\n\n\n⚠️ Length mismatch: shap_global=575, feature_names=115. Truncating to min length.\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\svm_shap_importance_top20.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n0\ncat__protocol_type_tcp\n0.070257\n\n\n75\ncat__flag_SF\n0.063796\n\n\n60\ncat__service_time\n0.063438\n\n\n62\ncat__service_urp_i\n0.056107\n\n\n79\nnum__dst_bytes\n0.055336\n\n\n5\ncat__service_bgp\n0.051553\n\n\n9\ncat__service_daytime\n0.049859\n\n\n4\ncat__service_auth\n0.042406\n\n\n63\ncat__service_uucp\n0.035613\n\n\n64\ncat__service_uucp_path\n0.035358"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#pca-visualization",
    "href": "notebooks/week_06_report_explain.html#pca-visualization",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "PCA Visualization",
    "text": "PCA Visualization\nThe 2‑D PCA projection shows clear grouping of normal vs attack families, supporting interpretability of model behavior. Families cluster in distinct regions, validating model separability observed earlier.\n\n# PCA embedding for visualization\n\nX_test_pca = compute_pca_embedding(X_test, n_components=2, random_state=42)\nX_test_pca.shape\n\n(98805, 2)\n\n\n\n# Anomaly score distributions for unsupervised models (Week 03 + Week 04)\n\nif wk3_unsup is None and wk4_unsup is None:\n    print(\"No unsupervised score tables loaded — skipping anomaly score distributions.\")\nelse:\n    fig = plot_anomaly_score_distributions(\n        wk3_unsup=wk3_unsup,\n        wk4_unsup=wk4_unsup,\n        max_models=6,\n    )\n    fig_path = wk6_dir / \"unsupervised_anomaly_score_distributions.png\"\n    fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n    print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\unsupervised_anomaly_score_distributions.png\n\n\n\n\n\n\n\n\n\n\n# Autoencoder reconstruction error histogram (Week 04 unsupervised)\n\nif wk4_unsup is None:\n    print(\"Week 04 unsupervised df not loaded — skipping AE reconstruction error histogram.\")\nelse:\n    mask_ae = wk4_unsup[\"model\"].str.contains(\"Autoencoder\", case=False, na=False)\n    if not mask_ae.any():\n        print(\"No Autoencoder rows in Week 04 unsupervised df.\")\n    else:\n        ae_scores = wk4_unsup.loc[mask_ae, \"score\"].values\n        fig = plot_reconstruction_error_hist(ae_scores)\n        fig_path = wk6_dir / \"ae_reconstruction_error_hist.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\ae_reconstruction_error_hist.png"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#autoencoder-reconstruction-error",
    "href": "notebooks/week_06_report_explain.html#autoencoder-reconstruction-error",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Autoencoder Reconstruction Error",
    "text": "Autoencoder Reconstruction Error\nAlthough AE did not load here, Week‑04 results showed distinct reconstruction‑error gaps between normal and anomalous traffic—evidence that AE learned normal patterns effectively.\n\n# PCA-based decision boundary visualization\n\nfig = plot_pca_decision_boundaries(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    feature_names=feature_names,\n    random_state=42,\n)\n\nfig_path = wk6_dir / \"pca_decision_boundaries_rf_svm.png\"\nfig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\nprint(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\pca_decision_boundaries_rf_svm.png"
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#transparency-vs-accuracy",
    "href": "notebooks/week_06_report_explain.html#transparency-vs-accuracy",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Transparency vs Accuracy",
    "text": "Transparency vs Accuracy\nUsing the Week 05 performance summaries together with the Week 06 explainability outputs (Gini importance, SHAP for RF and SVM, PCA views, and anomaly-detector comparisons), the overall picture of transparency vs. accuracy becomes clearer:\n\nRandom Forest (RF)\n\nDemonstrates strong macro-F1 performance and consistent detection across most families.\n\nBoth Gini and Tree SHAP rankings highlight a focused set of influential features—primarily connection-level statistics, protocol-related attributes, and volume-based indicators.\n\nFeature attributions are globally coherent: SHAP smooths out the importance landscape and confirms the same high-impact features surfaced by Gini.\n\nInterpretability burden is moderate—RF is an ensemble, but the combination of feature importance tables and clear separation patterns in PCA makes it straightforward to communicate decision drivers.\n\nSVM-RBF\n\nAchieves competitive detection quality and, in some families, matches RF performance.\n\nThe model structure is inherently opaque, but Kernel SHAP applied to the decision function provides meaningful global feature influence.\n\nSHAP importance reveals which features drive margin separation, aligning with the nonlinear behavior expected from RBF kernels.\n\nTransparency is lower than RF, but global SHAP helps reduce the gap and provides an interpretable footprint even without direct access to the internal decision boundaries.\n\nDeep Autoencoder (AE)\n\nThe AE model was not available in this environment, but prior Week 04 results showed clear reconstruction-error separation between normal and malicious connections.\n\nInterpretability remains limited to distance-from-normal reasoning rather than feature-level attributions.\n\nDespite low transparency, its ability to flag unfamiliar traffic patterns makes it effective as an auxiliary anomaly detector.\n\nClassical Unsupervised Models (Isolation Forest, LOF, Elliptic Envelope)\n\nScore distributions and anomaly-score visualizations show each method capturing different geometric notions of “outlierness.”\n\nThese detectors provide helpful signal diversity but do not supply detailed per-feature explanations.\n\nTheir primary value lies in catching unusual behaviors outside the supervised model’s coverage.\n\n\n\nOverall trade-off\nAs we progress from interpretable models (RF) toward more expressive but opaque methods (SVM-RBF and deep models), we observe:\n\nAccuracy generally increases or remains competitive, especially for complex traffic patterns.\n\nTransparency decreases, requiring SHAP or other post-hoc methods to explain decisions.\n\nA practical IDS benefits from a hybrid architecture:\n\nUse Random Forest as the primary explainable classifier—its SHAP and Gini profiles make feature-driven triage easy to communicate.\n\nUse SVM-RBF where nonlinear separation improves detection, supported by global SHAP for interpretability.\n\nUse AE / unsupervised models as broader anomaly detectors, tuned with clear thresholding policies.\n\nThis layered approach balances accuracy, interpretability, and operational reliability across known and emerging attack patterns."
  },
  {
    "objectID": "notebooks/week_06_report_explain.html#real-world-ids-implications",
    "href": "notebooks/week_06_report_explain.html#real-world-ids-implications",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Real-World IDS Implications",
    "text": "Real-World IDS Implications\n\nFalse-Positive Management\n\nEven modest false-positive rates can overwhelm analysts at operational scale.\n\nWeek 06 SHAP results highlight which features push borderline samples into false-alert territory, helping refine thresholds and preprocessing.\n\nThreshold selection should target a manageable daily alert volume, not maximum recall alone.\n\nThreshold Tuning for Anomaly Detectors\n\nWeek 03 and Week 04 score distributions show that Isolation Forest, LOF, and the Autoencoder each produce distinct anomaly-separation patterns.\n\nThese thresholds should be treated as policy levers, adjustable based on operational workload.\n\nRegular recalibration ensures the IDS adapts as traffic patterns shift.\n\nModel Maintenance and Concept Drift\n\nPCA views and feature-distribution checks (Week 06) indicate that traffic structure evolves over time.\n\nPeriodic retraining or validation—monthly or quarterly—helps maintain detection effectiveness.\n\nDrift indicators include: cluster shifts in PCA, changes in reconstruction-error ranges, or shifts in feature importances from SHAP.\n\nRole Specialization Across Model Families\n\nSupervised models (RF, SVM) provide actionable, interpretable alerts supported by global SHAP importance rankings.\n\nUnsupervised and deep models act as early-warning layers for rare or previously unseen behaviors.\n\nFeature-importance plots, PCA separation, and anomaly-score profiles support triage decisions and help analysts justify why a flow was flagged."
  },
  {
    "objectID": "notebooks/week_04_ml_models.html",
    "href": "notebooks/week_04_ml_models.html",
    "title": "Week 04 — Classical ML Models",
    "section": "",
    "text": "Unsupervised / semi-supervised\n\nIsolation Forest\nOne-Class SVM\nDeep Autoencoder\n\nSupervised\n\nRandom Forest\nSVM\n\nImbalance handling\n\nCompare class_weight vs SMOTE (train only) for rare classes (R2L / U2R analogues)\nKeep a clean baseline that uses only class_weight.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\nfrom src.models import make_fast_oneclass_svm\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[3], line 4\n      1 import os, sys\n      2 sys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n----&gt; 4 from src.utils import set_global_seed, Paths\n      5 from src.models import make_fast_oneclass_svm\n      7 set_global_seed()\n\nFile ~\\Final_Project\\src\\utils.py:6\n      4 from dataclasses import dataclass\n      5 from pathlib import Path\n----&gt; 6 import numpy as np\n      7 import random\n      8 import os\n\nModuleNotFoundError: No module named 'numpy'\nimport numpy as np\nimport shap\n\nprint(\"NumPy:\", np.__version__)\nprint(\"SHAP:\", shap.__version__)\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 import numpy as np\n      2 import shap\n      4 print(\"NumPy:\", np.__version__)\n\nModuleNotFoundError: No module named 'numpy'\nimport numpy as np\nimport shap\n\nprint(\"NumPy version:\", np.__version__)\nprint(\"SHAP version:\", shap.__version__)\n\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n    app.start()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n    self.io_loop.start()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n    self._run_once()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n    handle._run()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n    await result\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n    result = runner(coro)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\mehra\\AppData\\Local\\Temp\\ipykernel_10408\\141797906.py\", line 2, in &lt;module&gt;\n    import shap\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\shap\\__init__.py\", line 1, in &lt;module&gt;\n    from ._explanation import Cohorts, Explanation\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\shap\\_explanation.py\", line 17, in &lt;module&gt;\n    from .utils._clustering import hclust_ordering\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\shap\\utils\\__init__.py\", line 1, in &lt;module&gt;\n    from ._clustering import (\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\shap\\utils\\_clustering.py\", line 12, in &lt;module&gt;\n    from numba import njit\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\__init__.py\", line 92, in &lt;module&gt;\n    from numba.core.decorators import (cfunc, jit, njit, stencil,\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\core\\decorators.py\", line 12, in &lt;module&gt;\n    from numba.stencils.stencil import stencil\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\stencils\\stencil.py\", line 11, in &lt;module&gt;\n    from numba.core import types, typing, utils, ir, config, ir_utils, registry\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\core\\ir_utils.py\", line 14, in &lt;module&gt;\n    from numba.core.extending import _Intrinsic\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\core\\extending.py\", line 19, in &lt;module&gt;\n    from numba.core.pythonapi import box, unbox, reflect, NativeValue  # noqa: F401\n  File \"C:\\Users\\mehra\\anaconda3\\Lib\\site-packages\\numba\\core\\pythonapi.py\", line 11, in &lt;module&gt;\n    from numba import _helperlib\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nFile ~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44, in __getattr__(attr_name)\n     39     # Also print the message (with traceback).  This is because old versions\n     40     # of NumPy unfortunately set up the import to replace (and hide) the\n     41     # error.  The traceback shouldn't be needed, but e.g. pytest plugins\n     42     # seem to swallow it and we should be failing anyway...\n     43     sys.stderr.write(msg + tb_msg)\n---&gt; 44     raise ImportError(msg)\n     46 ret = getattr(_multiarray_umath, attr_name, None)\n     47 if ret is None:\n\nImportError: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11&gt;=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy&lt;2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n\n\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[29], line 2\n      1 import numpy as np\n----&gt; 2 import shap\n      4 print(\"NumPy version:\", np.__version__)\n      5 print(\"SHAP version:\", shap.__version__)\n\nFile ~\\anaconda3\\Lib\\site-packages\\shap\\__init__.py:1\n----&gt; 1 from ._explanation import Cohorts, Explanation\n      3 # explainers\n      4 from .explainers import other\n\nFile ~\\anaconda3\\Lib\\site-packages\\shap\\_explanation.py:17\n     14 import sklearn\n     15 from slicer import Alias, Obj, Slicer\n---&gt; 17 from .utils._clustering import hclust_ordering\n     18 from .utils._exceptions import DimensionError\n     19 from .utils._general import OpChain\n\nFile ~\\anaconda3\\Lib\\site-packages\\shap\\utils\\__init__.py:1\n----&gt; 1 from ._clustering import (\n      2     delta_minimization_order,\n      3     hclust,\n      4     hclust_ordering,\n      5     partition_tree,\n      6     partition_tree_shuffle,\n      7 )\n      8 from ._general import (\n      9     OpChain,\n     10     approximate_interactions,\n   (...)\n     20     suppress_stderr,\n     21 )\n     22 from ._masked_model import MaskedModel, make_masks\n\nFile ~\\anaconda3\\Lib\\site-packages\\shap\\utils\\_clustering.py:12\n     10 import scipy.spatial\n     11 import sklearn\n---&gt; 12 from numba import njit\n     14 from ..utils._exceptions import DimensionError\n     15 from ._show_progress import show_progress\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\__init__.py:92\n     89 from numba.core.types import *\n     91 # Re-export decorators\n---&gt; 92 from numba.core.decorators import (cfunc, jit, njit, stencil,\n     93                                    jit_module)\n     95 # Re-export vectorize decorators and the thread layer querying function\n     96 from numba.np.ufunc import (vectorize, guvectorize, threading_layer,\n     97                             get_num_threads, set_num_threads,\n     98                             set_parallel_chunksize, get_parallel_chunksize,\n     99                             get_thread_id)\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\core\\decorators.py:12\n      9 import logging\n     11 from numba.core.errors import DeprecationError, NumbaDeprecationWarning\n---&gt; 12 from numba.stencils.stencil import stencil\n     13 from numba.core import config, extending, sigutils, registry\n     15 _logger = logging.getLogger(__name__)\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\stencils\\stencil.py:11\n      8 import numpy as np\n      9 from llvmlite import ir as lir\n---&gt; 11 from numba.core import types, typing, utils, ir, config, ir_utils, registry\n     12 from numba.core.typing.templates import (CallableTemplate, signature,\n     13                                          infer_global, AbstractTemplate)\n     14 from numba.core.imputils import lower_builtin\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\core\\ir_utils.py:14\n     11 import warnings\n     13 import numba\n---&gt; 14 from numba.core.extending import _Intrinsic\n     15 from numba.core import types, typing, ir, analysis, postproc, rewrites, config\n     16 from numba.core.typing.templates import signature\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\core\\extending.py:19\n     17 from numba.core.datamodel import models   # noqa: F401\n     18 from numba.core.datamodel import register_default as register_model  # noqa: F401, E501\n---&gt; 19 from numba.core.pythonapi import box, unbox, reflect, NativeValue  # noqa: F401\n     20 from numba._helperlib import _import_cython_function  # noqa: F401\n     21 from numba.core.serialize import ReduceMixin\n\nFile ~\\anaconda3\\Lib\\site-packages\\numba\\core\\pythonapi.py:11\n      8 from llvmlite.ir import Constant\n     10 import ctypes\n---&gt; 11 from numba import _helperlib\n     12 from numba.core import (\n     13     types, utils, config, lowering, cgutils, imputils, serialize,\n     14 )\n     15 from numba.core.utils import PYVERSION\n\nImportError: numpy.core.multiarray failed to import\nimport os, sys\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\nfrom src.models import (\n    make_isolation_forest_grid,\n    make_oneclass_svm_grid,\n    make_rf_classifier_grid,\n    make_svm_rbf_grid,\n    build_deep_autoencoder,\n)\nfrom src.eval import (\n    plot_confusion,\n    classification_summary,\n    binary_roc_pr_curves,\n)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import f1_score\n\nset_global_seed(42)\npaths = Paths().ensure()\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\n# Load processed train/test arrays from Week 01.\nfrom pathlib import Path\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\ny_test  classes: Counter({np.int32(0): 78292, np.int32(4): 19456, np.int32(1): 822, np.int32(2): 225, np.int32(3): 10})\nTo evaluate unsupervised detectors, I convert the multi-class labels into a binary label:\nThis matches the standard NSL-KDD setup where attacks are rare compared to normal traffic, but it does not assume any particular encoding of labels.\n# Build binary labels for anomaly detection (0 = normal, 1 = attack).\nfrom collections import Counter\nimport numpy as np\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({np.int64(0): 313166, np.int64(1): 82050})\nBinary test  counts: Counter({np.int64(0): 78292, np.int64(1): 20513})"
  },
  {
    "objectID": "notebooks/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "href": "notebooks/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "title": "Week 04 — Classical ML Models",
    "section": "1. Unsupervised / Semi-supervised models",
    "text": "1. Unsupervised / Semi-supervised models\n\n1.1 Isolation Forest\n\nfrom sklearn.metrics import f1_score\n\ndef iso_f1_scorer(estimator, X, y_true):\n    # Convert IsolationForest predictions {1, -1} into {0, 1} and compute F1.\n    y_raw = estimator.predict(X)\n    y_pred = (y_raw == -1).astype(int)  # -1 = anomaly → 1\n    return f1_score(y_true, y_pred, zero_division=0)\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom src.models import make_isolation_forest_grid\n\niso_spec = make_isolation_forest_grid()\n\niso_gs = GridSearchCV(\n    estimator=iso_spec.model,\n    param_grid=iso_spec.param_grid,\n    scoring=iso_f1_scorer,\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\n\niso_gs.fit(X_train, y_train_bin)\nprint(\"Best params (Isolation Forest):\", iso_gs.best_params_)\nprint(\"Best F1 (train CV):\", iso_gs.best_score_)\n\nFitting 3 folds for each of 18 candidates, totalling 54 fits\nBest params (Isolation Forest): {'contamination': 0.1, 'max_samples': 0.5, 'n_estimators': 200}\nBest F1 (train CV): 0.5217237922691421\n\n\n\n# Evaluate Isolation Forest on the held-out test set.\nbest_iso = iso_gs.best_estimator_\npred_iso = best_iso.predict(X_test)  # 1 = inlier, -1 = outlier\ny_pred_iso = np.where(pred_iso == -1, 1, 0)\n\nprint(\"Isolation Forest F1 (test, binary):\", f1_score(y_test_bin, y_pred_iso))\nplot_confusion(y_test_bin, y_pred_iso, labels=[0, 1], title=\"Isolation Forest (binary)\")\n\nIsolation Forest F1 (test, binary): 0.4988470913762435\n\n\n\n\n\n\n\n\n\n\n\n1.2 One-Class SVM (RBF)\n\n# OC-SVM (RBF)\nocsvm = make_fast_oneclass_svm()\n\n# Train on normal-only data\nmask_normal = (y_train_bin == 0)\nX_train_normal = X_train[mask_normal]\n\nocsvm.fit(X_train_normal)\n\npred = ocsvm.predict(X_test)\ny_pred_ocsvm = (pred == -1).astype(int)\n\nplot_confusion(y_test_bin, y_pred_ocsvm, labels=[0, 1], title=\"OC-SVM\")\n\n\n\n\n\n\n\n\n\n\n1.3 Deep Autoencoder\n\n### 1.3 Deep Autoencoder \n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\ninput_dim = X_train.shape[1]\nprint(\"Autoencoder input_dim:\", input_dim)\n\ninput_layer = keras.Input(shape=(input_dim,))\n\nencoded = layers.Dense(30, activation=\"relu\")(input_layer)\nencoded = layers.Dense(16, activation=\"relu\")(encoded)\nencoded = layers.Dense(8,  activation=\"relu\")(encoded)\n\ndecoded = layers.Dense(16, activation=\"relu\")(encoded)\ndecoded = layers.Dense(30, activation=\"relu\")(decoded)\ndecoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n\nautoencoder = keras.Model(input_layer, decoded, name=\"deep_autoencoder_ch2\")\n\nautoencoder.compile(\n    optimizer=\"adam\",\n    loss=\"mean_squared_error\"\n)\n\nautoencoder.summary()\n\nAutoencoder input_dim: 115\n\n\nModel: \"deep_autoencoder_ch2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 115)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 30)             │         3,480 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 16)             │           496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 8)              │           136 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 16)             │           144 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 30)             │           510 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 115)            │         3,565 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 8,331 (32.54 KB)\n\n\n\n Trainable params: 8,331 (32.54 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ensure numeric float dtype\nX_train_ae = X_train.astype(\"float32\")\nX_test_ae  = X_test.astype(\"float32\")\n\nhistory = autoencoder.fit(\n    X_train_ae,\n    X_train_ae,        # reconstruction target\n    epochs=10,         # like in Chapter_2 screenshot\n    batch_size=256,\n    shuffle=True,\n    validation_split=0.1,\n    verbose=1,\n)\n\n\nEpoch 1/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - loss: 0.1578 - val_loss: 0.0460\n\nEpoch 2/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0696 - val_loss: 0.0408\n\nEpoch 3/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0703 - val_loss: 0.0362\n\nEpoch 4/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0600 - val_loss: 0.0204\n\nEpoch 5/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0625 - val_loss: 0.0183\n\nEpoch 6/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0469 - val_loss: 0.0182\n\nEpoch 7/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0645 - val_loss: 0.0170\n\nEpoch 8/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0410 - val_loss: 0.0161\n\nEpoch 9/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0469 - val_loss: 0.0162\n\nEpoch 10/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0336 - val_loss: 0.0125\n\n\n\n\n\n# Reconstruction error on training + test\nrecon_train = autoencoder.predict(X_train, verbose=0)\nrecon_test = autoencoder.predict(X_test, verbose=0)\n\ntrain_err = np.mean((X_train - recon_train) ** 2, axis=1)\ntest_err = np.mean((X_test - recon_test) ** 2, axis=1)\n\n# Choose threshold as a high percentile of training error\nthreshold = np.percentile(train_err, 99)\nprint(\"Autoencoder threshold (99th percentile of train error):\", threshold)\n\ny_pred_ae = (test_err &gt;= threshold).astype(int)\n\nprint(\"Autoencoder F1 (test, binary):\", f1_score(y_test_bin, y_pred_ae))\nplot_confusion(y_test_bin, y_pred_ae, labels=[0, 1], title=\"Autoencoder\")\n\nAutoencoder threshold (99th percentile of train error): 0.15334489130821294\nAutoencoder F1 (test, binary): 0.08649250614820658\n\n\n\n\n\n\n\n\n\n\n# ROC / PR curves for the autoencoder scores\nbinary_roc_pr_curves(y_test_bin, test_err, pos_label=1, title_prefix=\"Autoencoder \")\n\n(&lt;Figure size 1000x400 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Autoencoder ROC (AUC=0.953)'}, xlabel='False positive rate', ylabel='True positive rate'&gt;,\n        &lt;Axes: title={'center': 'Autoencoder Precision-Recall'}, xlabel='Recall', ylabel='Precision'&gt;],\n       dtype=object))"
  },
  {
    "objectID": "notebooks/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "href": "notebooks/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "title": "Week 04 — Classical ML Models",
    "section": "2. Supervised models: Random Forest and SVM (RBF)",
    "text": "2. Supervised models: Random Forest and SVM (RBF)\n\n2.1 Random Forest\nUse the full multi-class labels (y_train, y_test) and train:\n\nRandom Forest (class_weight=“balanced”; tune n_estimators, max_depth)\nSVM with RBF kernel (class_weight=“balanced”; tune C, gamma)\n\nThis gives a supervised baseline to compare against the unsupervised / semi-supervised detectors.\n\nrf_spec = make_rf_classifier_grid()\nrf_gs = GridSearchCV(\n    estimator=rf_spec.model,\n    param_grid=rf_spec.param_grid,\n    scoring=\"f1_weighted\",\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\nrf_gs.fit(X_train, y_train)\n\nprint(\"Random Forest best params:\", rf_gs.best_params_)\nprint(\"Random Forest best CV f1_weighted:\", rf_gs.best_score_)\n\nFitting 3 folds for each of 9 candidates, totalling 27 fits\nRandom Forest best params: {'max_depth': 20, 'n_estimators': 400}\nRandom Forest best CV f1_weighted: 0.9997615589652838\n\n\n\nbest_rf = rf_gs.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\n\nrf_report = classification_summary(y_test, y_pred_rf, print_report=True)\nplot_confusion(y_test, y_pred_rf, title=\"Random Forest\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.96      0.97      0.97     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 SVM-RBF\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# SVM-RBF\nsvm_fast = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", SVC(\n        kernel=\"rbf\",\n        class_weight=\"balanced\",\n        C=10,          # reasonable value\n        gamma=\"scale\"  # reasonable value\n    ))\n])\n\nsvm_fast.fit(X_train, y_train)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.97      0.98      0.97       822\n           2       0.72      0.98      0.83       225\n           3       0.73      0.80      0.76        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.88      0.95      0.91     98805\nweighted avg       1.00      1.00      1.00     98805"
  },
  {
    "objectID": "notebooks/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "href": "notebooks/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "title": "Week 04 — Classical ML Models",
    "section": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)",
    "text": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)\nNSL-KDD includes very rare R2L and U2R attack families. Instead of assuming I already know which integer labels correspond to these families, I follow a data-driven approach:\n\nIdentify the two rarest classes in y_train.\nTreat them as “R2L/U2R-like” minority classes.\nCompare:\n\nBaseline models using only class_weight=\"balanced\"\nModels trained on SMOTE-resampled data (train only), focusing on those rare classes.\n\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Identify the two rarest classes\nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes (treated as R2L/U2R-like):\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\nMinority classes (treated as R2L/U2R-like): [np.int32(3), np.int32(2)]\nClass counts: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\n\n\n\ndef evaluate_subset(classes_subset, y_true, y_pred):\n    import numpy as np\n    mask = np.isin(y_true, classes_subset)\n    y_true_sub = y_true[mask]\n    y_pred_sub = y_pred[mask]\n    print(\"Subset size:\", y_true_sub.shape[0])\n    return classification_summary(y_true_sub, y_pred_sub, print_report=True)\n\n\n3.1 Baseline: class_weight only\n\nprint(\"Random Forest (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf)\n\nprint(\"\\nSVM-RBF (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_svm)\n\nRandom Forest (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.90      0.90      0.90        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.63      0.63      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\nSVM-RBF (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       1.00      0.80      0.89        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.67      0.59      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n\n\n3.2 SMOTE + Random Forest / SVM-RBF (train only)\n\n# SMOTE strategy: upsample only the identified minority classes\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE counts:\", Counter(y_train_sm))\n\nSMOTE sampling strategy: {np.int32(3): 313166, np.int32(2): 313166}\nAfter SMOTE counts: Counter({np.int32(0): 313166, np.int32(2): 313166, np.int32(3): 313166, np.int32(4): 77822, np.int32(1): 3285})\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Reuse best params from the baseline RF grid search\nprint(\"Baseline RF best params:\", rf_gs.best_params_)\n\nrf_sm = RandomForestClassifier(\n    n_estimators=rf_gs.best_params_[\"n_estimators\"],\n    max_depth=rf_gs.best_params_[\"max_depth\"],\n    n_jobs=-1,\n    class_weight=\"balanced\",    # you can also set this to None, since SMOTE already balances\n    random_state=42,\n)\n\n# Fit on SMOTE-resampled data \nrf_sm.fit(X_train_sm, y_train_sm)\n\ny_pred_rf_sm = rf_sm.predict(X_test)\n\nprint(\"\\nRandom Forest + SMOTE — full test report\")\n_ = classification_summary(y_test, y_pred_rf_sm, print_report=True)\n\nprint(\"\\nRandom Forest + SMOTE — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf_sm)\n\nBaseline RF best params: {'max_depth': 20, 'n_estimators': 400}\n\nRandom Forest + SMOTE — full test report\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       0.99      0.98      0.99       225\n           3       0.75      0.90      0.82        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.95      0.98      0.96     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nRandom Forest + SMOTE — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.98       235\n   macro avg       0.60      0.63      0.62       235\nweighted avg       0.99      0.98      0.98       235\n\n\n\n\n\n4. SVM-RBF (Baseline, Fast Subset)\n\nimport numpy as np\nfrom collections import Counter\n\ndef build_svm_subset(X, y, minority_classes, max_samples=5000, random_state=42):\n    # Build a smaller train set for SVM:\n    # - Keep ALL samples from minority_classes\n    # - Subsample majority classes so total ~ max_samples\n\n    rng = np.random.RandomState(random_state)\n    y = np.asarray(y)\n\n    # Indices for minority and majority\n    mask_min = np.isin(y, minority_classes)\n    idx_min = np.where(mask_min)[0]\n    idx_maj = np.where(~mask_min)[0]\n\n    # Always keep all minority samples\n    keep_idx = list(idx_min)\n\n    # Remaining budget for majority\n    remaining = max(0, max_samples - len(idx_min))\n    if remaining &gt; 0 and len(idx_maj) &gt; remaining:\n        subsampled_maj = rng.choice(idx_maj, size=remaining, replace=False)\n    else:\n        subsampled_maj = idx_maj\n\n    keep_idx.extend(subsampled_maj)\n    keep_idx = np.array(keep_idx)\n\n    X_small = X[keep_idx]\n    y_small = y[keep_idx]\n\n    print(\"SVM subset size:\", X_small.shape, \"class counts:\", Counter(y_small))\n\n    return X_small, y_small\n\n\n\n5. SVM-RBF with SMOTE Oversampling\n\nfrom src.models import make_fast_svm_rbf\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Identify minority classes \nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes:\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\n# Build smaller subset for SVM baseline (no SMOTE)\nX_train_svm, y_train_svm = build_svm_subset(\n    X_train, y_train,\n    minority_classes=minority_classes,\n    max_samples=2000,        # you can drop to 3000 or 2000 if still slow\n    random_state=42\n)\n\n# SMOTE on FULL train, then subset for SVM\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm_full, y_train_sm_full = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE (full):\", Counter(y_train_sm_full))\n\n# Subset the SMOTE-resampled set for SVM\nX_train_sm_svm, y_train_sm_svm = build_svm_subset(\n    X_train_sm_full, y_train_sm_full,\n    minority_classes=minority_classes,\n    max_samples=5000,\n    random_state=42\n)\n\n# SVM baseline\nsvm_fast = make_fast_svm_rbf()\nsvm_fast.fit(X_train_svm, y_train_svm)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nprint(\"\\nSVM-RBF baseline (fast, subset)\")\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF (fast baseline, subset)\")\n\n# SVM on SMOTE-resampled subset\nsvm_sm = make_fast_svm_rbf()\nsvm_sm.fit(X_train_sm_svm, y_train_sm_svm)\n\ny_pred_svm_sm = svm_sm.predict(X_test)\n\nprint(\"\\nSVM-RBF + SMOTE (fast, subset)\")\nclassification_summary(y_test, y_pred_svm_sm, print_report=True)\n\nprint(\"\\nMinority class performance (SVM + SMOTE)\")\nevaluate_subset(minority_classes, y_test, y_pred_svm_sm)\n\nMinority classes: [np.int32(3), np.int32(2)]\nClass counts: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\nSVM subset size: (2000, 115) class counts: Counter({np.int32(2): 901, np.int32(0): 853, np.int32(4): 201, np.int32(3): 42, np.int32(1): 3})\nSMOTE sampling strategy: {np.int32(3): 313166, np.int32(2): 313166}\nAfter SMOTE (full): Counter({np.int32(0): 313166, np.int32(2): 313166, np.int32(3): 313166, np.int32(4): 77822, np.int32(1): 3285})\nSVM subset size: (1020605, 115) class counts: Counter({np.int32(2): 313166, np.int32(3): 313166, np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285})\n\nSVM-RBF baseline (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00     78292\n           1       1.00      0.07      0.13       822\n           2       0.13      1.00      0.23       225\n           3       0.11      0.60      0.19        10\n           4       0.97      0.96      0.97     19456\n\n    accuracy                           0.98     98805\n   macro avg       0.64      0.72      0.50     98805\nweighted avg       0.99      0.98      0.98     98805\n\n\nSVM-RBF + SMOTE (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.98      0.98      0.98       822\n           2       0.70      0.98      0.82       225\n           3       0.78      0.70      0.74        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.89      0.93      0.91     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nMinority class performance (SVM + SMOTE)\nSubset size: 235\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           2       1.00      0.98      0.99       225\n           3       1.00      0.70      0.82        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.50      0.42      0.45       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n '2': {'precision': 0.995475113122172,\n  'recall': 0.9777777777777777,\n  'f1-score': 0.9865470852017937,\n  'support': 225.0},\n '3': {'precision': 1.0,\n  'recall': 0.7,\n  'f1-score': 0.8235294117647058,\n  'support': 10.0},\n '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n 'accuracy': 0.9659574468085106,\n 'macro avg': {'precision': 0.498868778280543,\n  'recall': 0.4194444444444444,\n  'f1-score': 0.4525191242416249,\n  'support': 235.0},\n 'weighted avg': {'precision': 0.9956676614999519,\n  'recall': 0.9659574468085106,\n  'f1-score': 0.9796101629278751,\n  'support': 235.0}}\n\n\n\n\n\n\n\n\n\n\n# Save existing results for later evaluation\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\n# Use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk4_dir = reports_root / \"week_04\"\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Saving Week 04 results under:\", wk4_dir)\n\n# UNSUPERVISED / SEMI-SUPERVISED \n#\n# Use variables that ALREADY exist from earlier cells:\n#   y_test_bin     : binary true labels (0 = normal, 1 = attack)\n#   y_pred_iso     : Isolation Forest binary predictions\n#   y_pred_ocsvm   : OC-SVM binary predictions\n#   y_pred_ae      : Autoencoder binary predictions\n#   test_err       : Autoencoder reconstruction error (per test sample)\n#\n# Scores are taken from existing values:\n#   - Isolation Forest / OC-SVM: score = y_pred_* (0/1 as a crude anomaly score)\n#   - Autoencoder: score = test_err reconstruction error)\n\nunsup_frames = []\n\ndef add_binary_model_existing(name, y_true, y_pred, score):\n    df = pd.DataFrame({\n        \"model\":      name,\n        \"y_true_bin\": np.asarray(y_true).astype(int),\n        \"y_pred_bin\": np.asarray(y_pred).astype(int),\n        \"score\":      np.asarray(score).astype(float),\n    })\n    unsup_frames.append(df)\n    print(f\"Added unsupervised/semi-supervised model: {name}\")\n\n# Isolation Forest\nif \"y_test_bin\" in globals() and \"y_pred_iso\" in globals():\n    add_binary_model_existing(\n        \"Isolation Forest\",\n        y_test_bin,\n        y_pred_iso,\n        y_pred_iso,   # just reuse 0/1 prediction as score (no recomputation)\n    )\nelse:\n    print(\"Isolation Forest predictions not found; skipping.\")\n\n# OC-SVM\nif \"y_test_bin\" in globals() and \"y_pred_ocsvm\" in globals():\n    add_binary_model_existing(\n        \"OC-SVM (RBF)\",\n        y_test_bin,\n        y_pred_ocsvm,\n        y_pred_ocsvm,  # reuse 0/1 prediction as score\n    )\nelse:\n    print(\"OC-SVM predictions not found; skipping.\")\n\n# Autoencoder\nif \"y_test_bin\" in globals() and \"y_pred_ae\" in globals() and \"test_err\" in globals():\n    add_binary_model_existing(\n        \"Deep Autoencoder\",\n        y_test_bin,\n        y_pred_ae,\n        test_err,      # reconstruction error already computed\n    )\nelse:\n    print(\"Autoencoder predictions/errors not found; skipping.\")\n\n#if unsup_frames:\n#    unsup_df = pd.concat(unsup_frames, ignore_index=True)\n#    unsup_csv_path = wk4_dir / \"week04_unsupervised_predictions.csv\"\n#    unsup_parquet_path = wk4_dir / \"week04_unsupervised_predictions.parquet\"\n\n#    unsup_df.to_csv(unsup_csv_path, index=False)\n#    try:\n#        unsup_df.to_parquet(unsup_parquet_path, index=False)\n#    except Exception as e:\n#        print(\"Parquet save for unsupervised models failed (optional):\", e)\n\n#    print(\"\\nSaved unsupervised/semi-supervised results to:\")\n#    print(\" -\", unsup_csv_path)\n#    print(\" -\", unsup_parquet_path)\n#else:\n#    print(\"No unsupervised/semi-supervised models were added; nothing saved for that group.\")\n\nsup_frames = []\n\ndef add_supervised_existing(model_name, y_true, y_pred):\n    df = pd.DataFrame({\n        \"model\":  model_name,\n        \"y_true\": np.asarray(y_true),\n        \"y_pred\": np.asarray(y_pred),\n    })\n    sup_frames.append(df)\n    print(f\"Added supervised model: {model_name}\")\n\n# Random Forest baseline\nif \"y_test\" in globals() and \"y_pred_rf\" in globals():\n    add_supervised_existing(\"Random Forest (baseline)\", y_test, y_pred_rf)\nelse:\n    print(\"Random Forest baseline predictions not found; skipping.\")\n\n# SVM-RBF baseline\nif \"y_test\" in globals() and \"y_pred_svm\" in globals():\n    add_supervised_existing(\"SVM-RBF (baseline)\", y_test, y_pred_svm)\nelse:\n    print(\"SVM-RBF baseline predictions not found; skipping.\")\n\n# Random Forest + SMOTE\nif \"y_test\" in globals() and \"y_pred_rf_sm\" in globals():\n    add_supervised_existing(\"Random Forest + SMOTE\", y_test, y_pred_rf_sm)\nelse:\n    print(\"Random Forest + SMOTE predictions not found; skipping.\")\n\n# SVM-RBF + SMOTE\nif \"y_test\" in globals() and \"y_pred_svm_sm\" in globals():\n    add_supervised_existing(\"SVM-RBF + SMOTE\", y_test, y_pred_svm_sm)\nelse:\n    print(\"SVM-RBF + SMOTE predictions not found; skipping.\")\n\n#if sup_frames:\n#    sup_pred_df = pd.concat(sup_frames, ignore_index=True)\n#    sup_csv_path = wk4_dir / \"week04_supervised_predictions.csv\"\n#    sup_parquet_path = wk4_dir / \"week04_supervised_predictions.parquet\"\n\n#    sup_pred_df.to_csv(sup_csv_path, index=False)\n#    try:\n#        sup_pred_df.to_parquet(sup_parquet_path, index=False)\n#    except Exception as e:\n#        print(\"Parquet save for supervised models failed (optional):\", e)\n\n#    print(\"\\nSaved supervised results to:\")\n#    print(\" -\", sup_csv_path)\n#    print(\" -\", sup_parquet_path)\n#else:\n#    print(\"No supervised models were added; nothing saved for that group.\")\n\nSaving Week 04 results under: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nAdded unsupervised/semi-supervised model: Isolation Forest\nAdded unsupervised/semi-supervised model: OC-SVM (RBF)\nAdded unsupervised/semi-supervised model: Deep Autoencoder\nAdded supervised model: Random Forest (baseline)\nAdded supervised model: SVM-RBF (baseline)\nAdded supervised model: Random Forest + SMOTE\nAdded supervised model: SVM-RBF + SMOTE\n\n\n\n# Save Week 04 models for Week 06 explainability\nimport joblib\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\nmodels_dir = getattr(paths, \"models\", project_root / \"models\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nrf_path  = models_dir / \"week04_rf.joblib\"\nsvm_path = models_dir / \"week04_svm.joblib\"\nae_path  = models_dir / \"week04_ae.joblib\"\n\nprint(\"Saving models to:\", models_dir)\n\n# These variables already exist in your notebook:\n#   best_rf   -&gt; best RandomForest from GridSearch\n#   svm_fast  -&gt; SVM-RBF Pipeline (baseline)\n#   autoencoder -&gt; trained deep autoencoder\n\njoblib.dump(best_rf, rf_path)\nprint(\"Saved RF to:\", rf_path)\n\njoblib.dump(svm_fast, svm_path)\nprint(\"Saved SVM to:\", svm_path)\n\njoblib.dump(autoencoder, ae_path)\nprint(\"Saved Autoencoder to:\", ae_path)\n\nSaving models to: C:\\Users\\mehra\\Final_Project\\models\nSaved RF to: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSaved SVM to: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nSaved Autoencoder to: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib"
  },
  {
    "objectID": "notebooks/week_02_eda.html",
    "href": "notebooks/week_02_eda.html",
    "title": "Week 02 — Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Objective. Visualize distributions, correlations, categorical frequencies, and low-dimensional structure.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n\n\n\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import dist_plots, corr_heatmap, topn_bar\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\ntry:\n    import umap\nexcept:\n    umap = None\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\nprint('Loading from:', raw_path)\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\ndf.shape\n\nLoading from: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n(494021, 43)\n\n\n\n# Distributions. (Plot histogram + KDE for selected numeric features on a stratified sample.)\nimport numpy as np\nimport pandas as pd\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import dist_plots\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\n\n# Limit to top 3 by frequency\ntop_fams = df['family'].value_counts().head(3).index\ndf_small = df[df['family'].isin(top_fams)].copy()\nprint(\"Using families:\", list(top_fams))\n\n# Take a MUCH SMALLER stratified sample per family\nsample_per_family = 500 \ndf_sample = (\n    df_small\n    .groupby('family', group_keys=False)[df_small.columns]\n    .apply(lambda x: x.sample(min(sample_per_family, len(x)), random_state=42))\n)\n\nprint(\"Sampled rows:\", len(df_sample))\n\n# Use only a FEW numeric columns\nnumeric_cols = [\n    c for c in df_sample.columns\n    if c not in ['protocol_type', 'service', 'flag', 'label', 'family']\n]\n\nsel = numeric_cols[:3]   \nprint(\"Columns to plot:\", sel)\n\ndist_plots(df_sample, sel, paths.figs)\nprint('Saved distribution plots for columns:', sel)\nprint('Figures directory:', paths.figs)\n\nUsing families: ['DoS', 'normal', 'Probe']\nSampled rows: 1500\nColumns to plot: ['duration', 'src_bytes', 'dst_bytes']\nSaved distribution plots for columns: ['duration', 'src_bytes', 'dst_bytes']\nFigures directory: C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Correlation by compute Pearson and Spearman heatmaps on numeric features.\nimport numpy as np\nimport pandas as pd\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import corr_heatmap\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\n\nnumeric_cols = [c for c in df.columns if c not in ['protocol_type','service','flag','label','family']]\ncorr_cols = numeric_cols[:20]\ncorr_heatmap(df, corr_cols, paths.figs / 'eda_corr_pearson.png', method='pearson')\ncorr_heatmap(df, corr_cols, paths.figs / 'eda_corr_spearman.png', method='spearman')\nprint('Saved correlation heatmaps to', paths.figs)\n\nSaved correlation heatmaps to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Categorical frequencies. (Top-N bars for protocol_type, service, flag.)\ntopn_bar(df['protocol_type'], 10, paths.figs / 'eda_top_protocol.png', 'Top Protocol Types')\ntopn_bar(df['service'], 20, paths.figs / 'eda_top_service.png', 'Top Services')\ntopn_bar(df['flag'], 10, paths.figs / 'eda_top_flag.png', 'Top Flags')\nprint('Saved categorical frequency plots to', paths.figs)\n\nSaved categorical frequency plots to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Family imbalance\nfam_counts = df['family'].value_counts()\nfam_perc = (fam_counts / len(df) * 100).round(2)\ndisplay(pd.DataFrame({'count': fam_counts, 'percent': fam_perc}))\n\n\n\n\n\n\n\n\ncount\npercent\n\n\nfamily\n\n\n\n\n\n\nDoS\n391458\n79.24\n\n\nnormal\n97278\n19.69\n\n\nProbe\n4107\n0.83\n\n\nR2L\n1126\n0.23\n\n\nU2R\n52\n0.01\n\n\n\n\n\n\n\n\n# PCA visualization.\nnum_df = df.select_dtypes(include=[np.number])\nX = num_df.fillna(0.0).to_numpy()\npca2 = PCA(n_components=2, random_state=42).fit_transform(X)\nfig, ax = plt.subplots()\nsns.scatterplot(x=pca2[:,0], y=pca2[:,1], hue=df['family'], ax=ax, s=10, linewidth=0)\nax.set_title('PCA (2D) by Family')\nfig.savefig(paths.figs / 'eda_pca2.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)\nprint('Saved PCA 2D to', paths.figs)\n\n\n\n\n\n\n\n\nSaved PCA 2D to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# t-SNE and UMAP. \nsample = (\n    df\n    .groupby('family', group_keys=False)[df.columns]\n    .apply(lambda x: x.sample(min(1500, len(x)), random_state=42))\n)\nX_s = sample.select_dtypes(include=[np.number]).fillna(0.0).to_numpy()\nts = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\nts2 = ts.fit_transform(X_s)\nfig, ax = plt.subplots()\nsns.scatterplot(x=ts2[:,0], y=ts2[:,1], hue=sample['family'], s=8, linewidth=0, ax=ax)\nax.set_title('t-SNE (2D) by Family — stratified sample')\nfig.savefig(paths.figs / 'eda_tsne.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)\n\n\n\n\n\n\n\n\nSaved t-SNE/UMAP plots to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\nNotes\n\nHeavy-tailed numeric features may benefit from log-scale in later analysis.\nRare classes (R2L, U2R) present risk for model bias; consider class-weighting/SMOTE in Week 4."
  },
  {
    "objectID": "notebooks/week06_rewritten.html",
    "href": "notebooks/week06_rewritten.html",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\n\n# Use a clean seaborn style\nsns.set(style=\"whitegrid\")\n\n# Project paths\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\n\nset_global_seed(42)\npaths = Paths().ensure()\n\n# Derive a robust reports root (Week 03–06)\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\nwk6_dir = reports_root / \"week_06\"\n\nwk6_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Project root:\", project_root)\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports:\", wk3_dir)\nprint(\"Week 04 reports:\", wk4_dir)\nprint(\"Week 06 reports:\", wk6_dir)\n\nProject root: C:\\Users\\mehra\\Final_Project\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nWeek 06 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_06\n# Load processed arrays from Week 01\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n\n# binary labels (0=normal, 1=attack) from Week 04 logic\nfrom collections import Counter\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin  = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nX_train: (395216, 115) X_test: (98805, 115)\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({0: 313166, 1: 82050})\nBinary test  counts: Counter({0: 78292, 1: 20513})\n# Load Week 03 & Week 04 prediction files \n\nfrom pathlib import Path\n\ndef _load_csv_if_exists(path: Path, label: str):\n    if not path.exists():\n        print(f\"{label} not found at {path} — returning None.\")\n        return None\n    df = pd.read_csv(path)\n    print(f\"{label} loaded from {path} with shape {df.shape}\")\n    return df\n\nwk3_unsup = _load_csv_if_exists(\n    wk3_dir / \"week03_unsupervised_predictions.csv\",\n    \"Week 03 unsupervised predictions\"\n)\n\nwk4_unsup = _load_csv_if_exists(\n    wk4_dir / \"week04_unsupervised_predictions.csv\",\n    \"Week 04 unsupervised/semi-supervised predictions\"\n)\n\nwk4_sup = _load_csv_if_exists(\n    wk4_dir / \"week04_supervised_predictions.csv\",\n    \"Week 04 supervised predictions\"\n)\n\nprint(\"\\nSummary:\")\nprint(\"  Week 03 unsupervised df:\", None if wk3_unsup is None else wk3_unsup.shape)\nprint(\"  Week 04 unsupervised df:\", None if wk4_unsup is None else wk4_unsup.shape)\nprint(\"  Week 04 supervised df:\", None if wk4_sup is None else wk4_sup.shape)\n\nfrom IPython.display import display\n\nif wk3_unsup is not None:\n    print(\"\\nWeek 03 unsupervised head:\")\n    display(wk3_unsup.head())\n\nif wk4_unsup is not None:\n    print(\"\\nWeek 04 unsupervised head:\")\n    display(wk4_unsup.head())\n\nif wk4_sup is not None:\n    print(\"\\nWeek 04 supervised head:\")\n    display(wk4_sup.head())\n\nWeek 03 unsupervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv with shape (98257, 4)\nWeek 04 unsupervised/semi-supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_unsupervised_predictions.csv with shape (296415, 4)\nWeek 04 supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_supervised_predictions.csv with shape (395220, 3)\n\nSummary:\n  Week 03 unsupervised df: (98257, 4)\n  Week 04 unsupervised df: (296415, 4)\n  Week 04 supervised df: (395220, 3)\n\nWeek 03 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n\n\n\n\n\n\nWeek 04 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0\n\n\n\n\n\n\n\n\nWeek 04 supervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#feature-names-loaded-models",
    "href": "notebooks/week06_rewritten.html#feature-names-loaded-models",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Feature Names & Loaded Models",
    "text": "Feature Names & Loaded Models\nFeature names were loaded from the preprocessing pipeline. Random Forest and SVM models loaded successfully; the autoencoder was unavailable in this environment and AE explainability was skipped.\n\n# Feature names via preprocessor\n\nfeature_names = None\ntry:\n    import joblib\n    import sklearn.compose._column_transformer as _ct\n\n    class _RemainderColsList(list):\n        # Compatibility shim for old sklearn ColumnTransformer pickles.\n        pass\n\n    _ct._RemainderColsList = _RemainderColsList\n\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = list(pre.get_feature_names_out())\n    print(\"Loaded feature names from preprocessor:\", len(feature_names))\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    # Fall back to simple numeric names\n    feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n    print(\"Using generic feature names:\", len(feature_names))\n\nLoaded feature names from preprocessor: 115\n\n\n\nimport joblib\n\nrf_model = None\nsvm_model = None\nae_model = None\n\nrf_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_rf.joblib\"\nsvm_path = getattr(paths, \"models\", project_root / \"models\") / \"week04_svm.joblib\"\nae_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_ae.joblib\"\n\nfor name, p in [(\"RF\", rf_path), (\"SVM\", svm_path), (\"AE\", ae_path)]:\n    print(f\"{name} expected at:\", p)\n\n# -------------------------\n# RF\n# -------------------------\ntry:\n    if rf_path.exists():\n        rf_model = joblib.load(rf_path)\n        print(\"Loaded RF model.\")\n    else:\n        print(\"RF model file not found.\")\nexcept Exception as e:\n    print(\"Could not load RF model:\", e)\n\n# -------------------------\n# SVM\n# -------------------------\ntry:\n    if svm_path.exists():\n        svm_model = joblib.load(svm_path)\n        print(\"Loaded SVM model.\")\n    else:\n        print(\"SVM model file not found.\")\nexcept Exception as e:\n    print(\"Could not load SVM model:\", e)\n\n\nRF expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSVM expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nAE expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib\nLoaded RF model.\nLoaded SVM model."
  },
  {
    "objectID": "notebooks/week06_rewritten.html#explainability-utilities",
    "href": "notebooks/week06_rewritten.html#explainability-utilities",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Explainability Utilities",
    "text": "Explainability Utilities\nI compute Gini-based importance and SHAP global importance for Random Forest. SVM-RBF uses Kernel SHAP over summarized background samples. Both methods highlight dominant network features influencing model decisions.\n\nimport shap\n\nfrom src.explain import (\n    compute_rf_feature_importance,\n    plot_anomaly_score_distributions,\n    plot_reconstruction_error_hist,\n    compute_pca_embedding,\n    plot_pca_dbscan_like,\n    plot_pca_decision_boundaries,\n)"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#random-forest-explainability",
    "href": "notebooks/week06_rewritten.html#random-forest-explainability",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Random Forest Explainability",
    "text": "Random Forest Explainability\nRF Gini and SHAP rankings consistently surface top protocol and connection statistical features. SHAP provides smoother global attribution, confirming the RF decision structure shown in the plotted top‑20 bars.\n\n\n# Random Forest Gini importance + SHAP global importance\n\n# Gini-based importance\nrf_imp_df = compute_rf_feature_importance(rf_model, feature_names)\nrf_imp_df = rf_imp_df.sort_values(\"importance\", ascending=False)\n\n# SHAP-based global importance\ntry:\n    import shap  # ensure imported\n    # Subsample test set for explanation\n    rng = np.random.RandomState(42)\n    n_explain = min(1000, X_test.shape[0])\n    explain_idx = rng.choice(X_test.shape[0], size=n_explain, replace=False)\n    X_explain = X_test[explain_idx]\n\n    explainer = shap.TreeExplainer(rf_model)\n    shap_values = explainer.shap_values(X_explain)\n\n    # shap_values can be:\n    # - array of shape (n_samples, n_features)  -&gt; binary/regression\n    # - list of arrays (n_classes, n_samples, n_features) -&gt; multiclass\n    if isinstance(shap_values, list):\n        # Stack into 3D: (n_classes, n_samples, n_features)\n        shap_arr = np.stack(shap_values, axis=0)\n        # Take mean absolute across classes and samples -&gt; (n_features,)\n        shap_global = np.mean(np.abs(shap_arr), axis=(0, 1))\n    else:\n        # Directly mean absolute across samples -&gt; (n_features,)\n        shap_global = np.mean(np.abs(shap_values), axis=0)\n\n    # Ensure 1D vector\n    shap_global = np.asarray(shap_global).ravel()\n\n    # Safety check: align length with feature_names\n    if len(shap_global) != len(feature_names):\n        print(\n            f\"Length mismatch: shap_global={len(shap_global)}, \"\n            f\"feature_names={len(feature_names)}. Truncating to min length.\"\n        )\n        min_len = min(len(shap_global), len(feature_names))\n        shap_global = shap_global[:min_len]\n        feature_vec = feature_names[:min_len]\n    else:\n        feature_vec = feature_names\n\n    rf_shap_df = (\n        pd.DataFrame(\n            {\n                \"feature\": feature_vec,\n                \"importance\": shap_global,\n            }\n        )\n        .sort_values(\"importance\", ascending=False)\n    )\n\n    # 3. Two-column figure (Gini vs SHAP)\n    top_n = 20\n    fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n\n     sns.barplot(\n        data=rf_imp_df.head(top_n),\n        x=\"importance\",\n        y=\"feature\",\n        ax=axes[0],\n    )\n    axes[0].set_title(\"Random Forest — Top 20 Gini Importances\")\n    axes[0].set_xlabel(\"Gini importance\")\n    axes[0].set_ylabel(\"Feature\")\n\n    sns.barplot(\n        data=rf_shap_df.head(top_n),\n        x=\"importance\",\n        y=\"feature\",\n        ax=axes[1],\n    )\n    axes[1].set_title(\"Random Forest — Top 20 SHAP Importances\")\n    axes[1].set_xlabel(\"Mean |SHAP value|\")\n    axes[1].set_ylabel(\"\")\n\n    plt.tight_layout()\n    fig_path = wk6_dir / \"rf_gini_vs_shap_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    plt.show()\n\n    display(rf_shap_df.head(10))\n\nexcept Exception as e:\n    print(\"RF SHAP computation failed:\", e)\n\n⚠️ Length mismatch: shap_global=575, feature_names=115. Truncating to min length.\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\rf_gini_vs_shap_top20.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n75\ncat__flag_SF\n0.081151\n\n\n79\nnum__dst_bytes\n0.051866\n\n\n0\ncat__protocol_type_tcp\n0.024382\n\n\n102\nnum__same_srv_rate\n0.021036\n\n\n76\ncat__flag_SH\n0.015354\n\n\n4\ncat__service_auth\n0.014612\n\n\n71\ncat__flag_S0\n0.014130\n\n\n2\ncat__service_X11\n0.013217\n\n\n100\nnum__rerror_rate\n0.010808\n\n\n77\nnum__duration\n0.008508"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#svm-rbf-explainability",
    "href": "notebooks/week06_rewritten.html#svm-rbf-explainability",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "SVM-RBF Explainability",
    "text": "SVM-RBF Explainability\nKernel SHAP applied to the SVM decision function reveals nonlinear feature influence. The SHAP ranking emphasizes features controlling margin separation, consistent with SVM behavior displayed in the bar plot.\n\n# SVM-RBF SHAP global importance\n\ntry:\n    import shap\n\n    rng = np.random.RandomState(42)\n\n    n_background = min(100, X_train.shape[0])   \n    n_explain = min(150, X_test.shape[0])       \n\n    bg_idx = rng.choice(X_train.shape[0], size=n_background, replace=False)\n    ex_idx = rng.choice(X_test.shape[0], size=n_explain, replace=False)\n\n    X_bg = X_train[bg_idx]\n    X_ex = X_test[ex_idx]\n\n    # Use decision function for richer margins (multi-class or binary).\n    f = svm_model.decision_function\n\n    explainer = shap.KernelExplainer(f, X_bg)\n    shap_values = explainer.shap_values(X_ex, nsamples=100)  # was \"auto\"\n\n    # Aggregate to global |SHAP|\n    if isinstance(shap_values, list):\n        # shap_values: list of (n_samples, n_features) -&gt; stack to 3D\n        shap_arr = np.stack(shap_values, axis=0)    # (n_outputs, n_samples, n_features)\n        shap_global = np.mean(np.abs(shap_arr), axis=(0, 1))  # -&gt; (n_features,)\n    else:\n        # (n_samples, n_features)\n        shap_global = np.mean(np.abs(shap_values), axis=0)    # -&gt; (n_features,)\n\n    # Force 1D\n    shap_global = np.asarray(shap_global).ravel()\n\n    # Safety: align with feature_names\n    if len(shap_global) != len(feature_names):\n        print(\n            f\"Length mismatch: shap_global={len(shap_global)}, \"\n            f\"feature_names={len(feature_names)}. Truncating to min length.\"\n        )\n        m = min(len(shap_global), len(feature_names))\n        shap_global = shap_global[:m]\n        feature_vec = feature_names[:m]\n    else:\n        feature_vec = feature_names\n\n    svm_shap_df = (\n        pd.DataFrame(\n            {\n                \"feature\": feature_vec,\n                \"importance\": shap_global,\n            }\n        )\n        .sort_values(\"importance\", ascending=False)\n    )\n\n    # Plot: two-column layout \n    fig, axes = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={\"width_ratios\": [3, 1]})\n\n    sns.barplot(\n        data=svm_shap_df.head(20),\n        x=\"importance\",\n        y=\"feature\",\n        ax=axes[0],\n    )\n    axes[0].set_title(\"SVM-RBF — Top 20 SHAP Importances\")\n    axes[0].set_xlabel(\"Mean |SHAP value|\")\n    axes[0].set_ylabel(\"Feature\")\n\n    axes[1].axis(\"off\")\n    axes[1].text(\n        0.0,\n        0.5,\n        \"Right panel reserved for future local SVM explanations\\n\"\n        \"(e.g., per-connection SHAP).\",\n        va=\"center\",\n        ha=\"left\",\n        fontsize=10,\n    )\n\n    plt.tight_layout()\n    fig_path = wk6_dir / \"svm_shap_importance_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    plt.show()\n\n    display(svm_shap_df.head(10))\n\nexcept Exception as e:\n    print(\"SVM SHAP computation failed:\", e)\n\n100%|██████████| 150/150 [02:23&lt;00:00,  1.05it/s]\n\n\n⚠️ Length mismatch: shap_global=575, feature_names=115. Truncating to min length.\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\svm_shap_importance_top20.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature\nimportance\n\n\n\n\n0\ncat__protocol_type_tcp\n0.070257\n\n\n75\ncat__flag_SF\n0.063796\n\n\n60\ncat__service_time\n0.063438\n\n\n62\ncat__service_urp_i\n0.056107\n\n\n79\nnum__dst_bytes\n0.055336\n\n\n5\ncat__service_bgp\n0.051553\n\n\n9\ncat__service_daytime\n0.049859\n\n\n4\ncat__service_auth\n0.042406\n\n\n63\ncat__service_uucp\n0.035613\n\n\n64\ncat__service_uucp_path\n0.035358"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#pca-visualization",
    "href": "notebooks/week06_rewritten.html#pca-visualization",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "PCA Visualization",
    "text": "PCA Visualization\nThe 2‑D PCA projection shows clear grouping of normal vs attack families, supporting interpretability of model behavior. Families cluster in distinct regions, validating model separability observed earlier.\n\n# PCA embedding for visualization\n\nX_test_pca = compute_pca_embedding(X_test, n_components=2, random_state=42)\nX_test_pca.shape\n\n(98805, 2)\n\n\n\n# Anomaly score distributions for unsupervised models (Week 03 + Week 04)\n\nif wk3_unsup is None and wk4_unsup is None:\n    print(\"No unsupervised score tables loaded — skipping anomaly score distributions.\")\nelse:\n    fig = plot_anomaly_score_distributions(\n        wk3_unsup=wk3_unsup,\n        wk4_unsup=wk4_unsup,\n        max_models=6,\n    )\n    fig_path = wk6_dir / \"unsupervised_anomaly_score_distributions.png\"\n    fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n    print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\unsupervised_anomaly_score_distributions.png\n\n\n\n\n\n\n\n\n\n\n# Autoencoder reconstruction error histogram (Week 04 unsupervised)\n\nif wk4_unsup is None:\n    print(\"Week 04 unsupervised df not loaded — skipping AE reconstruction error histogram.\")\nelse:\n    mask_ae = wk4_unsup[\"model\"].str.contains(\"Autoencoder\", case=False, na=False)\n    if not mask_ae.any():\n        print(\"No Autoencoder rows in Week 04 unsupervised df.\")\n    else:\n        ae_scores = wk4_unsup.loc[mask_ae, \"score\"].values\n        fig = plot_reconstruction_error_hist(ae_scores)\n        fig_path = wk6_dir / \"ae_reconstruction_error_hist.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\ae_reconstruction_error_hist.png"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#autoencoder-reconstruction-error",
    "href": "notebooks/week06_rewritten.html#autoencoder-reconstruction-error",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Autoencoder Reconstruction Error",
    "text": "Autoencoder Reconstruction Error\nWeek‑04 results showed distinct reconstruction‑error gaps between normal and anomalous traffic—evidence that AE learned normal patterns effectively.\n\n# PCA-based decision boundary visualization\n\nfig = plot_pca_decision_boundaries(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    feature_names=feature_names,\n    random_state=42,\n)\n\nfig_path = wk6_dir / \"pca_decision_boundaries_rf_svm.png\"\nfig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\nprint(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\pca_decision_boundaries_rf_svm.png"
  },
  {
    "objectID": "notebooks/week06_rewritten.html#transparency-vs-accuracy",
    "href": "notebooks/week06_rewritten.html#transparency-vs-accuracy",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Transparency vs Accuracy",
    "text": "Transparency vs Accuracy\nUsing the Week 05 performance summaries together with the Week 06 explainability outputs (Gini importance, SHAP for RF and SVM, PCA views, and anomaly-detector comparisons), the overall picture of transparency vs. accuracy becomes clearer:\n\nRandom Forest (RF)\n\nDemonstrates strong macro-F1 performance and consistent detection across most families.\n\nBoth Gini and Tree SHAP rankings highlight a focused set of influential features—primarily connection-level statistics, protocol-related attributes, and volume-based indicators.\n\nFeature attributions are globally coherent: SHAP smooths out the importance landscape and confirms the same high-impact features surfaced by Gini.\n\nInterpretability burden is moderate—RF is an ensemble, but the combination of feature importance tables and clear separation patterns in PCA makes it straightforward to communicate decision drivers.\n\nSVM-RBF\n\nAchieves competitive detection quality and, in some families, matches RF performance.\n\nThe model structure is inherently opaque, but Kernel SHAP applied to the decision function provides meaningful global feature influence.\n\nSHAP importance reveals which features drive margin separation, aligning with the nonlinear behavior expected from RBF kernels.\n\nTransparency is lower than RF, but global SHAP helps reduce the gap and provides an interpretable footprint even without direct access to the internal decision boundaries.\n\nDeep Autoencoder (AE)\n\nThe AE model was not available in this environment, but prior Week 04 results showed clear reconstruction-error separation between normal and malicious connections.\n\nInterpretability remains limited to distance-from-normal reasoning rather than feature-level attributions.\n\nDespite low transparency, its ability to flag unfamiliar traffic patterns makes it effective as an auxiliary anomaly detector.\n\nClassical Unsupervised Models (Isolation Forest, LOF, Elliptic Envelope)\n\nScore distributions and anomaly-score visualizations show each method capturing different geometric notions of “outlierness.”\n\nThese detectors provide helpful signal diversity but do not supply detailed per-feature explanations.\n\nTheir primary value lies in catching unusual behaviors outside the supervised model’s coverage.\n\n\n\nOverall trade-off\nAs we progress from interpretable models (RF) toward more expressive but opaque methods (SVM-RBF and deep models), we observe:\n\nAccuracy generally increases or remains competitive, especially for complex traffic patterns.\n\nTransparency decreases, requiring SHAP or other post-hoc methods to explain decisions.\n\nA practical IDS benefits from a hybrid architecture:\n\nUse Random Forest as the primary explainable classifier—its SHAP and Gini profiles make feature-driven triage easy to communicate.\n\nUse SVM-RBF where nonlinear separation improves detection, supported by global SHAP for interpretability.\n\nUse AE / unsupervised models as broader anomaly detectors, tuned with clear thresholding policies.\n\nThis layered approach balances accuracy, interpretability, and operational reliability across known and emerging attack patterns."
  },
  {
    "objectID": "notebooks/week06_rewritten.html#real-world-ids-implications",
    "href": "notebooks/week06_rewritten.html#real-world-ids-implications",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Real-World IDS Implications",
    "text": "Real-World IDS Implications\n\nFalse-Positive Management\n\nEven modest false-positive rates can overwhelm analysts at operational scale.\n\nWeek 06 SHAP results highlight which features push borderline samples into false-alert territory, helping refine thresholds and preprocessing.\n\nThreshold selection should target a manageable daily alert volume, not maximum recall alone.\n\nThreshold Tuning for Anomaly Detectors\n\nWeek 03 and Week 04 score distributions show that Isolation Forest, LOF, and the Autoencoder each produce distinct anomaly-separation patterns.\n\nThese thresholds should be treated as policy levers, adjustable based on operational workload.\n\nRegular recalibration ensures the IDS adapts as traffic patterns shift.\n\nModel Maintenance and Concept Drift\n\nPCA views and feature-distribution checks (Week 06) indicate that traffic structure evolves over time.\n\nPeriodic retraining or validation—monthly or quarterly—helps maintain detection effectiveness.\n\nDrift indicators include: cluster shifts in PCA, changes in reconstruction-error ranges, or shifts in feature importances from SHAP.\n\nRole Specialization Across Model Families\n\nSupervised models (RF, SVM) provide actionable, interpretable alerts supported by global SHAP importance rankings.\n\nUnsupervised and deep models act as early-warning layers for rare or previously unseen behaviors.\n\nFeature-importance plots, PCA separation, and anomaly-score profiles support triage decisions and help analysts justify why a flow was flagged."
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html",
    "href": "notebooks/Last/week_05_model_evaluation.html",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "",
    "text": "# Imports, project root, and paths.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Sklearn metrics & helpers\nfrom sklearn.metrics import (\n    confusion_matrix,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    average_precision_score,\n)\nfrom sklearn.calibration import CalibrationDisplay\n\n# Project utilities\nfrom src.utils import Paths, set_global_seed\n\n# Optional: helper for binary label mapping (used in earlier weeks)\ntry:\n    from src.eval import to_binary  # used in earlier weeks\nexcept ImportError:\n    to_binary = None\n\nset_global_seed(42)\npaths = Paths().ensure()\n\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\nfrom pathlib import Path as _Path\n\n_this_dir = _Path.cwd()\n_project_root = _this_dir.parent if _this_dir.name == \"notebooks\" else _this_dir\n\n# Try to use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", _project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\n\nwk3_dir.mkdir(parents=True, exist_ok=True)\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports folder:\", wk3_dir)\nprint(\"Week 04 reports folder:\", wk4_dir)\n\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_04\n# Load processed train/test arrays from Week 01 \nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\n# Optional binary labels (normal vs attack) if a helper exists.\nif to_binary is not None:\n    normal_label = \"normal\"  # adjust if your label encoding is different\n    y_train_bin = to_binary(y_train, normal_label)\n    y_test_bin  = to_binary(y_test, normal_label)\n    print(\"Binary train counts:\", Counter(y_train_bin))\n    print(\"Binary test  counts:\", Counter(y_test_bin))\nelse:\n    y_train_bin = None\n    y_test_bin = None\n    print(\"NOTE: src.eval.to_binary not found – binary labels not created automatically.\")\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\ny_test  classes: Counter({0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10})\nNOTE: src.eval.to_binary not found – binary labels not created automatically.\n# Helper functions for metrics and plotting.\n\ndef multiclass_metrics(y_true, y_pred, y_proba=None, labels=None, average=\"macro\"):\n    \"\"\"Compute macro + per-class metrics for a multiclass setting.\n\n    Parameters\n    ----------\n    y_true, y_pred : array-like\n        True and predicted class labels.\n    y_proba : array-like of shape (n_samples, n_classes), optional\n        Predicted probabilities for each class (for ROC/PR).\n    labels : list, optional\n        Label order. If None, inferred from y_true.\n    average : str\n        Averaging scheme for global metrics.\n    \"\"\"\n    if labels is None:\n        labels = np.unique(y_true)\n    \n    prec, rec, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=average, zero_division=0\n    )\n    \n    per_class_df = pd.DataFrame({\n        \"family\": labels,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1\": f1,\n        \"support\": support,\n    })\n    \n    metrics = {\n        \"precision_macro\": macro_prec,\n        \"recall_macro\": macro_rec,\n        \"f1_macro\": macro_f1,\n    }\n    \n    # ROC-AUC (OvR) and PR-AUC\n    if y_proba is not None:\n        # Build one-hot encoding from labels.\n        label_to_idx = {lab: i for i, lab in enumerate(labels)}\n        y_true_idx = np.array([label_to_idx[lab] for lab in y_true])\n        Y_true_ovr = np.eye(len(labels))[y_true_idx]\n        \n        try:\n            roc_macro_ovr = roc_auc_score(Y_true_ovr, y_proba, average=\"macro\", multi_class=\"ovr\")\n        except Exception:\n            roc_macro_ovr = np.nan\n        \n        try:\n            pr_macro_ovr = average_precision_score(Y_true_ovr, y_proba, average=\"macro\")\n        except Exception:\n            pr_macro_ovr = np.nan\n        \n        metrics[\"roc_auc_ovr_macro\"] = roc_macro_ovr\n        metrics[\"pr_auc_ovr_macro\"] = pr_macro_ovr\n    else:\n        metrics[\"roc_auc_ovr_macro\"] = np.nan\n        metrics[\"pr_auc_ovr_macro\"] = np.nan\n    \n    return metrics, per_class_df\n\n\ndef binary_threshold_metrics(y_true_bin, scores, thresholds):\n    \"\"\"Sweep thresholds over anomaly scores for binary detectors.\n\n    Assumes higher `scores` = more anomalous (1 = attack, 0 = normal).\n    Returns a DataFrame with precision/recall/F1 for each threshold.\n    \"\"\"\n    rows = []\n    for thr in thresholds:\n        y_pred_bin = (scores &gt;= thr).astype(int)\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n        )\n        rows.append({\n            \"threshold\": thr,\n            \"precision\": prec,\n            \"recall\": rec,\n            \"f1\": f1,\n        })\n    return pd.DataFrame(rows)\n\n\ndef plot_confusion(cm, labels, title):\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_title(title)\n    plt.tight_layout()\n    return fig, ax"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#load-week-04-supervised-results",
    "href": "notebooks/Last/week_05_model_evaluation.html#load-week-04-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 supervised results",
    "text": "Load Week 04 supervised results\n\n# Load Week 04 supervised predictions.\npred_parquet = wk4_dir / \"week04_supervised_predictions.parquet\"\npred_csv     = wk4_dir / \"week04_supervised_predictions.csv\"\n\nif pred_parquet.exists():\n    sup_pred_df = pd.read_parquet(pred_parquet)\nelif pred_csv.exists():\n    sup_pred_df = pd.read_csv(pred_csv)\nelse:\n    raise FileNotFoundError(\n        f\"Could not find Week 04 prediction file. Expected one of:\\n{pred_parquet}\\n{pred_csv}\\n\"\n        \"Please save your Week 04 test predictions to one of these paths.\"\n    )\n\nprint(\"Loaded Week 04 supervised predictions:\")\ndisplay(sup_pred_df.head())\n\n# Identify probability columns and families from colnames (proba_&lt;family&gt;).\nproba_cols = [c for c in sup_pred_df.columns if c.startswith(\"proba_\")]\nfamilies_from_proba = [c.replace(\"proba_\", \"\") for c in proba_cols]\n\nLoaded Week 04 supervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "href": "notebooks/Last/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 03 unsupervised / statistical results",
    "text": "Load Week 03 unsupervised / statistical results\n\n# Load Week 03 unsupervised predictions\nunsup_parquet = wk3_dir / \"week03_unsupervised_predictions.parquet\"\nunsup_csv     = wk3_dir / \"week03_unsupervised_predictions.csv\"\n\nunsup_df = None\nif unsup_parquet.exists():\n    unsup_df = pd.read_parquet(unsup_parquet)\nelif unsup_csv.exists():\n    unsup_df = pd.read_csv(unsup_csv)\n\nif unsup_df is not None:\n    print(\"Loaded Week 03 unsupervised predictions:\")\n    display(unsup_df.head())\nelse:\n    print(\"Week 03 unsupervised prediction file not found — \"\n          \"unsupervised models will be skipped unless you add it.\")\n\nLoaded Week 03 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "href": "notebooks/Last/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 unsupervised / semi-supervised results",
    "text": "Load Week 04 unsupervised / semi-supervised results\n\n# Load Week 04 unsupervised predictions\nunsup4_parquet = wk4_dir / \"week04_unsupervised_predictions.parquet\"\nunsup4_csv     = wk4_dir / \"week04_unsupervised_predictions.csv\"\n\nunsup4_df = None\nif unsup4_parquet.exists():\n    unsup4_df = pd.read_parquet(unsup4_parquet)\nelif unsup4_csv.exists():\n    unsup4_df = pd.read_csv(unsup4_csv)\n\nif unsup4_df is not None:\n    print(\"Loaded Week 04 unsupervised predictions:\")\n    display(unsup4_df.head())\nelse:\n    print(\"Week 04 unsupervised prediction file not found — \"\n          \"these models will be skipped unless you add it.\")\n\nLoaded Week 04 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#metric-table",
    "href": "notebooks/Last/week_05_model_evaluation.html#metric-table",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Metric Table",
    "text": "Metric Table\n\nPrecision, Recall, F1 (macro)\nROC-AUC (OvR) and PR-AUC (macro) where probabilities/scores are available\n\nSupervised models are treated as multiclass, while unsupervised models are evaluated in the binary attack vs normal setting.\n\n# Build metric table from saved Week 03 / Week 04 outputs.\n\nrows = []\nper_class_by_model = {}  # store per-family metrics for later deep-dive\n\n# Supervised (multiclass)\nfor model_name, g in sup_pred_df.groupby(\"model\"):\n    y_true = g[\"y_true\"].to_numpy()\n    y_pred = g[\"y_pred\"].to_numpy()\n\n    # Use proba_* columns if present\n    if proba_cols:\n        y_proba = g[proba_cols].to_numpy()\n        labels_order = families_from_proba\n    else:\n        y_proba = None\n        labels_order = np.unique(y_true)\n\n    metrics, per_class = multiclass_metrics(\n        y_true,\n        y_pred,\n        y_proba=y_proba,\n        labels=labels_order,\n    )\n    per_class[\"model\"] = model_name\n    per_class_by_model[model_name] = per_class\n\n    rows.append({\n        \"model\": model_name,\n        \"type\": \"supervised\",\n        **metrics,\n    })\n\n# Unsupervised (binary) from Week 03 + Week 04 \ncombined_unsup = None\nif 'unsup_df' in globals() and unsup_df is not None:\n    combined_unsup = unsup_df.copy()\nif 'unsup4_df' in globals() and unsup4_df is not None:\n    if combined_unsup is None:\n        combined_unsup = unsup4_df.copy()\n    else:\n        combined_unsup = pd.concat([combined_unsup, unsup4_df], ignore_index=True)\n\nif combined_unsup is not None:\n    for model_name, g in combined_unsup.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        y_pred_b = g[\"y_pred_bin\"].to_numpy()\n\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_b, y_pred_b, average=\"binary\", zero_division=0\n        )\n\n        scores = g[\"score\"].to_numpy() if \"score\" in g.columns else None\n        if scores is not None:\n            try:\n                roc_auc = roc_auc_score(y_true_b, scores)\n            except Exception:\n                roc_auc = np.nan\n            try:\n                pr_auc = average_precision_score(y_true_b, scores)\n            except Exception:\n                pr_auc = np.nan\n        else:\n            roc_auc = np.nan\n            pr_auc = np.nan\n\n        rows.append({\n            \"model\": model_name,\n            \"type\": \"unsupervised\",\n            \"precision_macro\": prec,\n            \"recall_macro\": rec,\n            \"f1_macro\": f1,\n            \"roc_auc_ovr_macro\": roc_auc,\n            \"pr_auc_ovr_macro\": pr_auc,\n        })\n\nif rows:\n    metric_table = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\nelse:\n    metric_table = pd.DataFrame(columns=[\n        \"model\", \"type\",\n        \"precision_macro\", \"recall_macro\", \"f1_macro\",\n        \"roc_auc_ovr_macro\", \"pr_auc_ovr_macro\"\n    ])\n\nmetric_table\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n1\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n2\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n3\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n4\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n5\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "href": "notebooks/Last/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Family-wise deep-dive (focus on R2L / U2R)",
    "text": "Family-wise deep-dive (focus on R2L / U2R)\n\nSelect the best supervised model by macro F1.\n\nInspect per-family precision/recall/F1.\n\nPlot its confusion matrix on the test set.\n\n\n# Select best supervised model by macro F1.\nsup_only = metric_table[metric_table[\"type\"] == \"supervised\"]\nbest_name = sup_only.sort_values(\"f1_macro\", ascending=False)[\"model\"].iloc[0]\nprint(\"Best supervised model by macro F1:\", best_name)\n\nbest_block = sup_pred_df[sup_pred_df[\"model\"] == best_name].reset_index(drop=True)\ny_true_best = best_block[\"y_true\"].to_numpy()\ny_pred_best = best_block[\"y_pred\"].to_numpy()\n\n# Recompute per-family metrics for the best model \nif best_name in per_class_by_model:\n    per_class_best = per_class_by_model[best_name]\nelse:\n    labels = np.unique(y_true_best)\n    metrics_best, per_class_best = multiclass_metrics(y_true_best, y_pred_best, labels=labels)\n\ndisplay(per_class_best.sort_values(\"f1\"))\n\nlabels_cm = np.unique(y_true_best)\ncm_best = confusion_matrix(y_true_best, y_pred_best, labels=labels_cm)\nfig, ax = plot_confusion(cm_best, labels_cm, f\"Confusion matrix – {best_name}\")\nplt.show()\n\nBest supervised model by macro F1: Random Forest (baseline)\n\n\n\n\n\n\n\n\n\nfamily\nprecision\nrecall\nf1\nsupport\nmodel\n\n\n\n\n3\n3\n0.818182\n0.900000\n0.857143\n10\nRandom Forest (baseline)\n\n\n2\n2\n0.995475\n0.977778\n0.986547\n225\nRandom Forest (baseline)\n\n\n1\n1\n0.998778\n0.993917\n0.996341\n822\nRandom Forest (baseline)\n\n\n4\n4\n0.999383\n0.999794\n0.999589\n19456\nRandom Forest (baseline)\n\n\n0\n0\n0.999987\n0.999974\n0.999981\n78292\nRandom Forest (baseline)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2L / U2R error analysis with examples\nUsing the best supervised model, we inspect misclassified R2L and U2R examples and show a few raw feature rows as concrete cases.\n\n# R2L and U2R error analysis.\n\nr2l_label = \"R2L\"  # adjust if needed\nu2r_label = \"U2R\"  # adjust if needed\n\nmask_r2l = (y_true_best == r2l_label)\nmask_u2r = (y_true_best == u2r_label)\n\nprint(\"R2L support:\", mask_r2l.sum())\nprint(\"U2R support:\", mask_u2r.sum())\n\nmis_r2l_idx = np.where(mask_r2l & (y_pred_best != r2l_label))[0]\nmis_u2r_idx = np.where(mask_u2r & (y_pred_best != u2r_label))[0]\n\nprint(\"Misclassified R2L:\", len(mis_r2l_idx))\nprint(\"Misclassified U2R:\", len(mis_u2r_idx))\n\n# Build a feature DataFrame for X_test \ntry:\n    import joblib\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = pre.get_feature_names_out()\n    X_test_df = pd.DataFrame(X_test, columns=feature_names)\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    X_test_df = pd.DataFrame(X_test)\n    X_test_df[\"family\"] = y_test\n\ndef show_examples(indices, family_name, n=5):\n    if len(indices) == 0:\n        print(f\"No misclassifications found for {family_name}.\")\n        return\n    idx_sel = indices[:n]\n    ex = X_test_df.iloc[idx_sel].copy()\n    ex[\"y_true\"] = y_true_best[idx_sel]\n    ex[\"y_pred\"] = y_pred_best[idx_sel]\n    display(ex)\n\nprint(\"\\nExample misclassified R2L samples:\")\nshow_examples(mis_r2l_idx, \"R2L\")\n\nprint(\"\\nExample misclassified U2R samples:\")\nshow_examples(mis_u2r_idx, \"U2R\")\n\nR2L support: 0\nU2R support: 0\nMisclassified R2L: 0\nMisclassified U2R: 0\n\nExample misclassified R2L samples:\nNo misclassifications found for R2L.\n\nExample misclassified U2R samples:\nNo misclassifications found for U2R."
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#robustness-checks",
    "href": "notebooks/Last/week_05_model_evaluation.html#robustness-checks",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Robustness checks",
    "text": "Robustness checks\n\n1) Threshold sensitivity for unsupervised models\nSweep a range of thresholds and inspect how binary F1 changes for each detector.\n\n# Threshold sensitivity for unsupervised detectors.\n\nif (unsup_df is not None) and (\"score\" in unsup_df.columns):\n    thresholds = np.linspace(0.1, 0.9, 9)  # generic [0.1, 0.9]; adjust if needed\n    \n    for model_name, g in unsup_df.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        scores = g[\"score\"].to_numpy()\n        \n        # Normalize scores into [0, 1] for a stable threshold range.\n        s_min, s_max = scores.min(), scores.max()\n        if s_max &gt; s_min:\n            scores_norm = (scores - s_min) / (s_max - s_min)\n        else:\n            scores_norm = scores\n        \n        df_thr = binary_threshold_metrics(y_true_b, scores_norm, thresholds)\n        df_thr[\"model\"] = model_name\n        display(df_thr)\n        \n        fig, ax = plt.subplots()\n        ax.plot(df_thr[\"threshold\"], df_thr[\"f1\"], marker=\"o\")\n        ax.set_xlabel(\"Threshold (on normalized score)\")\n        ax.set_ylabel(\"F1 (binary)\")\n        ax.set_title(f\"Threshold sensitivity – {model_name}\")\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No unsupervised scores available – threshold robustness skipped.\")\n\n\n\n\n\n\n\n\nthreshold\nprecision\nrecall\nf1\nmodel\n\n\n\n\n0\n0.1\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n1\n0.2\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n2\n0.3\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n3\n0.4\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n4\n0.5\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n5\n0.6\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n6\n0.7\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n7\n0.8\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n8\n0.9\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)"
  },
  {
    "objectID": "notebooks/Last/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "href": "notebooks/Last/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Final summary & ranked shortlist",
    "text": "Final summary & ranked shortlist\nWe now combine:\n\nGlobal metrics (F1, ROC-AUC, PR-AUC) for all methods into a single summary table and then draft a ranked shortlist with trade-offs.\n\n\n# Merge metric table only (runtime columns already included in metric_table)\n\nsummary = metric_table.copy()\n\nsummary_sorted = summary.sort_values(\n    [\"type\", \"f1_macro\"],\n    ascending=[True, False]\n).reset_index(drop=True)\n\nsummary_sorted\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n1\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n2\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n3\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n5\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009\n\n\n\n\n\n\n\n\n# Ranked shortlist narrative based on `summary_sorted`.\n\nimport pandas as pd\n\ndef _fmt(x):\n    if pd.isna(x):\n        return \"N/A\"\n    try:\n        return f\"{x:.3f}\"\n    except Exception:\n        return str(x)\n\nlines = []\nlines.append(\"### Ranked shortlist (auto-generated)\\n\")\n\n# Top supervised models\nif 'summary_sorted' not in globals():\n    print(\"summary_sorted is not defined. Run the previous cells first.\")\nelse:\n    sup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"supervised\"] if \"type\" in summary_sorted.columns else summary_sorted.copy()\n    unsup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"unsupervised\"] if \"type\" in summary_sorted.columns else summary_sorted.iloc[0:0]\n\n    if not sup.empty:\n        lines.append(\"**Supervised models (multiclass)**\\n\")\n        top_sup = sup.sort_values(\"f1_macro\", ascending=False).head(3)\n\n        for rank, (_, row) in enumerate(top_sup.iterrows(), start=1):\n            name = row[\"model\"]\n            f1m = _fmt(row.get(\"f1_macro\"))\n            prec = _fmt(row.get(\"precision_macro\"))\n            rec = _fmt(row.get(\"recall_macro\"))\n            roc = _fmt(row.get(\"roc_auc_ovr_macro\"))\n            prc = _fmt(row.get(\"pr_auc_ovr_macro\"))\n            ft = _fmt(row.get(\"fit_time_sec\")) if \"fit_time_sec\" in row else \"N/A\"\n            pt = _fmt(row.get(\"predict_time_sec\")) if \"predict_time_sec\" in row else \"N/A\"\n\n            lines.append(\n                f\"{rank}. **{name}** — macro F1 = {f1m}, \"\n                f\"precision = {prec}, recall = {rec}, \"\n                f\"ROC-AUC (OvR) = {roc}, PR-AUC = {prc}; \"\n                f\"fit time ≈ {ft}s, predict time ≈ {pt}s.\"\n            )\n\n        lines.append(\"\")\n\n    # Best unsupervised / semi-supervised model\n    if not unsup.empty:\n        best_unsup = unsup.sort_values(\"f1_macro\", ascending=False).iloc[0]\n        name = best_unsup[\"model\"]\n        f1m = _fmt(best_unsup.get(\"f1_macro\"))\n        prec = _fmt(best_unsup.get(\"precision_macro\"))\n        rec = _fmt(best_unsup.get(\"recall_macro\"))\n        roc = _fmt(best_unsup.get(\"roc_auc_ovr_macro\"))\n        prc = _fmt(best_unsup.get(\"pr_auc_ovr_macro\"))\n\n        lines.append(\"**Unsupervised / semi-supervised models (binary anomaly detection)**\\n\")\n        lines.append(\n            f\"- **{name}** — best binary macro F1 = {f1m}, \"\n            f\"precision = {prec}, recall = {rec}, \"\n            f\"ROC-AUC = {roc}, PR-AUC = {prc}. \"\n            \"Use as an anomaly tripwire to complement the supervised family classifier.\"\n        )\n    else:\n        lines.append(\"_No unsupervised results found in `summary_sorted`._\")\n\n    # Print the narrative as markdown-style text\n    print(\"\\n\".join(lines))\n\n### Ranked shortlist (auto-generated)\n\n**Supervised models (multiclass)**\n\n1. **Random Forest (baseline)** — macro F1 = 0.968, precision = 0.962, recall = 0.974, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n2. **Random Forest + SMOTE** — macro F1 = 0.960, precision = 0.948, recall = 0.975, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n3. **SVM-RBF + SMOTE** — macro F1 = 0.905, precision = 0.891, recall = 0.930, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n\n**Unsupervised / semi-supervised models (binary anomaly detection)**\n\n- **Z-Score (wrong_fragment, |Z|&gt;2)** — best binary macro F1 = 1.000, precision = 1.000, recall = 1.000, ROC-AUC = 1.000, PR-AUC = 1.000. Use as an anomaly tripwire to complement the supervised family classifier."
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html",
    "href": "notebooks/Last/week_03_stat_models.html",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "",
    "text": "Z-Score\nElliptic Envelope\nLocal Outlier Factor\nDBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n# Set up imports and load the raw NSL-KDD data. \nimport os, sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.unsupervised import (\n    make_binary_subset, z_score_feature, z_score_labels,\n    run_elliptic_envelope, run_mahalanobis,lof_grid_search, run_dbscan\n)\n\npaths = Paths().ensure()\nset_global_seed(42)\n\n# Load the same raw file used in Week 01.\nraw_path = paths.data_raw / 'NSL-KDD.raw'\nprint('Loading raw data from:', raw_path)\ndf = load_raw_nsl_kdd(raw_path)\ndf = map_attack_family(df)\ndf.shape\n\nLoading raw data from: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n(494021, 43)\n# Create a binary subset: normal vs teardrop. (Mirror Chapter_2.)\ndf_bin = make_binary_subset(df, normal_label='normal', attack_label='teardrop')\ndf_bin['Label'].value_counts()\n\nLabel\n0    97278\n1      979\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "href": "notebooks/Last/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Z-Score rule on a single feature",
    "text": "Z-Score rule on a single feature\nWe follow Chapter_2 and use the wrong_fragment feature with a threshold of |Z|&gt;2 to flag anomalies (attacks).\n\n# Compute Z-score for 'wrong_fragment' and map to labels using |Z| &gt; 2.\nz = z_score_feature(df_bin['wrong_fragment'])\ndf_bin['Z'] = z\ndf_bin['Z_Pred'] = z_score_labels(z, k=2.0)\n\ncm_z = confusion_matrix(df_bin['Label'], df_bin['Z_Pred'])\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_z, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('Z-Score rule on wrong_fragment (|Z|&gt;2)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_z\n\n\n\n\n\n\n\n\narray([[97278,     0],\n       [    0,   979]])"
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "href": "notebooks/Last/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Elliptic Envelope (Robust covariance)",
    "text": "Elliptic Envelope (Robust covariance)\nI use EllipticEnvelope with contamination=0.1 and random_state=0, dropping non-numeric / categorical columns\n\nell_res = run_elliptic_envelope(df_bin, contamination=0.1, random_state=0)\ncm_ell = ell_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_ell, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('EllipticEnvelope (contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_ell\n\n\n\n\n\n\n\n\narray([[87729,  9549],\n       [  702,   277]])"
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "href": "notebooks/Last/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Mahalanobis distance with robust covariance",
    "text": "Mahalanobis distance with robust covariance\nWe now fit a robust covariance model (MinCovDet) on the same numeric feature space, compute squared Mahalanobis distances, and threshold them using the chi-square quantile at alpha = 0.99. Points with distance² above the threshold are treated as anomalies.\n\n# Mahalanobis distance model. (Robust covariance + chi-square threshold.)\nfrom src.unsupervised import run_mahalanobis\n\nmahal_res = run_mahalanobis(df_bin, alpha=0.99, robust=True)\n\ncm_mahal = mahal_res.confusion\nprint(\"Chi-square threshold (alpha=0.99):\", mahal_res.threshold)\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_mahal, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Mahalanobis distance (robust covariance, α=0.99)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\ncm_mahal\n\nChi-square threshold (alpha=0.99): 63.690739751564465\n\n\n\n\n\n\n\n\n\narray([[53378, 43900],\n       [    0,   979]])"
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html#local-outlier-factor-lof",
    "href": "notebooks/Last/week_03_stat_models.html#local-outlier-factor-lof",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\nFirst, I run a single LOF model with a modest neighborhood size; then, I grid search over k to inspect accuracy, precision, and recall as a function of the neighborhood size.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nactual = df_bin['Label'].to_numpy()\ndrop_cols = [c for c in ['Label', 'label', 'target', 'protocol_type', 'service', 'flag', 'family'] if c in df_bin.columns]\nX = df_bin.drop(columns=drop_cols)\n\nk0 = 5\nlof0 = LocalOutlierFactor(n_neighbors=k0, contamination=0.1)\npred0 = lof0.fit_predict(X)\npred0_rescored = np.where(pred0 == -1, 1, 0)\ncm_lof0 = confusion_matrix(actual, pred0_rescored)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_lof0, annot=True, fmt='d', cmap='YlGnBu')\nplt.title(f'LOF (k={k0}, contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_lof0\n\n\n\n\n\n\n\n\narray([[87465,  9813],\n       [  966,    13]])\n\n\n\n# Grid search over k for LOF\nk_values = list(range(100, 1501, 100))  # 100, 200, ..., 3000\nlof_grid = lof_grid_search(df_bin, k_values)\n\nplt.figure(figsize=(8,5))\nplt.plot(lof_grid.ks, lof_grid.accuracies, label='Accuracy')\nplt.plot(lof_grid.ks, lof_grid.precisions, label='Precision')\nplt.plot(lof_grid.ks, lof_grid.recalls, label='Recall')\nplt.xlabel('k (n_neighbors)')\nplt.ylabel('Metric (%)')\nplt.title('LOF metrics vs k')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "notebooks/Last/week_03_stat_models.html#dbscan",
    "href": "notebooks/Last/week_03_stat_models.html#dbscan",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "DBSCAN",
    "text": "DBSCAN\nI run DBSCAN with eps=0.2 and min_samples=5 on the same feature space, rescoring cluster labels so that noise points are treated as anomalies.\n\ndb_res = run_dbscan(df_bin, eps=0.2, min_samples=5)\ncm_db = db_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_db, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('DBSCAN (eps=0.2, min_samples=5)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_db\n\n\n\n\n\n\n\n\narray([[11475, 85803],\n       [    0,   979]])\n\n\n\n# Save anomaly detection results for Week 05\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\n# Determine project root and reports directory\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk3_dir.mkdir(parents=True, exist_ok=True)\n\nframes = []\n\n# 1) Z-SCORE MODEL \nif {\"Label\", \"Z\", \"Z_Pred\"}.issubset(df_bin.columns):\n    z_df = pd.DataFrame({\n        \"model\":      \"Z-Score (wrong_fragment, |Z|&gt;2)\",\n        \"y_true_bin\": df_bin[\"Label\"].astype(int).to_numpy(),\n        \"y_pred_bin\": df_bin[\"Z_Pred\"].astype(int).to_numpy(),\n        # Score: |Z| (higher = more anomalous)\n        \"score\":      np.abs(df_bin[\"Z\"]).to_numpy(),\n    })\n    frames.append(z_df)\n    print(\"Added Z-Score results.\")\nelse:\n    print(\"Z-score fields not found. Skipping Z-score model.\")\n\n\n# Helper function to add result objects\ndef add_res_object(name):\n    obj = globals().get(name, None)\n    if obj is None:\n        print(f\"{name} not found, skipping.\")\n        return\n    \n    needed = [\"name\", \"y_true\", \"y_pred\", \"scores\"]\n    missing = [a for a in needed if not hasattr(obj, a)]\n    if missing:\n        print(f\"{name} missing attributes {missing}, skipping.\")\n        return\n    \n    df = pd.DataFrame({\n        \"model\":      obj.name,\n        \"y_true_bin\": np.array(obj.y_true).astype(int),\n        \"y_pred_bin\": np.array(obj.y_pred).astype(int),\n        \"score\":      np.array(obj.scores).astype(float),\n    })\n    frames.append(df)\n    print(f\"Added results for {obj.name}.\")\n\n\n# Elliptic Envelope\n# Mahalanobis\n# LOF\n# DBSCAN\nadd_res_object(\"ell_res\")\nadd_res_object(\"mahal_res\")\nadd_res_object(\"lof0\")\nadd_res_object(\"db_res\")\n\n\n# Save all results\nif not frames:\n    raise ValueError(\"No Week 03 results found. Run the model cells first.\")\n\nunsup_df = pd.concat(frames, ignore_index=True)\n\ncsv_path     = wk3_dir / \"week03_unsupervised_predictions.csv\"\n\nunsup_df.to_csv(csv_path, index=False)\n\n\nprint(\"\\nSaved Week 03 anomaly detection results:\")\nprint(\" -\", csv_path)\n\nunsup_df.head()\n\nAdded Z-Score results.\nell_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nmahal_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nlof0 missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\ndb_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\n\nSaved Week 03 anomaly detection results:\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.parquet\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/Last/week_01_data_prep.html",
    "href": "notebooks/Last/week_01_data_prep.html",
    "title": "Week 01 — Problem Definition & Data Preparation",
    "section": "",
    "text": "Objective. Load NSL-KDD corrected 10% file, encode categoricals, scale numerics, and persist split artifacts.\n\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n\n\n\n# Set up environment and constants. \nfrom pathlib import Path\nimport pandas as pd\nfrom src.utils import set_global_seed, Paths\nfrom src.io import load_raw_nsl_kdd, map_attack_family, save_numpy\nfrom src.prep import split_and_fit\nimport joblib\nset_global_seed(42)\npaths = Paths().ensure()\n\n\n# Load data from corrected file. \nraw_path = paths.data_raw / \"NSL-KDD.raw\"\nprint(raw_path)  # optional: confirm the resolved path\ndf = load_raw_nsl_kdd(raw_path)\ndf = map_attack_family(df)\ndisplay(df.head())\ndf.shape\n\nC:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n\n\n\n\n\n\n\nduration\nprotocol_type\nservice\nflag\nsrc_bytes\ndst_bytes\nland\nwrong_fragment\nurgent\nhot\n...\ndst_host_same_srv_rate\ndst_host_diff_srv_rate\ndst_host_same_src_port_rate\ndst_host_srv_diff_host_rate\ndst_host_serror_rate\ndst_host_srv_serror_rate\ndst_host_rerror_rate\ndst_host_srv_rerror_rate\nlabel\nfamily\n\n\n\n\n0\n0\ntcp\nhttp\nSF\n181\n5450\n0\n0\n0\n0\n...\n1.0\n0.0\n0.11\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n1\n0\ntcp\nhttp\nSF\n239\n486\n0\n0\n0\n0\n...\n1.0\n0.0\n0.05\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n2\n0\ntcp\nhttp\nSF\n235\n1337\n0\n0\n0\n0\n...\n1.0\n0.0\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n3\n0\ntcp\nhttp\nSF\n219\n1337\n0\n0\n0\n0\n...\n1.0\n0.0\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n4\n0\ntcp\nhttp\nSF\n217\n2032\n0\n0\n0\n0\n...\n1.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n\n\n5 rows × 43 columns\n\n\n\n(494021, 43)\n\n\n\n# Check family distribution and Confirm class imbalance.\nfam_counts = df['family'].value_counts()\nfam_perc = (fam_counts / len(df)).round(4)\ndisplay(pd.DataFrame({'count': fam_counts, 'percent': fam_perc}))\n\n\n\n\n\n\n\n\ncount\npercent\n\n\nfamily\n\n\n\n\n\n\nDoS\n391458\n0.7924\n\n\nnormal\n97278\n0.1969\n\n\nProbe\n4107\n0.0083\n\n\nR2L\n1126\n0.0023\n\n\nU2R\n52\n0.0001\n\n\n\n\n\n\n\n\n# Split, encode, and scale. (Fit preprocessor on train only.)\nX_train, X_test, y_train, y_test, pre = split_and_fit(df, paths)\nX_train.shape, X_test.shape\n\n((395216, 115), (98805, 115))\n\n\n\n# Ensure y labels are integer-encoded (required for ML models)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test  = le.transform(y_test)\n\nprint(\"Label classes:\", le.classes_)\n\nLabel classes: ['DoS' 'Probe' 'R2L' 'U2R' 'normal']\n\n\n\n# Persist artifacts. (Save arrays and preprocessor.)\nsave_numpy(paths.data_proc / 'X_train.npy', X_train)\nsave_numpy(paths.data_proc / 'X_test.npy', X_test)\nsave_numpy(paths.data_proc / 'y_train.npy', y_train)\nsave_numpy(paths.data_proc / 'y_test.npy', y_test)\njoblib.dump(pre, paths.data_proc / 'preprocessor.joblib')\nprint('Saved processed arrays and preprocessor to', paths.data_proc)\n\nSaved processed arrays and preprocessor to C:\\Users\\mehra\\Final_Project\\data\\processed\n\n\n\n# Brief summary. (Report shapes and missing checks.)\nsummary = {\n    'rows': len(df), 'cols': df.shape[1],\n    'X_train_shape': X_train.shape, 'X_test_shape': X_test.shape,\n    'y_train_counts': {k:int(v) for k,v in pd.Series(y_train).value_counts().items()},\n    'y_test_counts': {k:int(v) for k,v in pd.Series(y_test).value_counts().items()},\n    'missing_total': int(df.isna().sum().sum())\n}\nsummary\n\n{'rows': 494021,\n 'cols': 43,\n 'X_train_shape': (395216, 115),\n 'X_test_shape': (98805, 115),\n 'y_train_counts': {0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42},\n 'y_test_counts': {0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10},\n 'missing_total': 0}"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html",
    "href": "notebooks/final/week_05_model_evaluation.html",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "",
    "text": "# Imports, project root, and paths.\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*covariance matrix associated to your dataset is not full rank.*\"\n)\n\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Sklearn metrics & helpers\nfrom sklearn.metrics import (\n    confusion_matrix,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    average_precision_score,\n)\nfrom sklearn.calibration import CalibrationDisplay\n\n# Project utilities\nfrom src.utils import Paths, set_global_seed\n\n# Optional: helper for binary label mapping (used in earlier weeks)\ntry:\n    from src.eval import to_binary  # used in earlier weeks\nexcept ImportError:\n    to_binary = None\n\nset_global_seed(42)\npaths = Paths().ensure()\n\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\nfrom pathlib import Path as _Path\n\n_this_dir = _Path.cwd()\n_project_root = _this_dir.parent if _this_dir.name == \"notebooks\" else _this_dir\n\n# Try to use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", _project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\n\nwk3_dir.mkdir(parents=True, exist_ok=True)\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports folder:\", wk3_dir)\nprint(\"Week 04 reports folder:\", wk4_dir)\n\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_04\n# Load processed train/test arrays from Week 01 \nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\n# Optional binary labels (normal vs attack) if a helper exists.\nif to_binary is not None:\n    normal_label = \"normal\"  # adjust if your label encoding is different\n    y_train_bin = to_binary(y_train, normal_label)\n    y_test_bin  = to_binary(y_test, normal_label)\n    print(\"Binary train counts:\", Counter(y_train_bin))\n    print(\"Binary test  counts:\", Counter(y_test_bin))\nelse:\n    y_train_bin = None\n    y_test_bin = None\n    print(\"NOTE: src.eval.to_binary not found – binary labels not created automatically.\")\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\ny_test  classes: Counter({0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10})\nNOTE: src.eval.to_binary not found – binary labels not created automatically.\n# Helper functions for metrics and plotting.\n\ndef multiclass_metrics(y_true, y_pred, y_proba=None, labels=None, average=\"macro\"):\n    \"\"\"Compute macro + per-class metrics for a multiclass setting.\n\n    Parameters\n    ----------\n    y_true, y_pred : array-like\n        True and predicted class labels.\n    y_proba : array-like of shape (n_samples, n_classes), optional\n        Predicted probabilities for each class (for ROC/PR).\n    labels : list, optional\n        Label order. If None, inferred from y_true.\n    average : str\n        Averaging scheme for global metrics.\n    \"\"\"\n    if labels is None:\n        labels = np.unique(y_true)\n    \n    prec, rec, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=average, zero_division=0\n    )\n    \n    per_class_df = pd.DataFrame({\n        \"family\": labels,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1\": f1,\n        \"support\": support,\n    })\n    \n    metrics = {\n        \"precision_macro\": macro_prec,\n        \"recall_macro\": macro_rec,\n        \"f1_macro\": macro_f1,\n    }\n    \n    # ROC-AUC (OvR) and PR-AUC\n    if y_proba is not None:\n        # Build one-hot encoding from labels.\n        label_to_idx = {lab: i for i, lab in enumerate(labels)}\n        y_true_idx = np.array([label_to_idx[lab] for lab in y_true])\n        Y_true_ovr = np.eye(len(labels))[y_true_idx]\n        \n        try:\n            roc_macro_ovr = roc_auc_score(Y_true_ovr, y_proba, average=\"macro\", multi_class=\"ovr\")\n        except Exception:\n            roc_macro_ovr = np.nan\n        \n        try:\n            pr_macro_ovr = average_precision_score(Y_true_ovr, y_proba, average=\"macro\")\n        except Exception:\n            pr_macro_ovr = np.nan\n        \n        metrics[\"roc_auc_ovr_macro\"] = roc_macro_ovr\n        metrics[\"pr_auc_ovr_macro\"] = pr_macro_ovr\n    else:\n        metrics[\"roc_auc_ovr_macro\"] = np.nan\n        metrics[\"pr_auc_ovr_macro\"] = np.nan\n    \n    return metrics, per_class_df\n\n\ndef binary_threshold_metrics(y_true_bin, scores, thresholds):\n    \"\"\"Sweep thresholds over anomaly scores for binary detectors.\n\n    Assumes higher `scores` = more anomalous (1 = attack, 0 = normal).\n    Returns a DataFrame with precision/recall/F1 for each threshold.\n    \"\"\"\n    rows = []\n    for thr in thresholds:\n        y_pred_bin = (scores &gt;= thr).astype(int)\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n        )\n        rows.append({\n            \"threshold\": thr,\n            \"precision\": prec,\n            \"recall\": rec,\n            \"f1\": f1,\n        })\n    return pd.DataFrame(rows)\n\n\ndef plot_confusion(cm, labels, title):\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_title(title)\n    plt.tight_layout()\n    return fig, ax"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#load-week-04-supervised-results",
    "href": "notebooks/final/week_05_model_evaluation.html#load-week-04-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 supervised results",
    "text": "Load Week 04 supervised results\n\n# Load Week 04 supervised predictions.\npred_parquet = wk4_dir / \"week04_supervised_predictions.parquet\"\npred_csv     = wk4_dir / \"week04_supervised_predictions.csv\"\n\nif pred_parquet.exists():\n    sup_pred_df = pd.read_parquet(pred_parquet)\nelif pred_csv.exists():\n    sup_pred_df = pd.read_csv(pred_csv)\nelse:\n    raise FileNotFoundError(\n        f\"Could not find Week 04 prediction file. Expected one of:\\n{pred_parquet}\\n{pred_csv}\\n\"\n        \"Please save your Week 04 test predictions to one of these paths.\"\n    )\n\nprint(\"Loaded Week 04 supervised predictions:\")\ndisplay(sup_pred_df.head())\n\n# Identify probability columns and families from colnames (proba_&lt;family&gt;).\nproba_cols = [c for c in sup_pred_df.columns if c.startswith(\"proba_\")]\nfamilies_from_proba = [c.replace(\"proba_\", \"\") for c in proba_cols]\n\nLoaded Week 04 supervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "href": "notebooks/final/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 03 unsupervised / statistical results",
    "text": "Load Week 03 unsupervised / statistical results\n\n# Load Week 03 unsupervised predictions\nunsup_parquet = wk3_dir / \"week03_unsupervised_predictions.parquet\"\nunsup_csv     = wk3_dir / \"week03_unsupervised_predictions.csv\"\n\nunsup_df = None\nif unsup_parquet.exists():\n    unsup_df = pd.read_parquet(unsup_parquet)\nelif unsup_csv.exists():\n    unsup_df = pd.read_csv(unsup_csv)\n\nif unsup_df is not None:\n    print(\"Loaded Week 03 unsupervised predictions:\")\n    display(unsup_df.head())\nelse:\n    print(\"Week 03 unsupervised prediction file not found — \"\n          \"unsupervised models will be skipped unless you add it.\")\n\nLoaded Week 03 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "href": "notebooks/final/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 unsupervised / semi-supervised results",
    "text": "Load Week 04 unsupervised / semi-supervised results\n\n# Load Week 04 unsupervised predictions\nunsup4_parquet = wk4_dir / \"week04_unsupervised_predictions.parquet\"\nunsup4_csv     = wk4_dir / \"week04_unsupervised_predictions.csv\"\n\nunsup4_df = None\nif unsup4_parquet.exists():\n    unsup4_df = pd.read_parquet(unsup4_parquet)\nelif unsup4_csv.exists():\n    unsup4_df = pd.read_csv(unsup4_csv)\n\nif unsup4_df is not None:\n    print(\"Loaded Week 04 unsupervised predictions:\")\n    display(unsup4_df.head())\nelse:\n    print(\"Week 04 unsupervised prediction file not found — \"\n          \"these models will be skipped unless you add it.\")\n\nLoaded Week 04 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#metric-table",
    "href": "notebooks/final/week_05_model_evaluation.html#metric-table",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Metric Table",
    "text": "Metric Table\n\nPrecision, Recall, F1 (macro)\nROC-AUC (OvR) and PR-AUC (macro) where probabilities/scores are available\n\nSupervised models are treated as multiclass, while unsupervised models are evaluated in the binary attack vs normal setting.\n\n# Build metric table from saved Week 03 / Week 04 outputs.\n\nrows = []\nper_class_by_model = {}  # store per-family metrics for later deep-dive\n\n# Supervised (multiclass)\nfor model_name, g in sup_pred_df.groupby(\"model\"):\n    y_true = g[\"y_true\"].to_numpy()\n    y_pred = g[\"y_pred\"].to_numpy()\n\n    # Use proba_* columns if present\n    if proba_cols:\n        y_proba = g[proba_cols].to_numpy()\n        labels_order = families_from_proba\n    else:\n        y_proba = None\n        labels_order = np.unique(y_true)\n\n    metrics, per_class = multiclass_metrics(\n        y_true,\n        y_pred,\n        y_proba=y_proba,\n        labels=labels_order,\n    )\n    per_class[\"model\"] = model_name\n    per_class_by_model[model_name] = per_class\n\n    rows.append({\n        \"model\": model_name,\n        \"type\": \"supervised\",\n        **metrics,\n    })\n\n# Unsupervised (binary) from Week 03 + Week 04 \ncombined_unsup = None\nif 'unsup_df' in globals() and unsup_df is not None:\n    combined_unsup = unsup_df.copy()\nif 'unsup4_df' in globals() and unsup4_df is not None:\n    if combined_unsup is None:\n        combined_unsup = unsup4_df.copy()\n    else:\n        combined_unsup = pd.concat([combined_unsup, unsup4_df], ignore_index=True)\n\nif combined_unsup is not None:\n    for model_name, g in combined_unsup.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        y_pred_b = g[\"y_pred_bin\"].to_numpy()\n\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_b, y_pred_b, average=\"binary\", zero_division=0\n        )\n\n        scores = g[\"score\"].to_numpy() if \"score\" in g.columns else None\n        if scores is not None:\n            try:\n                roc_auc = roc_auc_score(y_true_b, scores)\n            except Exception:\n                roc_auc = np.nan\n            try:\n                pr_auc = average_precision_score(y_true_b, scores)\n            except Exception:\n                pr_auc = np.nan\n        else:\n            roc_auc = np.nan\n            pr_auc = np.nan\n\n        rows.append({\n            \"model\": model_name,\n            \"type\": \"unsupervised\",\n            \"precision_macro\": prec,\n            \"recall_macro\": rec,\n            \"f1_macro\": f1,\n            \"roc_auc_ovr_macro\": roc_auc,\n            \"pr_auc_ovr_macro\": pr_auc,\n        })\n\nif rows:\n    metric_table = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\nelse:\n    metric_table = pd.DataFrame(columns=[\n        \"model\", \"type\",\n        \"precision_macro\", \"recall_macro\", \"f1_macro\",\n        \"roc_auc_ovr_macro\", \"pr_auc_ovr_macro\"\n    ])\n\nmetric_table\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n1\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n2\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n3\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n4\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n5\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "href": "notebooks/final/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Family-wise deep-dive (focus on R2L / U2R)",
    "text": "Family-wise deep-dive (focus on R2L / U2R)\n\nSelect the best supervised model by macro F1.\n\nInspect per-family precision/recall/F1.\n\nPlot its confusion matrix on the test set.\n\n\n# Select best supervised model by macro F1.\nsup_only = metric_table[metric_table[\"type\"] == \"supervised\"]\nbest_name = sup_only.sort_values(\"f1_macro\", ascending=False)[\"model\"].iloc[0]\nprint(\"Best supervised model by macro F1:\", best_name)\n\nbest_block = sup_pred_df[sup_pred_df[\"model\"] == best_name].reset_index(drop=True)\ny_true_best = best_block[\"y_true\"].to_numpy()\ny_pred_best = best_block[\"y_pred\"].to_numpy()\n\n# Recompute per-family metrics for the best model \nif best_name in per_class_by_model:\n    per_class_best = per_class_by_model[best_name]\nelse:\n    labels = np.unique(y_true_best)\n    metrics_best, per_class_best = multiclass_metrics(y_true_best, y_pred_best, labels=labels)\n\ndisplay(per_class_best.sort_values(\"f1\"))\n\nlabels_cm = np.unique(y_true_best)\ncm_best = confusion_matrix(y_true_best, y_pred_best, labels=labels_cm)\nfig, ax = plot_confusion(cm_best, labels_cm, f\"Confusion matrix – {best_name}\")\nplt.show()\n\nBest supervised model by macro F1: Random Forest (baseline)\n\n\n\n\n\n\n\n\n\nfamily\nprecision\nrecall\nf1\nsupport\nmodel\n\n\n\n\n3\n3\n0.818182\n0.900000\n0.857143\n10\nRandom Forest (baseline)\n\n\n2\n2\n0.995475\n0.977778\n0.986547\n225\nRandom Forest (baseline)\n\n\n1\n1\n0.998778\n0.993917\n0.996341\n822\nRandom Forest (baseline)\n\n\n4\n4\n0.999383\n0.999794\n0.999589\n19456\nRandom Forest (baseline)\n\n\n0\n0\n0.999987\n0.999974\n0.999981\n78292\nRandom Forest (baseline)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2L / U2R error analysis with examples\nUsing the best supervised model, we inspect misclassified R2L and U2R examples and show a few raw feature rows as concrete cases.\n\n# R2L and U2R error analysis.\n\nr2l_label = \"R2L\"  # adjust if needed\nu2r_label = \"U2R\"  # adjust if needed\n\nmask_r2l = (y_true_best == r2l_label)\nmask_u2r = (y_true_best == u2r_label)\n\nprint(\"R2L support:\", mask_r2l.sum())\nprint(\"U2R support:\", mask_u2r.sum())\n\nmis_r2l_idx = np.where(mask_r2l & (y_pred_best != r2l_label))[0]\nmis_u2r_idx = np.where(mask_u2r & (y_pred_best != u2r_label))[0]\n\nprint(\"Misclassified R2L:\", len(mis_r2l_idx))\nprint(\"Misclassified U2R:\", len(mis_u2r_idx))\n\n# Build a feature DataFrame for X_test \ntry:\n    import joblib\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = pre.get_feature_names_out()\n    X_test_df = pd.DataFrame(X_test, columns=feature_names)\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    X_test_df = pd.DataFrame(X_test)\n    X_test_df[\"family\"] = y_test\n\ndef show_examples(indices, family_name, n=5):\n    if len(indices) == 0:\n        print(f\"No misclassifications found for {family_name}.\")\n        return\n    idx_sel = indices[:n]\n    ex = X_test_df.iloc[idx_sel].copy()\n    ex[\"y_true\"] = y_true_best[idx_sel]\n    ex[\"y_pred\"] = y_pred_best[idx_sel]\n    display(ex)\n\nprint(\"\\nExample misclassified R2L samples:\")\nshow_examples(mis_r2l_idx, \"R2L\")\n\nprint(\"\\nExample misclassified U2R samples:\")\nshow_examples(mis_u2r_idx, \"U2R\")\n\nR2L support: 0\nU2R support: 0\nMisclassified R2L: 0\nMisclassified U2R: 0\n\nExample misclassified R2L samples:\nNo misclassifications found for R2L.\n\nExample misclassified U2R samples:\nNo misclassifications found for U2R."
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#robustness-checks",
    "href": "notebooks/final/week_05_model_evaluation.html#robustness-checks",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Robustness checks",
    "text": "Robustness checks\n\n1) Threshold sensitivity for unsupervised models\nSweep a range of thresholds and inspect how binary F1 changes for each detector.\n\n# Threshold sensitivity for unsupervised detectors.\n\nif (unsup_df is not None) and (\"score\" in unsup_df.columns):\n    thresholds = np.linspace(0.1, 0.9, 9)  # generic [0.1, 0.9]; adjust if needed\n    \n    for model_name, g in unsup_df.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        scores = g[\"score\"].to_numpy()\n        \n        # Normalize scores into [0, 1] for a stable threshold range.\n        s_min, s_max = scores.min(), scores.max()\n        if s_max &gt; s_min:\n            scores_norm = (scores - s_min) / (s_max - s_min)\n        else:\n            scores_norm = scores\n        \n        df_thr = binary_threshold_metrics(y_true_b, scores_norm, thresholds)\n        df_thr[\"model\"] = model_name\n        display(df_thr)\n        \n        fig, ax = plt.subplots()\n        ax.plot(df_thr[\"threshold\"], df_thr[\"f1\"], marker=\"o\")\n        ax.set_xlabel(\"Threshold (on normalized score)\")\n        ax.set_ylabel(\"F1 (binary)\")\n        ax.set_title(f\"Threshold sensitivity – {model_name}\")\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No unsupervised scores available – threshold robustness skipped.\")\n\n\n\n\n\n\n\n\nthreshold\nprecision\nrecall\nf1\nmodel\n\n\n\n\n0\n0.1\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n1\n0.2\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n2\n0.3\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n3\n0.4\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n4\n0.5\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n5\n0.6\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n6\n0.7\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n7\n0.8\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n8\n0.9\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)"
  },
  {
    "objectID": "notebooks/final/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "href": "notebooks/final/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Final summary & ranked shortlist",
    "text": "Final summary & ranked shortlist\nWe now combine:\n\nGlobal metrics (F1, ROC-AUC, PR-AUC) for all methods into a single summary table and then draft a ranked shortlist with trade-offs.\n\n\n# Merge metric table only (runtime columns already included in metric_table)\n\nsummary = metric_table.copy()\n\nsummary_sorted = summary.sort_values(\n    [\"type\", \"f1_macro\"],\n    ascending=[True, False]\n).reset_index(drop=True)\n\nsummary_sorted\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n1\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n2\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n3\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n5\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009\n\n\n\n\n\n\n\n\n# Ranked shortlist narrative based on `summary_sorted`.\n\nimport pandas as pd\n\ndef _fmt(x):\n    if pd.isna(x):\n        return \"N/A\"\n    try:\n        return f\"{x:.3f}\"\n    except Exception:\n        return str(x)\n\nlines = []\nlines.append(\"### Ranked shortlist (auto-generated)\\n\")\n\n# Top supervised models\nif 'summary_sorted' not in globals():\n    print(\"summary_sorted is not defined. Run the previous cells first.\")\nelse:\n    sup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"supervised\"] if \"type\" in summary_sorted.columns else summary_sorted.copy()\n    unsup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"unsupervised\"] if \"type\" in summary_sorted.columns else summary_sorted.iloc[0:0]\n\n    if not sup.empty:\n        lines.append(\"**Supervised models (multiclass)**\\n\")\n        top_sup = sup.sort_values(\"f1_macro\", ascending=False).head(3)\n\n        for rank, (_, row) in enumerate(top_sup.iterrows(), start=1):\n            name = row[\"model\"]\n            f1m = _fmt(row.get(\"f1_macro\"))\n            prec = _fmt(row.get(\"precision_macro\"))\n            rec = _fmt(row.get(\"recall_macro\"))\n            roc = _fmt(row.get(\"roc_auc_ovr_macro\"))\n            prc = _fmt(row.get(\"pr_auc_ovr_macro\"))\n            ft = _fmt(row.get(\"fit_time_sec\")) if \"fit_time_sec\" in row else \"N/A\"\n            pt = _fmt(row.get(\"predict_time_sec\")) if \"predict_time_sec\" in row else \"N/A\"\n\n            lines.append(\n                f\"{rank}. **{name}** — macro F1 = {f1m}, \"\n                f\"precision = {prec}, recall = {rec}, \"\n                f\"ROC-AUC (OvR) = {roc}, PR-AUC = {prc}; \"\n                f\"fit time ≈ {ft}s, predict time ≈ {pt}s.\"\n            )\n\n        lines.append(\"\")\n\n    # Best unsupervised / semi-supervised model\n    if not unsup.empty:\n        best_unsup = unsup.sort_values(\"f1_macro\", ascending=False).iloc[0]\n        name = best_unsup[\"model\"]\n        f1m = _fmt(best_unsup.get(\"f1_macro\"))\n        prec = _fmt(best_unsup.get(\"precision_macro\"))\n        rec = _fmt(best_unsup.get(\"recall_macro\"))\n        roc = _fmt(best_unsup.get(\"roc_auc_ovr_macro\"))\n        prc = _fmt(best_unsup.get(\"pr_auc_ovr_macro\"))\n\n        lines.append(\"**Unsupervised / semi-supervised models (binary anomaly detection)**\\n\")\n        lines.append(\n            f\"- **{name}** — best binary macro F1 = {f1m}, \"\n            f\"precision = {prec}, recall = {rec}, \"\n            f\"ROC-AUC = {roc}, PR-AUC = {prc}. \"\n            \"Use as an anomaly tripwire to complement the supervised family classifier.\"\n        )\n    else:\n        lines.append(\"_No unsupervised results found in `summary_sorted`._\")\n\n    # Print the narrative as markdown-style text\n    print(\"\\n\".join(lines))\n\n### Ranked shortlist (auto-generated)\n\n**Supervised models (multiclass)**\n\n1. **Random Forest (baseline)** — macro F1 = 0.968, precision = 0.962, recall = 0.974, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n2. **Random Forest + SMOTE** — macro F1 = 0.960, precision = 0.948, recall = 0.975, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n3. **SVM-RBF + SMOTE** — macro F1 = 0.905, precision = 0.891, recall = 0.930, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n\n**Unsupervised / semi-supervised models (binary anomaly detection)**\n\n- **Z-Score (wrong_fragment, |Z|&gt;2)** — best binary macro F1 = 1.000, precision = 1.000, recall = 1.000, ROC-AUC = 1.000, PR-AUC = 1.000. Use as an anomaly tripwire to complement the supervised family classifier."
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html",
    "href": "notebooks/final/week_03_stat_models.html",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "",
    "text": "Objective: Reproduce the Z-Score, Elliptic Envelope, Local Outlier Factor, and DBSCAN experiments from Chapter_2.ipynb on the NSL-KDD-style data, while using the common project utilities (Paths, set_global_seed).\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*covariance matrix associated to your dataset is not full rank.*\"\n)\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n# Set up imports and load the raw NSL-KDD data. (Match Chapter_2 sampling logic.)\nimport os, sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.unsupervised import (\n    make_binary_subset, z_score_feature, z_score_labels,\n    run_elliptic_envelope, run_mahalanobis,lof_grid_search, run_dbscan\n)\n\npaths = Paths().ensure()\nset_global_seed(42)\n\n# Load the same raw file used in Week 01.\nraw_path = paths.data_raw / 'NSL-KDD.raw'\nprint('Loading raw data from:', raw_path)\ndf = load_raw_nsl_kdd(raw_path)\ndf = map_attack_family(df)\ndf.shape\n\nLoading raw data from: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n(494021, 43)\n# Create a binary subset: normal vs teardrop. (Mirror Chapter_2.)\ndf_bin = make_binary_subset(df, normal_label='normal', attack_label='teardrop')\ndf_bin['Label'].value_counts()\n\nLabel\n0    97278\n1      979\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "href": "notebooks/final/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Z-Score rule on a single feature",
    "text": "Z-Score rule on a single feature\nWe follow Chapter_2 and use the wrong_fragment feature with a threshold of |Z|&gt;2 to flag anomalies (attacks).\n\n# Compute Z-score for 'wrong_fragment' and map to labels using |Z| &gt; 2.\nz = z_score_feature(df_bin['wrong_fragment'])\ndf_bin['Z'] = z\ndf_bin['Z_Pred'] = z_score_labels(z, k=2.0)\n\ncm_z = confusion_matrix(df_bin['Label'], df_bin['Z_Pred'])\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_z, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('Z-Score rule on wrong_fragment (|Z|&gt;2)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_z\n\n\n\n\n\n\n\n\narray([[97278,     0],\n       [    0,   979]], dtype=int64)"
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "href": "notebooks/final/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Elliptic Envelope (Robust covariance)",
    "text": "Elliptic Envelope (Robust covariance)\nWe use EllipticEnvelope with contamination=0.1 and random_state=0, dropping non-numeric / categorical columns, as in Chapter_2.\n\nell_res = run_elliptic_envelope(df_bin, contamination=0.1, random_state=0)\ncm_ell = ell_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_ell, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('EllipticEnvelope (contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_ell\n\n\n\n\n\n\n\n\narray([[87507,  9771],\n       [  924,    55]], dtype=int64)"
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "href": "notebooks/final/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Mahalanobis distance with robust covariance",
    "text": "Mahalanobis distance with robust covariance\nWe now fit a robust covariance model (MinCovDet) on the same numeric feature space, compute squared Mahalanobis distances, and threshold them using the chi-square quantile at \\(\\alpha = 0.99\\). Points with distance² above the threshold are treated as anomalies.\n\n# Mahalanobis distance model. (Robust covariance + chi-square threshold.)\nfrom src.unsupervised import run_mahalanobis\n\nmahal_res = run_mahalanobis(df_bin, alpha=0.99, robust=True)\n\ncm_mahal = mahal_res.confusion\nprint(\"Chi-square threshold (alpha=0.99):\", mahal_res.threshold)\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_mahal, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Mahalanobis distance (robust covariance, α=0.99)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\ncm_mahal\n\nChi-square threshold (alpha=0.99): 63.690739751564465\n\n\n\n\n\n\n\n\n\narray([[55137, 42141],\n       [    0,   979]], dtype=int64)"
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html#local-outlier-factor-lof",
    "href": "notebooks/final/week_03_stat_models.html#local-outlier-factor-lof",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\nFirst, we run a single LOF model with a modest neighborhood size; then, we grid search over k to inspect accuracy, precision, and recall as a function of the neighborhood size.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nactual = df_bin['Label'].to_numpy()\ndrop_cols = [c for c in ['Label', 'label', 'target', 'protocol_type', 'service', 'flag', 'family'] if c in df_bin.columns]\nX = df_bin.drop(columns=drop_cols)\n\nk0 = 5\nlof0 = LocalOutlierFactor(n_neighbors=k0, contamination=0.1)\npred0 = lof0.fit_predict(X)\npred0_rescored = np.where(pred0 == -1, 1, 0)\ncm_lof0 = confusion_matrix(actual, pred0_rescored)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_lof0, annot=True, fmt='d', cmap='YlGnBu')\nplt.title(f'LOF (k={k0}, contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_lof0\n\n\n\n\n\n\n\n\narray([[87465,  9813],\n       [  966,    13]], dtype=int64)\n\n\n\n# Grid search over k for LOF\nk_values = list(range(100, 1501, 100))  # 100, 200, ..., 3000\nlof_grid = lof_grid_search(df_bin, k_values)\n\nplt.figure(figsize=(8,5))\nplt.plot(lof_grid.ks, lof_grid.accuracies, label='Accuracy')\nplt.plot(lof_grid.ks, lof_grid.precisions, label='Precision')\nplt.plot(lof_grid.ks, lof_grid.recalls, label='Recall')\nplt.xlabel('k (n_neighbors)')\nplt.ylabel('Metric (%)')\nplt.title('LOF metrics vs k')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "notebooks/final/week_03_stat_models.html#dbscan",
    "href": "notebooks/final/week_03_stat_models.html#dbscan",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "DBSCAN",
    "text": "DBSCAN\nWe run DBSCAN with eps=0.2 and min_samples=5 on the same feature space, rescoring cluster labels so that noise points are treated as anomalies.\n\ndb_res = run_dbscan(df_bin, eps=0.2, min_samples=5)\ncm_db = db_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_db, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('DBSCAN (eps=0.2, min_samples=5)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_db\n\n\n\n\n\n\n\n\narray([[11475, 85803],\n       [    0,   979]], dtype=int64)\n\n\n\n# Save anomaly detection results for Week 05\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\n# Determine project root and reports directory\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk3_dir.mkdir(parents=True, exist_ok=True)\n\nframes = []\n\n# 1) Z-SCORE MODEL \nif {\"Label\", \"Z\", \"Z_Pred\"}.issubset(df_bin.columns):\n    z_df = pd.DataFrame({\n        \"model\":      \"Z-Score (wrong_fragment, |Z|&gt;2)\",\n        \"y_true_bin\": df_bin[\"Label\"].astype(int).to_numpy(),\n        \"y_pred_bin\": df_bin[\"Z_Pred\"].astype(int).to_numpy(),\n        # Score: |Z| (higher = more anomalous)\n        \"score\":      np.abs(df_bin[\"Z\"]).to_numpy(),\n    })\n    frames.append(z_df)\n    print(\"Added Z-Score results.\")\nelse:\n    print(\"Z-score fields not found. Skipping Z-score model.\")\n\n\n# Helper function to add result objects\ndef add_res_object(name):\n    obj = globals().get(name, None)\n    if obj is None:\n        print(f\"{name} not found, skipping.\")\n        return\n    \n    needed = [\"name\", \"y_true\", \"y_pred\", \"scores\"]\n    missing = [a for a in needed if not hasattr(obj, a)]\n    if missing:\n        print(f\"{name} missing attributes {missing}, skipping.\")\n        return\n    \n    df = pd.DataFrame({\n        \"model\":      obj.name,\n        \"y_true_bin\": np.array(obj.y_true).astype(int),\n        \"y_pred_bin\": np.array(obj.y_pred).astype(int),\n        \"score\":      np.array(obj.scores).astype(float),\n    })\n    frames.append(df)\n    print(f\"Added results for {obj.name}.\")\n\n\n# Elliptic Envelope\n# Mahalanobis\n# LOF\n# DBSCAN\nadd_res_object(\"ell_res\")\nadd_res_object(\"mahal_res\")\nadd_res_object(\"lof0\")\nadd_res_object(\"db_res\")\n\n\n# Save all results\nif not frames:\n    raise ValueError(\"No Week 03 results found. Run the model cells first.\")\n\nunsup_df = pd.concat(frames, ignore_index=True)\n\ncsv_path     = wk3_dir / \"week03_unsupervised_predictions.csv\"\nparquet_path = wk3_dir / \"week03_unsupervised_predictions.parquet\"\n\nunsup_df.to_csv(csv_path, index=False)\ntry:\n    unsup_df.to_parquet(parquet_path, index=False)\nexcept Exception as e:\n    print(\"Parquet save failed (optional):\", e)\n\nprint(\"\\nSaved Week 03 anomaly detection results:\")\nprint(\" -\", csv_path)\nprint(\" -\", parquet_path)\n\nunsup_df.head()\n\nAdded Z-Score results.\nell_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nmahal_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nlof0 missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\ndb_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\n\nSaved Week 03 anomaly detection results:\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.parquet\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/final/week_04_ml_models.html",
    "href": "notebooks/final/week_04_ml_models.html",
    "title": "Week 04 — Classical ML Models",
    "section": "",
    "text": "Unsupervised / semi-supervised\n\nIsolation Forest (tune n_estimators, max_samples, contamination)\nOne-Class SVM (RBF kernel; tune nu, gamma; scaled inputs)\nDeep Autoencoder (MSE reconstruction loss, early stopping, threshold on reconstruction error)\n\nSupervised\n\nRandom Forest (class-weight balanced; tune n_estimators, max_depth)\nSVM (RBF kernel, class-weight balanced; tune C, gamma)\n\nImbalance handling\n\nCompare class_weight vs SMOTE (train only) for rare classes (R2L / U2R analogues)\nKeep a clean baseline that uses only class_weight.\nHelper codes in src/models.py and src/eval.py.\nimport warnings\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*covariance matrix associated to your dataset is not full rank.*\"\n)\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\nfrom src.models import make_fast_oneclass_svm\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\nimport os, sys\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\nfrom src.models import (\n    make_isolation_forest_grid,\n    make_oneclass_svm_grid,\n    make_rf_classifier_grid,\n    make_svm_rbf_grid,\n    build_deep_autoencoder,\n)\nfrom src.eval import (\n    plot_confusion,\n    classification_summary,\n    binary_roc_pr_curves,\n)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import f1_score\n\nset_global_seed(42)\npaths = Paths().ensure()\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\n# Load processed train/test arrays from Week 01.\nfrom pathlib import Path\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\ny_test  classes: Counter({0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10})\nTo evaluate unsupervised detectors, we convert the multi-class labels into a binary label:\nThis matches the standard NSL-KDD setup where attacks are rare compared to normal traffic, but it does not assume any particular encoding of labels.\n# Build binary labels for anomaly detection (0 = normal, 1 = attack).\nfrom collections import Counter\nimport numpy as np\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({0: 313166, 1: 82050})\nBinary test  counts: Counter({0: 78292, 1: 20513})"
  },
  {
    "objectID": "notebooks/final/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "href": "notebooks/final/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "title": "Week 04 — Classical ML Models",
    "section": "1. Unsupervised / Semi-supervised models",
    "text": "1. Unsupervised / Semi-supervised models\n\n1.1 Isolation Forest\n\nfrom sklearn.metrics import f1_score\n\ndef iso_f1_scorer(estimator, X, y_true):\n    \"\"\"\n    Convert IsolationForest predictions {1, -1} into {0, 1} and compute F1.\n    \"\"\"\n    y_raw = estimator.predict(X)\n    y_pred = (y_raw == -1).astype(int)  # -1 = anomaly → 1\n    return f1_score(y_true, y_pred, zero_division=0)\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom src.models import make_isolation_forest_grid\n\niso_spec = make_isolation_forest_grid()\n\niso_gs = GridSearchCV(\n    estimator=iso_spec.model,\n    param_grid=iso_spec.param_grid,\n    scoring=iso_f1_scorer,\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\n\niso_gs.fit(X_train, y_train_bin)\nprint(\"Best params (Isolation Forest):\", iso_gs.best_params_)\nprint(\"Best F1 (train CV):\", iso_gs.best_score_)\n\nFitting 3 folds for each of 18 candidates, totalling 54 fits\nBest params (Isolation Forest): {'contamination': 0.1, 'max_samples': 0.5, 'n_estimators': 200}\nBest F1 (train CV): 0.5217237922691421\n\n\n\n# Evaluate Isolation Forest on the held-out test set.\nbest_iso = iso_gs.best_estimator_\npred_iso = best_iso.predict(X_test)  # 1 = inlier, -1 = outlier\ny_pred_iso = np.where(pred_iso == -1, 1, 0)\n\nprint(\"Isolation Forest F1 (test, binary):\", f1_score(y_test_bin, y_pred_iso))\nplot_confusion(y_test_bin, y_pred_iso, labels=[0, 1], title=\"Isolation Forest (binary)\")\n\nIsolation Forest F1 (test, binary): 0.4988470913762435\n\n\n\n\n\n\n\n\n\n\n\n1.2 One-Class SVM (RBF)\n\n# OC-SVM (RBF)\nocsvm = make_fast_oneclass_svm()\n\n# Train on normal-only data\nmask_normal = (y_train_bin == 0)\nX_train_normal = X_train[mask_normal]\n\nocsvm.fit(X_train_normal)\n\npred = ocsvm.predict(X_test)\ny_pred_ocsvm = (pred == -1).astype(int)\n\nplot_confusion(y_test_bin, y_pred_ocsvm, labels=[0, 1], title=\"OC-SVM\")\n\n\n\n\n\n\n\n\n\n\n1.3 Deep Autoencoder\n\n### 1.3 Deep Autoencoder — Chapter_2 architecture on Week04 features\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\n# Use the same X_train / X_test that you loaded at the top of Week04\ninput_dim = X_train.shape[1]\nprint(\"Autoencoder input_dim:\", input_dim)\n\n# Architecture matching the screenshot:  input_dim → 30 → 16 → 8 → 16 → 30 → input_dim\ninput_layer = keras.Input(shape=(input_dim,))\n\nencoded = layers.Dense(30, activation=\"relu\")(input_layer)\nencoded = layers.Dense(16, activation=\"relu\")(encoded)\nencoded = layers.Dense(8,  activation=\"relu\")(encoded)\n\ndecoded = layers.Dense(16, activation=\"relu\")(encoded)\ndecoded = layers.Dense(30, activation=\"relu\")(decoded)\ndecoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n\nautoencoder = keras.Model(input_layer, decoded, name=\"deep_autoencoder_ch2\")\n\nautoencoder.compile(\n    optimizer=\"adam\",\n    loss=\"mean_squared_error\"\n)\n\nautoencoder.summary()\n\nAutoencoder input_dim: 115\n\n\nModel: \"deep_autoencoder_ch2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 115)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 30)             │         3,480 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 16)             │           496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 8)              │           136 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 16)             │           144 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 30)             │           510 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 115)            │         3,565 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 8,331 (32.54 KB)\n\n\n\n Trainable params: 8,331 (32.54 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ensure numeric float dtype\nX_train_ae = X_train.astype(\"float32\")\nX_test_ae  = X_test.astype(\"float32\")\n\nhistory = autoencoder.fit(\n    X_train_ae,\n    X_train_ae,        # reconstruction target\n    epochs=10,         # like in Chapter_2 screenshot\n    batch_size=256,\n    shuffle=True,\n    validation_split=0.1,\n    verbose=1,\n)\n\n\nEpoch 1/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - loss: 0.1647 - val_loss: 0.0458\n\nEpoch 2/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.1108 - val_loss: 0.0363\n\nEpoch 3/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0769 - val_loss: 0.0338\n\nEpoch 4/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0594 - val_loss: 0.0386\n\nEpoch 5/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0547 - val_loss: 0.0264\n\nEpoch 6/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0655 - val_loss: 0.0216\n\nEpoch 7/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0583 - val_loss: 0.0175\n\nEpoch 8/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0544 - val_loss: 0.0146\n\nEpoch 9/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0278 - val_loss: 0.0145\n\nEpoch 10/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0386 - val_loss: 0.0149\n\n\n\n\n\n# Reconstruction error on training + test\nrecon_train = autoencoder.predict(X_train, verbose=0)\nrecon_test = autoencoder.predict(X_test, verbose=0)\n\ntrain_err = np.mean((X_train - recon_train) ** 2, axis=1)\ntest_err = np.mean((X_test - recon_test) ** 2, axis=1)\n\n# Choose threshold as a high percentile of *training* error\nthreshold = np.percentile(train_err, 99)\nprint(\"Autoencoder threshold (99th percentile of train error):\", threshold)\n\ny_pred_ae = (test_err &gt;= threshold).astype(int)\n\nprint(\"Autoencoder F1 (test, binary):\", f1_score(y_test_bin, y_pred_ae))\nplot_confusion(y_test_bin, y_pred_ae, labels=[0, 1], title=\"Autoencoder\")\n\nAutoencoder threshold (99th percentile of train error): 0.1440299195070224\nAutoencoder F1 (test, binary): 0.08864985163204747\n\n\n\n\n\n\n\n\n\n\n# ROC / PR curves for the autoencoder scores\nbinary_roc_pr_curves(y_test_bin, test_err, pos_label=1, title_prefix=\"Autoencoder \")\n\n(&lt;Figure size 1000x400 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Autoencoder ROC (AUC=0.982)'}, xlabel='False positive rate', ylabel='True positive rate'&gt;,\n        &lt;Axes: title={'center': 'Autoencoder Precision-Recall'}, xlabel='Recall', ylabel='Precision'&gt;],\n       dtype=object))"
  },
  {
    "objectID": "notebooks/final/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "href": "notebooks/final/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "title": "Week 04 — Classical ML Models",
    "section": "2. Supervised models: Random Forest and SVM (RBF)",
    "text": "2. Supervised models: Random Forest and SVM (RBF)\n\n2.1 Random Forest\nUse the full multi-class labels (y_train, y_test) and train:\n\nRandom Forest (class_weight=“balanced”; tune n_estimators, max_depth)\nSVM with RBF kernel (class_weight=“balanced”; tune C, gamma)\n\nThis gives a supervised baseline to compare against the unsupervised / semi-supervised detectors.\n\nrf_spec = make_rf_classifier_grid()\nrf_gs = GridSearchCV(\n    estimator=rf_spec.model,\n    param_grid=rf_spec.param_grid,\n    scoring=\"f1_weighted\",\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\nrf_gs.fit(X_train, y_train)\n\nprint(\"Random Forest best params:\", rf_gs.best_params_)\nprint(\"Random Forest best CV f1_weighted:\", rf_gs.best_score_)\n\nFitting 3 folds for each of 9 candidates, totalling 27 fits\nRandom Forest best params: {'max_depth': 20, 'n_estimators': 400}\nRandom Forest best CV f1_weighted: 0.9997615589652838\n\n\n\nbest_rf = rf_gs.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\n\nrf_report = classification_summary(y_test, y_pred_rf, print_report=True)\nplot_confusion(y_test, y_pred_rf, title=\"Random Forest\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.96      0.97      0.97     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 SVM-RBF\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# SVM-RBF\nsvm_fast = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", SVC(\n        kernel=\"rbf\",\n        class_weight=\"balanced\",\n        C=10,          # reasonable value\n        gamma=\"scale\"  # reasonable value\n    ))\n])\n\nsvm_fast.fit(X_train, y_train)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.97      0.98      0.97       822\n           2       0.72      0.98      0.83       225\n           3       0.73      0.80      0.76        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.88      0.95      0.91     98805\nweighted avg       1.00      1.00      1.00     98805"
  },
  {
    "objectID": "notebooks/final/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "href": "notebooks/final/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "title": "Week 04 — Classical ML Models",
    "section": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)",
    "text": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)\nNSL-KDD includes very rare R2L and U2R attack families. Instead of assuming we already know which integer labels correspond to these families, we follow a data-driven approach:\n\nIdentify the two rarest classes in y_train.\nTreat them as “R2L/U2R-like” minority classes.\nCompare:\n\nBaseline models using only class_weight=\"balanced\"\nModels trained on SMOTE-resampled data (train only), focusing on those rare classes.\n\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Identify the two rarest classes\nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes (treated as R2L/U2R-like):\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\nMinority classes (treated as R2L/U2R-like): [3, 2]\nClass counts: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\n\n\n\ndef evaluate_subset(classes_subset, y_true, y_pred):\n    import numpy as np\n    mask = np.isin(y_true, classes_subset)\n    y_true_sub = y_true[mask]\n    y_pred_sub = y_pred[mask]\n    print(\"Subset size:\", y_true_sub.shape[0])\n    return classification_summary(y_true_sub, y_pred_sub, print_report=True)\n\n\n3.1 Baseline: class_weight only\n\nprint(\"Random Forest (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf)\n\nprint(\"\\nSVM-RBF (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_svm)\n\nRandom Forest (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.90      0.90      0.90        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.63      0.63      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\nSVM-RBF (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       1.00      0.80      0.89        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.67      0.59      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n\n\n3.2 SMOTE + Random Forest / SVM-RBF (train only)\n\n# SMOTE strategy: upsample only the identified minority classes\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE counts:\", Counter(y_train_sm))\n\nSMOTE sampling strategy: {3: 313166, 2: 313166}\nAfter SMOTE counts: Counter({0: 313166, 2: 313166, 3: 313166, 4: 77822, 1: 3285})\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Reuse best params from the *baseline* RF grid search\nprint(\"Baseline RF best params:\", rf_gs.best_params_)\n\nrf_sm = RandomForestClassifier(\n    n_estimators=rf_gs.best_params_[\"n_estimators\"],\n    max_depth=rf_gs.best_params_[\"max_depth\"],\n    n_jobs=-1,\n    class_weight=\"balanced\",    # you can also set this to None, since SMOTE already balances\n    random_state=42,\n)\n\n# Fit on SMOTE-resampled data (single fit, much faster than GridSearchCV)\nrf_sm.fit(X_train_sm, y_train_sm)\n\ny_pred_rf_sm = rf_sm.predict(X_test)\n\nprint(\"\\nRandom Forest + SMOTE — full test report\")\n_ = classification_summary(y_test, y_pred_rf_sm, print_report=True)\n\nprint(\"\\nRandom Forest + SMOTE — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf_sm)\n\nBaseline RF best params: {'max_depth': 20, 'n_estimators': 400}\n\nRandom Forest + SMOTE — full test report\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       0.99      0.98      0.99       225\n           3       0.75      0.90      0.82        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.95      0.98      0.96     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nRandom Forest + SMOTE — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.98       235\n   macro avg       0.60      0.63      0.62       235\nweighted avg       0.99      0.98      0.98       235\n\n\n\n\n\n4. SVM-RBF (Baseline, Fast Subset)\n\nimport numpy as np\nfrom collections import Counter\n\ndef build_svm_subset(X, y, minority_classes, max_samples=5000, random_state=42):\n    # Build a smaller train set for SVM:\n    # - Keep ALL samples from minority_classes\n    # - Subsample majority classes so total ~ max_samples\n\n    rng = np.random.RandomState(random_state)\n    y = np.asarray(y)\n\n    # Indices for minority and majority\n    mask_min = np.isin(y, minority_classes)\n    idx_min = np.where(mask_min)[0]\n    idx_maj = np.where(~mask_min)[0]\n\n    # Always keep all minority samples\n    keep_idx = list(idx_min)\n\n    # Remaining budget for majority\n    remaining = max(0, max_samples - len(idx_min))\n    if remaining &gt; 0 and len(idx_maj) &gt; remaining:\n        subsampled_maj = rng.choice(idx_maj, size=remaining, replace=False)\n    else:\n        subsampled_maj = idx_maj\n\n    keep_idx.extend(subsampled_maj)\n    keep_idx = np.array(keep_idx)\n\n    X_small = X[keep_idx]\n    y_small = y[keep_idx]\n\n    print(\"SVM subset size:\", X_small.shape, \"class counts:\", Counter(y_small))\n\n    return X_small, y_small\n\n\n\n5. SVM-RBF with SMOTE Oversampling\n\nfrom src.models import make_fast_svm_rbf\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# 1) Identify minority classes \nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes:\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\n# 2) Build smaller subset for SVM baseline (no SMOTE)\nX_train_svm, y_train_svm = build_svm_subset(\n    X_train, y_train,\n    minority_classes=minority_classes,\n    max_samples=2000,        # you can drop to 3000 or 2000 if still slow\n    random_state=42\n)\n\n# 3) SMOTE on FULL train, then subset for SVM\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm_full, y_train_sm_full = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE (full):\", Counter(y_train_sm_full))\n\n# Subset the SMOTE-resampled set for SVM\nX_train_sm_svm, y_train_sm_svm = build_svm_subset(\n    X_train_sm_full, y_train_sm_full,\n    minority_classes=minority_classes,\n    max_samples=5000,\n    random_state=42\n)\n\n# 4) SVM baseline (no SMOTE)\nsvm_fast = make_fast_svm_rbf()\nsvm_fast.fit(X_train_svm, y_train_svm)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nprint(\"\\nSVM-RBF baseline (fast, subset)\")\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF (fast baseline, subset)\")\n\n# 5) SVM on SMOTE-resampled subset\nsvm_sm = make_fast_svm_rbf()\nsvm_sm.fit(X_train_sm_svm, y_train_sm_svm)\n\ny_pred_svm_sm = svm_sm.predict(X_test)\n\nprint(\"\\nSVM-RBF + SMOTE (fast, subset)\")\nclassification_summary(y_test, y_pred_svm_sm, print_report=True)\n\nprint(\"\\nMinority class performance (SVM + SMOTE)\")\nevaluate_subset(minority_classes, y_test, y_pred_svm_sm)\n\nMinority classes: [3, 2]\nClass counts: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\nSVM subset size: (2000, 115) class counts: Counter({2: 901, 0: 853, 4: 201, 3: 42, 1: 3})\nSMOTE sampling strategy: {3: 313166, 2: 313166}\nAfter SMOTE (full): Counter({0: 313166, 2: 313166, 3: 313166, 4: 77822, 1: 3285})\nSVM subset size: (1020605, 115) class counts: Counter({2: 313166, 3: 313166, 0: 313166, 4: 77822, 1: 3285})\n\nSVM-RBF baseline (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00     78292\n           1       1.00      0.07      0.13       822\n           2       0.13      1.00      0.23       225\n           3       0.11      0.60      0.19        10\n           4       0.97      0.96      0.97     19456\n\n    accuracy                           0.98     98805\n   macro avg       0.64      0.72      0.50     98805\nweighted avg       0.99      0.98      0.98     98805\n\n\nSVM-RBF + SMOTE (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.98      0.98      0.98       822\n           2       0.70      0.98      0.82       225\n           3       0.78      0.70      0.74        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.89      0.93      0.91     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nMinority class performance (SVM + SMOTE)\nSubset size: 235\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           2       1.00      0.98      0.99       225\n           3       1.00      0.70      0.82        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.50      0.42      0.45       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n '2': {'precision': 0.995475113122172,\n  'recall': 0.9777777777777777,\n  'f1-score': 0.9865470852017937,\n  'support': 225.0},\n '3': {'precision': 1.0,\n  'recall': 0.7,\n  'f1-score': 0.8235294117647058,\n  'support': 10.0},\n '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n 'accuracy': 0.9659574468085106,\n 'macro avg': {'precision': 0.498868778280543,\n  'recall': 0.4194444444444444,\n  'f1-score': 0.4525191242416249,\n  'support': 235.0},\n 'weighted avg': {'precision': 0.9956676614999519,\n  'recall': 0.9659574468085106,\n  'f1-score': 0.9796101629278751,\n  'support': 235.0}}\n\n\n\n\n\n\n\n\n\n\n# Save existing results for later evaluation\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\n# Use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk4_dir = reports_root / \"week_04\"\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Saving Week 04 results under:\", wk4_dir)\n\n# UNSUPERVISED / SEMI-SUPERVISED \n#\n# Use variables that ALREADY exist from earlier cells:\n#   y_test_bin     : binary true labels (0 = normal, 1 = attack)\n#   y_pred_iso     : Isolation Forest binary predictions\n#   y_pred_ocsvm   : OC-SVM binary predictions\n#   y_pred_ae      : Autoencoder binary predictions\n#   test_err       : Autoencoder reconstruction error (per test sample)\n#\n# Scores are taken from existing values:\n#   - Isolation Forest / OC-SVM: score = y_pred_* (0/1 as a crude anomaly score)\n#   - Autoencoder: score = test_err reconstruction error)\n\nunsup_frames = []\n\ndef add_binary_model_existing(name, y_true, y_pred, score):\n    df = pd.DataFrame({\n        \"model\":      name,\n        \"y_true_bin\": np.asarray(y_true).astype(int),\n        \"y_pred_bin\": np.asarray(y_pred).astype(int),\n        \"score\":      np.asarray(score).astype(float),\n    })\n    unsup_frames.append(df)\n    print(f\"Added unsupervised/semi-supervised model: {name}\")\n\n# Isolation Forest\nif \"y_test_bin\" in globals() and \"y_pred_iso\" in globals():\n    add_binary_model_existing(\n        \"Isolation Forest\",\n        y_test_bin,\n        y_pred_iso,\n        y_pred_iso,   # just reuse 0/1 prediction as score (no recomputation)\n    )\nelse:\n    print(\"Isolation Forest predictions not found; skipping.\")\n\n# OC-SVM\nif \"y_test_bin\" in globals() and \"y_pred_ocsvm\" in globals():\n    add_binary_model_existing(\n        \"OC-SVM (RBF)\",\n        y_test_bin,\n        y_pred_ocsvm,\n        y_pred_ocsvm,  # reuse 0/1 prediction as score\n    )\nelse:\n    print(\"OC-SVM predictions not found; skipping.\")\n\n# Autoencoder\nif \"y_test_bin\" in globals() and \"y_pred_ae\" in globals() and \"test_err\" in globals():\n    add_binary_model_existing(\n        \"Deep Autoencoder\",\n        y_test_bin,\n        y_pred_ae,\n        test_err,      # reconstruction error already computed\n    )\nelse:\n    print(\"Autoencoder predictions/errors not found; skipping.\")\n\nif unsup_frames:\n    unsup_df = pd.concat(unsup_frames, ignore_index=True)\n    unsup_csv_path = wk4_dir / \"week04_unsupervised_predictions.csv\"\n    unsup_parquet_path = wk4_dir / \"week04_unsupervised_predictions.parquet\"\n\n    unsup_df.to_csv(unsup_csv_path, index=False)\n    try:\n        unsup_df.to_parquet(unsup_parquet_path, index=False)\n    except Exception as e:\n        print(\"Parquet save for unsupervised models failed (optional):\", e)\n\n    print(\"\\nSaved unsupervised/semi-supervised results to:\")\n    print(\" -\", unsup_csv_path)\n    print(\" -\", unsup_parquet_path)\nelse:\n    print(\"No unsupervised/semi-supervised models were added; nothing saved for that group.\")\n\nsup_frames = []\n\ndef add_supervised_existing(model_name, y_true, y_pred):\n    df = pd.DataFrame({\n        \"model\":  model_name,\n        \"y_true\": np.asarray(y_true),\n        \"y_pred\": np.asarray(y_pred),\n    })\n    sup_frames.append(df)\n    print(f\"Added supervised model: {model_name}\")\n\n# Random Forest baseline\nif \"y_test\" in globals() and \"y_pred_rf\" in globals():\n    add_supervised_existing(\"Random Forest (baseline)\", y_test, y_pred_rf)\nelse:\n    print(\"Random Forest baseline predictions not found; skipping.\")\n\n# SVM-RBF baseline\nif \"y_test\" in globals() and \"y_pred_svm\" in globals():\n    add_supervised_existing(\"SVM-RBF (baseline)\", y_test, y_pred_svm)\nelse:\n    print(\"SVM-RBF baseline predictions not found; skipping.\")\n\n# Random Forest + SMOTE\nif \"y_test\" in globals() and \"y_pred_rf_sm\" in globals():\n    add_supervised_existing(\"Random Forest + SMOTE\", y_test, y_pred_rf_sm)\nelse:\n    print(\"Random Forest + SMOTE predictions not found; skipping.\")\n\n# SVM-RBF + SMOTE\nif \"y_test\" in globals() and \"y_pred_svm_sm\" in globals():\n    add_supervised_existing(\"SVM-RBF + SMOTE\", y_test, y_pred_svm_sm)\nelse:\n    print(\"SVM-RBF + SMOTE predictions not found; skipping.\")\n\nif sup_frames:\n    sup_pred_df = pd.concat(sup_frames, ignore_index=True)\n    sup_csv_path = wk4_dir / \"week04_supervised_predictions.csv\"\n    sup_parquet_path = wk4_dir / \"week04_supervised_predictions.parquet\"\n\n    sup_pred_df.to_csv(sup_csv_path, index=False)\n    try:\n        sup_pred_df.to_parquet(sup_parquet_path, index=False)\n    except Exception as e:\n        print(\"Parquet save for supervised models failed (optional):\", e)\n\n    print(\"\\nSaved supervised results to:\")\n    print(\" -\", sup_csv_path)\n    print(\" -\", sup_parquet_path)\nelse:\n    print(\"No supervised models were added; nothing saved for that group.\")\n\nSaving Week 04 results under: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nAdded unsupervised/semi-supervised model: Isolation Forest\nAdded unsupervised/semi-supervised model: OC-SVM (RBF)\nAdded unsupervised/semi-supervised model: Deep Autoencoder\n\nSaved unsupervised/semi-supervised results to:\n - C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_unsupervised_predictions.csv\n - C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_unsupervised_predictions.parquet\nAdded supervised model: Random Forest (baseline)\nAdded supervised model: SVM-RBF (baseline)\nAdded supervised model: Random Forest + SMOTE\nAdded supervised model: SVM-RBF + SMOTE\n\nSaved supervised results to:\n - C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_supervised_predictions.csv\n - C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_supervised_predictions.parquet\n\n\n\n# Save Week 04 models for Week 06 explainability\nimport joblib\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\nmodels_dir = getattr(paths, \"models\", project_root / \"models\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nrf_path  = models_dir / \"week04_rf.joblib\"\nsvm_path = models_dir / \"week04_svm.joblib\"\nae_path  = models_dir / \"week04_ae.joblib\"\n\nprint(\"Saving models to:\", models_dir)\n\n# These variables already exist in your notebook:\n#   best_rf   -&gt; best RandomForest from GridSearch\n#   svm_fast  -&gt; SVM-RBF Pipeline (baseline)\n#   autoencoder -&gt; trained deep autoencoder\n\njoblib.dump(best_rf, rf_path)\nprint(\"Saved RF to:\", rf_path)\n\njoblib.dump(svm_fast, svm_path)\nprint(\"Saved SVM to:\", svm_path)\n\njoblib.dump(autoencoder, ae_path)\nprint(\"Saved Autoencoder to:\", ae_path)\n\nSaving models to: C:\\Users\\mehra\\Final_Project\\models\nSaved RF to: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSaved SVM to: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nSaved Autoencoder to: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib"
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html",
    "href": "notebooks/final/week_06_report_explain.html",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "",
    "text": "Global & local explainability (Random Forest, SVM, Autoencoder)\nUnsupervised model structure (DBSCAN, LOF, IF, Autoencoder)\nDecision boundaries in a 2D PCA plane\nSynthesis: transparency vs accuracy\nReal-world IDS implications: false positives, thresholds, maintenance\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\n\n# Use a clean seaborn style\nsns.set(style=\"whitegrid\")\n\n# Project paths\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\n\nset_global_seed(42)\npaths = Paths().ensure()\n\n# Derive a robust reports root (Week 03–06)\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\nwk6_dir = reports_root / \"week_06\"\n\nwk6_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Project root:\", project_root)\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports:\", wk3_dir)\nprint(\"Week 04 reports:\", wk4_dir)\nprint(\"Week 06 reports:\", wk6_dir)\n\nProject root: C:\\Users\\mehra\\Final_Project\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nWeek 06 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_06\n# Load processed arrays from Week 01\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n\n# Optional: binary labels (0=normal, 1=attack) from Week 04 logic\nfrom collections import Counter\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin  = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nX_train: (395216, 115) X_test: (98805, 115)\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({0: 313166, 1: 82050})\nBinary test  counts: Counter({0: 78292, 1: 20513})\n# Load Week 03 & Week 04 prediction files\n\ndef _load_first_existing(paths):\n    for p in paths:\n        if p.exists():\n            if p.suffix == \".parquet\":\n                return pd.read_parquet(p)\n            else:\n                return pd.read_csv(p)\n    return None\n\nwk3_unsup = _load_first_existing([\n    wk3_dir / \"week03_unsupervised_predictions.parquet\",\n    wk3_dir / \"week03_unsupervised_predictions.csv\",\n])\n\nwk4_unsup = _load_first_existing([\n    wk4_dir / \"week04_unsupervised_predictions.parquet\",\n    wk4_dir / \"week04_unsupervised_predictions.csv\",\n])\n\nwk4_sup = _load_first_existing([\n    wk4_dir / \"week04_supervised_predictions.parquet\",\n    wk4_dir / \"week04_supervised_predictions.csv\",\n])\n\nprint(\"Week 03 unsupervised df:\", None if wk3_unsup is None else wk3_unsup.shape)\nprint(\"Week 04 unsupervised df:\", None if wk4_unsup is None else wk4_unsup.shape)\nprint(\"Week 04 supervised df:\", None if wk4_sup is None else wk4_sup.shape)\n\nif wk3_unsup is not None:\n    display(wk3_unsup.head())\n\nif wk4_unsup is not None:\n    display(wk4_unsup.head())\n\nif wk4_sup is not None:\n    display(wk4_sup.head())\n\nWeek 03 unsupervised df: (98257, 4)\nWeek 04 unsupervised df: (296415, 4)\nWeek 04 supervised df: (395220, 3)\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#explainability-helpers-feature-names-models",
    "href": "notebooks/final/week_06_report_explain.html#explainability-helpers-feature-names-models",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Explainability helpers: feature names & models",
    "text": "Explainability helpers: feature names & models\n\n# Feature names via preprocessor (if available)\n\nfeature_names = None\ntry:\n    import joblib\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = list(pre.get_feature_names_out())\n    print(\"Loaded feature names from preprocessor:\", len(feature_names))\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    # Fall back to simple numeric names\n    feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n    print(\"Using generic feature names:\", len(feature_names))\n\nLoaded feature names from preprocessor: 115\n\n\n\nimport joblib\n\nrf_model = None\nsvm_model = None\nae_model = None\n\nrf_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_rf.joblib\"\nsvm_path = getattr(paths, \"models\", project_root / \"models\") / \"week04_svm.joblib\"\nae_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_ae.joblib\"\n\nfor name, p in [(\"RF\", rf_path), (\"SVM\", svm_path), (\"AE\", ae_path)]:\n    print(f\"{name} expected at:\", p)\n\ntry:\n    if rf_path.exists():\n        rf_model = joblib.load(rf_path)\n        print(\"Loaded RF model.\")\n    else:\n        print(\"RF model file not found.\")\nexcept Exception as e:\n    print(\"Could not load RF model:\", e)\n\ntry:\n    if svm_path.exists():\n        svm_model = joblib.load(svm_path)\n        print(\"Loaded SVM model.\")\n    else:\n        print(\"SVM model file not found.\")\nexcept Exception as e:\n    print(\"Could not load SVM model:\", e)\n\ntry:\n    if ae_path.exists():\n        ae_model = joblib.load(ae_path)\n        print(\"Loaded Autoencoder model.\")\n    else:\n        print(\"Autoencoder model file not found.\")\nexcept Exception as e:\n    print(\"Could not load Autoencoder model:\", e)\n\nRF expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSVM expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nAE expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib\nRF model file not found.\nSVM model file not found.\nAutoencoder model file not found."
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#import-explainability-utilities-from-src.explain",
    "href": "notebooks/final/week_06_report_explain.html#import-explainability-utilities-from-src.explain",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Import explainability utilities from src.explain",
    "text": "Import explainability utilities from src.explain\nThe src.explain module contains reusable helpers for:\n\nRandom Forest feature importance (Gini & permutation)\nSHAP for SVM and Autoencoder\nUnsupervised visualizations (PCA + anomaly scores)\nPCA decision boundary plots\n\n\nfrom src.explain import (\n    compute_rf_feature_importance,\n    compute_permutation_importance,\n    compute_kernel_shap,\n    compute_deep_shap,\n    plot_anomaly_score_distributions,\n    plot_reconstruction_error_hist,\n    compute_pca_embedding,\n    plot_pca_dbscan_like,\n    plot_pca_decision_boundaries,\n)"
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#random-forest-explainability",
    "href": "notebooks/final/week_06_report_explain.html#random-forest-explainability",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Random Forest Explainability",
    "text": "Random Forest Explainability\nRandom Forest classifier:\n\nGini-based feature importance\nPermutation importance on a test subset\n\nit shows which features matter most to the RF decision function.\n\n# Random Forest feature importance & permutation importance\n\nif rf_model is None:\n    print(\"RF model not loaded — skipping RF explainability. \"\n          \"Save your Random Forest as 'week04_rf.joblib' in paths.models to enable this section.\")\nelse:\n    # Gini importance\n    rf_imp_df = compute_rf_feature_importance(rf_model, feature_names)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.barplot(data=rf_imp_df.head(20), x=\"importance\", y=\"feature\", ax=ax)\n    ax.set_title(\"Random Forest — Top 20 Gini Feature Importances\")\n    plt.tight_layout()\n    fig_path = wk6_dir / \"rf_gini_importance_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    \n    # Permutation importance on a test set\n    rf_perm_df = compute_permutation_importance(\n        rf_model,\n        X_test,\n        y_test,\n        feature_names,\n        n_repeats=10,\n        max_samples=3000,\n        random_state=42,\n    )\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.barplot(data=rf_perm_df.head(20), x=\"importance\", y=\"feature\", ax=ax)\n    ax.set_title(\"Random Forest — Top 20 Permutation Importances\")\n    plt.tight_layout()\n    fig_path = wk6_dir / \"rf_permutation_importance_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    \n    rf_imp_df.head()\n\nRF model not loaded — skipping RF explainability. Save your Random Forest as 'week04_rf.joblib' in paths.models to enable this section."
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#shap-explainability-for-svm-and-autoencoder",
    "href": "notebooks/final/week_06_report_explain.html#shap-explainability-for-svm-and-autoencoder",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "SHAP Explainability for SVM and Autoencoder",
    "text": "SHAP Explainability for SVM and Autoencoder\nFor models that are less directly interpretable (SVM-RBF, deep Autoencoder), we use SHAP:\n\nKernel SHAP for SVM (black-box)\nDeep SHAP for the Autoencoder (if supported)\n\nWe compute both summary and per-sample explanations and to keep runtime reasonable, we work on small subsamples of X_test.\n\n# SHAP for SVM-RBF\n\nif svm_model is None:\n    print(\"SVM model not loaded — skipping SVM SHAP. \"\n          \"Save your SVM as 'week04_svm.joblib' in paths.models to enable this section.\")\nelse:\n    try:\n        svm_shap_res = compute_kernel_shap(\n            model=svm_model,\n            X_background=X_train,\n            X_explain=X_test,\n            feature_names=feature_names,\n            max_background=200,\n            max_explain=200,\n        )\n        \n        fig = svm_shap_res[\"summary_fig\"]\n        fig_path = wk6_dir / \"svm_kernel_shap_summary.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n        \n        # Local example\n        local_idx = 0\n        local_fig = svm_shap_res[\"local_plots\"][local_idx]\n        local_path = wk6_dir / f\"svm_kernel_shap_local_{local_idx}.png\"\n        local_fig.savefig(local_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", local_path)\n        \n    except Exception as e:\n        print(\"SVM SHAP computation failed:\", e)\n\nSVM model not loaded — skipping SVM SHAP. Save your SVM as 'week04_svm.joblib' in paths.models to enable this section.\n\n\n\n# SHAP for Autoencoder \n\nif ae_model is None:\n    print(\"Autoencoder model not loaded — skipping AE SHAP. \"\n          \"Save your autoencoder as 'week04_ae.joblib' in paths.models to enable this section.\")\nelse:\n    try:\n        ae_shap_res = compute_deep_shap(\n            model=ae_model,\n            X_background=X_train,\n            X_explain=X_test,\n            feature_names=feature_names,\n            max_background=200,\n            max_explain=200,\n        )\n        \n        fig = ae_shap_res[\"summary_fig\"]\n        fig_path = wk6_dir / \"ae_deep_shap_summary.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n        \n        local_idx = 0\n        local_fig = ae_shap_res[\"local_plots\"][local_idx]\n        local_path = wk6_dir / f\"ae_deep_shap_local_{local_idx}.png\"\n        local_fig.savefig(local_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", local_path)\n        \n    except Exception as e:\n        print(\"Autoencoder SHAP computation failed:\", e)\n\nAutoencoder model not loaded — skipping AE SHAP. Save your autoencoder as 'week04_ae.joblib' in paths.models to enable this section."
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#unsupervised-models-structure-score-distributions",
    "href": "notebooks/final/week_06_report_explain.html#unsupervised-models-structure-score-distributions",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Unsupervised Models — Structure & Score Distributions",
    "text": "Unsupervised Models — Structure & Score Distributions\nUsing the saved results from Week 03 and Week 04, we visualize:\n\nDBSCAN-style separation in PCA space (binary normal vs anomaly by DBSCAN)\nAnomaly score distributions for LOF, Isolation Forest, etc.\nReconstruction error histogram for the Autoencoder\n\n\n# PCA embedding for visualization\n\nX_test_pca = compute_pca_embedding(X_test, n_components=2, random_state=42)\nX_test_pca.shape\n\n(98805, 2)\n\n\n\n# DBSCAN-like separation in PCA plane (using Week 03 DBSCAN predictions)\n\nif wk3_unsup is None:\n    print(\"Week 03 unsupervised df not loaded — skipping DBSCAN visualization.\")\nelse:\n    # Look for DBSCAN rows\n    mask_db = wk3_unsup[\"model\"].str.contains(\"DBSCAN\", case=False, na=False)\n    if not mask_db.any():\n        print(\"No DBSCAN rows in Week 03 df — skipping DBSCAN visualization.\")\n    else:\n        db_rows = wk3_unsup[mask_db].reset_index(drop=True)\n        # Assume ordering is aligned with X_test; if not, this is approximate\n        if len(db_rows) != X_test_pca.shape[0]:\n            print(\"DBSCAN rows length does not match X_test; using min length to align.\")\n        n = min(len(db_rows), X_test_pca.shape[0])\n        plot_pca_dbscan_like(\n            X_pca=X_test_pca[:n],\n            labels_pred=db_rows[\"y_pred_bin\"].values[:n],\n            labels_true=db_rows[\"y_true_bin\"].values[:n],\n            title=\"PCA plane – DBSCAN-like anomaly separation (Week 03)\",\n            save_path=wk6_dir / \"pca_dbscan_like.png\",\n        )\n\nNo DBSCAN rows in Week 03 df — skipping DBSCAN visualization.\n\n\n\n# Anomaly score distributions for unsupervised models (Week 03 + Week 04)\n\nif wk3_unsup is None and wk4_unsup is None:\n    print(\"No unsupervised score tables loaded — skipping anomaly score distributions.\")\nelse:\n    fig = plot_anomaly_score_distributions(\n        wk3_unsup=wk3_unsup,\n        wk4_unsup=wk4_unsup,\n        max_models=6,\n    )\n    fig_path = wk6_dir / \"unsupervised_anomaly_score_distributions.png\"\n    fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n    print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\unsupervised_anomaly_score_distributions.png\n\n\n\n\n\n\n\n\n\n\n# Autoencoder reconstruction error histogram (Week 04 unsupervised)\n\nif wk4_unsup is None:\n    print(\"Week 04 unsupervised df not loaded — skipping AE reconstruction error histogram.\")\nelse:\n    mask_ae = wk4_unsup[\"model\"].str.contains(\"Autoencoder\", case=False, na=False)\n    if not mask_ae.any():\n        print(\"No Autoencoder rows in Week 04 unsupervised df.\")\n    else:\n        ae_scores = wk4_unsup.loc[mask_ae, \"score\"].values\n        fig = plot_reconstruction_error_hist(ae_scores)\n        fig_path = wk6_dir / \"ae_reconstruction_error_hist.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\ae_reconstruction_error_hist.png"
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#pca-decision-boundaries-for-rf-and-svm",
    "href": "notebooks/final/week_06_report_explain.html#pca-decision-boundaries-for-rf-and-svm",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "PCA Decision Boundaries for RF and SVM",
    "text": "PCA Decision Boundaries for RF and SVM\nTo findout how the supervised models separate classes:\n\nCompute a 2D PCA projection of X_train and X_test.\nFit lightweight proxy models (RF/SVM) on the PCA coordinates only.\nPlot their decision regions and the true labels.\n\nThis gives an interpretable view of decision boundaries in a low-dimensional space.\n\n# PCA-based decision boundary visualization\n\nfig = plot_pca_decision_boundaries(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    feature_names=feature_names,\n    random_state=42,\n)\n\nfig_path = wk6_dir / \"pca_decision_boundaries_rf_svm.png\"\nfig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\nprint(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\pca_decision_boundaries_rf_svm.png"
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#transparency-vs-accuracy",
    "href": "notebooks/final/week_06_report_explain.html#transparency-vs-accuracy",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Transparency vs Accuracy",
    "text": "Transparency vs Accuracy\nFrom the Week 05 metric tables and the Week 06 explainability plots, we can summarize:\n\nRandom Forest (RF)\n\nStrong overall macro F1 and robust detection across families.\n\nGini and permutation importances reveal a small subset of features (e.g., connection counts, service type, error flags) driving most decisions.\n\nInterpretation burden is moderate: RF is an ensemble, but feature ranking + partial dependence give a clear story.\n\nSVM-RBF\n\nCompetitive or slightly better performance on some families, but decision function is more opaque.\n\nRequires Kernel SHAP for post-hoc explanations; local explanations are powerful but more compute-intensive and harder to summarize for non-experts.\n\nDeep Autoencoder\n\nExcellent at flagging rare or unseen patterns via reconstruction error.\n\nDirect interpretability is low; we rely on SHAP and error distributions to understand which features contribute to anomalies.\n\nBest used as an early-warning tripwire, not as the sole decision-maker.\n\nClassical unsupervised models (DBSCAN, LOF, Isolation Forest)\n\nProvide intuitive geometric views (dense vs sparse regions, anomaly score tails).\n\nHowever, binary outputs alone are not directly tied to specific attack semantics; they complement, but do not replace, supervised classifiers.\n\n\nOverall trade-off:\n- As we move from tree-based models → kernel methods → deep models, accuracy and flexibility tend to improve, but so does the interpretability burden.\n- A practical IDS design will often favor a hybrid stack:\n- RF (or similar) as the main, explainable classifier.\n- Deep / unsupervised models as anomaly detectors with well-chosen thresholds and clear triage pathways."
  },
  {
    "objectID": "notebooks/final/week_06_report_explain.html#real-world-ids-implications",
    "href": "notebooks/final/week_06_report_explain.html#real-world-ids-implications",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Real-World IDS Implications",
    "text": "Real-World IDS Implications\n\nFalse-positive budget\n\nEven a small false-positive rate (e.g., 0.5%) can overwhelm analysts at scale.\n\nUse anomaly score distributions and calibration checks to set thresholds that keep FPs within an acceptable budget (e.g., N alerts per day).\n\nThreshold tuning\n\nFor anomaly detectors (Isolation Forest, LOF, Autoencoder, DBSCAN-like scores), treat thresholds as policy knobs, not fixed constants.\n\nRevisit them regularly based on alert volumes and feedback from incident responders.\n\nModel maintenance & drift\n\nNetwork behavior evolves: new services, user patterns, and attack techniques appear.\n\nSchedule periodic retraining / revalidation (e.g., monthly or quarterly) and monitor simple drift indicators (feature distributions, reconstruction error shifts).\n\nRole specialization of models\n\nUse supervised models (RF/SVM) for labeled attack families and clear triage explanations.\n\nUse unsupervised / deep models as canaries to catch unusual behavior that does not match existing signatures.\n\nSHAP and PCA views help justify why a particular flow was flagged, supporting analyst trust."
  },
  {
    "objectID": "notebooks/Last/week_02_eda.html",
    "href": "notebooks/Last/week_02_eda.html",
    "title": "Week 02 — Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Objective. Visualize distributions, correlations, categorical frequencies, and low-dimensional structure.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n\n\n\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import dist_plots, corr_heatmap, topn_bar\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\ntry:\n    import umap\nexcept:\n    umap = None\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\nprint('Loading from:', raw_path)\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\ndf.shape\n\nLoading from: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n(494021, 43)\n\n\n\n# Distributions. (Plot histogram + KDE for selected numeric features on a stratified sample.)\nimport numpy as np\nimport pandas as pd\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import dist_plots\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\n\n# Limit to top 3 by frequency\ntop_fams = df['family'].value_counts().head(3).index\ndf_small = df[df['family'].isin(top_fams)].copy()\nprint(\"Using families:\", list(top_fams))\n\n# Take a MUCH SMALLER stratified sample per family\nsample_per_family = 500 \ndf_sample = (\n    df_small\n    .groupby('family', group_keys=False)[df_small.columns]\n    .apply(lambda x: x.sample(min(sample_per_family, len(x)), random_state=42))\n)\n\nprint(\"Sampled rows:\", len(df_sample))\n\n# Use only a FEW numeric columns\nnumeric_cols = [\n    c for c in df_sample.columns\n    if c not in ['protocol_type', 'service', 'flag', 'label', 'family']\n]\n\nsel = numeric_cols[:3]   \nprint(\"Columns to plot:\", sel)\n\ndist_plots(df_sample, sel, paths.figs)\nprint('Saved distribution plots for columns:', sel)\nprint('Figures directory:', paths.figs)\n\nUsing families: ['DoS', 'normal', 'Probe']\nSampled rows: 1500\nColumns to plot: ['duration', 'src_bytes', 'dst_bytes']\nSaved distribution plots for columns: ['duration', 'src_bytes', 'dst_bytes']\nFigures directory: C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Correlation by compute Pearson and Spearman heatmaps on numeric features.\nimport numpy as np\nimport pandas as pd\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.plots import corr_heatmap\n\npaths = Paths().ensure()\nset_global_seed(42)\nraw_path = paths.data_raw / 'NSL-KDD.raw'\ndf = map_attack_family(load_raw_nsl_kdd(raw_path))\n\nnumeric_cols = [c for c in df.columns if c not in ['protocol_type','service','flag','label','family']]\ncorr_cols = numeric_cols[:20]\ncorr_heatmap(df, corr_cols, paths.figs / 'eda_corr_pearson.png', method='pearson')\ncorr_heatmap(df, corr_cols, paths.figs / 'eda_corr_spearman.png', method='spearman')\nprint('Saved correlation heatmaps to', paths.figs)\n\nSaved correlation heatmaps to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Categorical frequencies. (Top-N bars for protocol_type, service, flag.)\ntopn_bar(df['protocol_type'], 10, paths.figs / 'eda_top_protocol.png', 'Top Protocol Types')\ntopn_bar(df['service'], 20, paths.figs / 'eda_top_service.png', 'Top Services')\ntopn_bar(df['flag'], 10, paths.figs / 'eda_top_flag.png', 'Top Flags')\nprint('Saved categorical frequency plots to', paths.figs)\n\nSaved categorical frequency plots to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# Family imbalance\nfam_counts = df['family'].value_counts()\nfam_perc = (fam_counts / len(df) * 100).round(2)\ndisplay(pd.DataFrame({'count': fam_counts, 'percent': fam_perc}))\n\n\n\n\n\n\n\n\ncount\npercent\n\n\nfamily\n\n\n\n\n\n\nDoS\n391458\n79.24\n\n\nnormal\n97278\n19.69\n\n\nProbe\n4107\n0.83\n\n\nR2L\n1126\n0.23\n\n\nU2R\n52\n0.01\n\n\n\n\n\n\n\n\n# PCA visualization.\nnum_df = df.select_dtypes(include=[np.number])\nX = num_df.fillna(0.0).to_numpy()\npca2 = PCA(n_components=2, random_state=42).fit_transform(X)\nfig, ax = plt.subplots()\nsns.scatterplot(x=pca2[:,0], y=pca2[:,1], hue=df['family'], ax=ax, s=10, linewidth=0)\nax.set_title('PCA (2D) by Family')\nfig.savefig(paths.figs / 'eda_pca2.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)\nprint('Saved PCA 2D to', paths.figs)\n\n\n\n\n\n\n\n\nSaved PCA 2D to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\n# t-SNE and UMAP. \nsample = (\n    df\n    .groupby('family', group_keys=False)[df.columns]\n    .apply(lambda x: x.sample(min(1500, len(x)), random_state=42))\n)\nX_s = sample.select_dtypes(include=[np.number]).fillna(0.0).to_numpy()\nts = TSNE(n_components=2, random_state=42, perplexity=30, init='pca')\nts2 = ts.fit_transform(X_s)\nfig, ax = plt.subplots()\nsns.scatterplot(x=ts2[:,0], y=ts2[:,1], hue=sample['family'], s=8, linewidth=0, ax=ax)\nax.set_title('t-SNE (2D) by Family — stratified sample')\nfig.savefig(paths.figs / 'eda_tsne.png', dpi=150, bbox_inches='tight')\nplt.show()\nplt.close(fig)\n\n\n\n\n\n\n\n\nSaved t-SNE/UMAP plots to C:\\Users\\mehra\\Final_Project\\notebooks\\figures\n\n\n\nNotes\n\nHeavy-tailed numeric features may benefit from log-scale in later analysis.\nRare classes (R2L, U2R) present risk for model bias; consider class-weighting/SMOTE in Week 4."
  },
  {
    "objectID": "notebooks/Last/week_04_ml_models.html",
    "href": "notebooks/Last/week_04_ml_models.html",
    "title": "Week 04 — Classical ML Models",
    "section": "",
    "text": "Unsupervised / semi-supervised\n\nIsolation Forest\nOne-Class SVM\nDeep Autoencoder\n\nSupervised\n\nRandom Forest\nSVM\n\nImbalance handling\n\nCompare class_weight vs SMOTE (train only) for rare classes (R2L / U2R analogues)\nKeep a clean baseline that uses only class_weight.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\nfrom src.models import make_fast_oneclass_svm\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\nimport os, sys\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\nfrom src.models import (\n    make_isolation_forest_grid,\n    make_oneclass_svm_grid,\n    make_rf_classifier_grid,\n    make_svm_rbf_grid,\n    build_deep_autoencoder,\n)\nfrom src.eval import (\n    plot_confusion,\n    classification_summary,\n    binary_roc_pr_curves,\n)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import f1_score\n\nset_global_seed(42)\npaths = Paths().ensure()\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\n# Load processed train/test arrays from Week 01.\nfrom pathlib import Path\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\ny_test  classes: Counter({np.int32(0): 78292, np.int32(4): 19456, np.int32(1): 822, np.int32(2): 225, np.int32(3): 10})\nTo evaluate unsupervised detectors, I convert the multi-class labels into a binary label:\nThis matches the standard NSL-KDD setup where attacks are rare compared to normal traffic, but it does not assume any particular encoding of labels.\n# Build binary labels for anomaly detection (0 = normal, 1 = attack).\nfrom collections import Counter\nimport numpy as np\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({np.int64(0): 313166, np.int64(1): 82050})\nBinary test  counts: Counter({np.int64(0): 78292, np.int64(1): 20513})"
  },
  {
    "objectID": "notebooks/Last/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "href": "notebooks/Last/week_04_ml_models.html#unsupervised-semi-supervised-models",
    "title": "Week 04 — Classical ML Models",
    "section": "1. Unsupervised / Semi-supervised models",
    "text": "1. Unsupervised / Semi-supervised models\n\n1.1 Isolation Forest\n\nfrom sklearn.metrics import f1_score\n\ndef iso_f1_scorer(estimator, X, y_true):\n    # Convert IsolationForest predictions {1, -1} into {0, 1} and compute F1.\n    y_raw = estimator.predict(X)\n    y_pred = (y_raw == -1).astype(int)  # -1 = anomaly → 1\n    return f1_score(y_true, y_pred, zero_division=0)\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom src.models import make_isolation_forest_grid\n\niso_spec = make_isolation_forest_grid()\n\niso_gs = GridSearchCV(\n    estimator=iso_spec.model,\n    param_grid=iso_spec.param_grid,\n    scoring=iso_f1_scorer,\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\n\niso_gs.fit(X_train, y_train_bin)\nprint(\"Best params (Isolation Forest):\", iso_gs.best_params_)\nprint(\"Best F1 (train CV):\", iso_gs.best_score_)\n\nFitting 3 folds for each of 18 candidates, totalling 54 fits\nBest params (Isolation Forest): {'contamination': 0.1, 'max_samples': 0.5, 'n_estimators': 200}\nBest F1 (train CV): 0.5217237922691421\n\n\n\n# Evaluate Isolation Forest on the held-out test set.\nbest_iso = iso_gs.best_estimator_\npred_iso = best_iso.predict(X_test)  # 1 = inlier, -1 = outlier\ny_pred_iso = np.where(pred_iso == -1, 1, 0)\n\nprint(\"Isolation Forest F1 (test, binary):\", f1_score(y_test_bin, y_pred_iso))\nplot_confusion(y_test_bin, y_pred_iso, labels=[0, 1], title=\"Isolation Forest (binary)\")\n\nIsolation Forest F1 (test, binary): 0.4988470913762435\n\n\n\n\n\n\n\n\n\n\n\n1.2 One-Class SVM (RBF)\n\n# OC-SVM (RBF)\nocsvm = make_fast_oneclass_svm()\n\n# Train on normal-only data\nmask_normal = (y_train_bin == 0)\nX_train_normal = X_train[mask_normal]\n\nocsvm.fit(X_train_normal)\n\npred = ocsvm.predict(X_test)\ny_pred_ocsvm = (pred == -1).astype(int)\n\nplot_confusion(y_test_bin, y_pred_ocsvm, labels=[0, 1], title=\"OC-SVM\")\n\n\n\n\n\n\n\n\n\n\n1.3 Deep Autoencoder\n\n### 1.3 Deep Autoencoder \n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\ninput_dim = X_train.shape[1]\nprint(\"Autoencoder input_dim:\", input_dim)\n\ninput_layer = keras.Input(shape=(input_dim,))\n\nencoded = layers.Dense(30, activation=\"relu\")(input_layer)\nencoded = layers.Dense(16, activation=\"relu\")(encoded)\nencoded = layers.Dense(8,  activation=\"relu\")(encoded)\n\ndecoded = layers.Dense(16, activation=\"relu\")(encoded)\ndecoded = layers.Dense(30, activation=\"relu\")(decoded)\ndecoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n\nautoencoder = keras.Model(input_layer, decoded, name=\"deep_autoencoder_ch2\")\n\nautoencoder.compile(\n    optimizer=\"adam\",\n    loss=\"mean_squared_error\"\n)\n\nautoencoder.summary()\n\nAutoencoder input_dim: 115\n\n\nModel: \"deep_autoencoder_ch2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer (InputLayer)        │ (None, 115)            │             0 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 30)             │         3,480 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 16)             │           496 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 8)              │           136 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (Dense)                 │ (None, 16)             │           144 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (Dense)                 │ (None, 30)             │           510 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (Dense)                 │ (None, 115)            │         3,565 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\n\n\n Total params: 8,331 (32.54 KB)\n\n\n\n Trainable params: 8,331 (32.54 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\n# Ensure numeric float dtype\nX_train_ae = X_train.astype(\"float32\")\nX_test_ae  = X_test.astype(\"float32\")\n\nhistory = autoencoder.fit(\n    X_train_ae,\n    X_train_ae,        # reconstruction target\n    epochs=10,         # like in Chapter_2 screenshot\n    batch_size=256,\n    shuffle=True,\n    validation_split=0.1,\n    verbose=1,\n)\n\n\nEpoch 1/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 5s 2ms/step - loss: 0.1578 - val_loss: 0.0460\n\nEpoch 2/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0696 - val_loss: 0.0408\n\nEpoch 3/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0703 - val_loss: 0.0362\n\nEpoch 4/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0600 - val_loss: 0.0204\n\nEpoch 5/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0625 - val_loss: 0.0183\n\nEpoch 6/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0469 - val_loss: 0.0182\n\nEpoch 7/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 4s 3ms/step - loss: 0.0645 - val_loss: 0.0170\n\nEpoch 8/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0410 - val_loss: 0.0161\n\nEpoch 9/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0469 - val_loss: 0.0162\n\nEpoch 10/10\n\n1390/1390 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - loss: 0.0336 - val_loss: 0.0125\n\n\n\n\n\n# Reconstruction error on training + test\nrecon_train = autoencoder.predict(X_train, verbose=0)\nrecon_test = autoencoder.predict(X_test, verbose=0)\n\ntrain_err = np.mean((X_train - recon_train) ** 2, axis=1)\ntest_err = np.mean((X_test - recon_test) ** 2, axis=1)\n\n# Choose threshold as a high percentile of training error\nthreshold = np.percentile(train_err, 99)\nprint(\"Autoencoder threshold (99th percentile of train error):\", threshold)\n\ny_pred_ae = (test_err &gt;= threshold).astype(int)\n\nprint(\"Autoencoder F1 (test, binary):\", f1_score(y_test_bin, y_pred_ae))\nplot_confusion(y_test_bin, y_pred_ae, labels=[0, 1], title=\"Autoencoder\")\n\nAutoencoder threshold (99th percentile of train error): 0.15334489130821294\nAutoencoder F1 (test, binary): 0.08649250614820658\n\n\n\n\n\n\n\n\n\n\n# ROC / PR curves for the autoencoder scores\nbinary_roc_pr_curves(y_test_bin, test_err, pos_label=1, title_prefix=\"Autoencoder \")\n\n(&lt;Figure size 1000x400 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Autoencoder ROC (AUC=0.953)'}, xlabel='False positive rate', ylabel='True positive rate'&gt;,\n        &lt;Axes: title={'center': 'Autoencoder Precision-Recall'}, xlabel='Recall', ylabel='Precision'&gt;],\n       dtype=object))"
  },
  {
    "objectID": "notebooks/Last/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "href": "notebooks/Last/week_04_ml_models.html#supervised-models-random-forest-and-svm-rbf",
    "title": "Week 04 — Classical ML Models",
    "section": "2. Supervised models: Random Forest and SVM (RBF)",
    "text": "2. Supervised models: Random Forest and SVM (RBF)\n\n2.1 Random Forest\nUse the full multi-class labels (y_train, y_test) and train:\n\nRandom Forest (class_weight=“balanced”; tune n_estimators, max_depth)\nSVM with RBF kernel (class_weight=“balanced”; tune C, gamma)\n\nThis gives a supervised baseline to compare against the unsupervised / semi-supervised detectors.\n\nrf_spec = make_rf_classifier_grid()\nrf_gs = GridSearchCV(\n    estimator=rf_spec.model,\n    param_grid=rf_spec.param_grid,\n    scoring=\"f1_weighted\",\n    n_jobs=-1,\n    cv=3,\n    verbose=1,\n)\nrf_gs.fit(X_train, y_train)\n\nprint(\"Random Forest best params:\", rf_gs.best_params_)\nprint(\"Random Forest best CV f1_weighted:\", rf_gs.best_score_)\n\nFitting 3 folds for each of 9 candidates, totalling 27 fits\nRandom Forest best params: {'max_depth': 20, 'n_estimators': 400}\nRandom Forest best CV f1_weighted: 0.9997615589652838\n\n\n\nbest_rf = rf_gs.best_estimator_\ny_pred_rf = best_rf.predict(X_test)\n\nrf_report = classification_summary(y_test, y_pred_rf, print_report=True)\nplot_confusion(y_test, y_pred_rf, title=\"Random Forest\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.96      0.97      0.97     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\n\n\n\n\n\n\n\n\n\n\n2.2 SVM-RBF\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# SVM-RBF\nsvm_fast = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"svc\", SVC(\n        kernel=\"rbf\",\n        class_weight=\"balanced\",\n        C=10,          # reasonable value\n        gamma=\"scale\"  # reasonable value\n    ))\n])\n\nsvm_fast.fit(X_train, y_train)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF\")\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.97      0.98      0.97       822\n           2       0.72      0.98      0.83       225\n           3       0.73      0.80      0.76        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.88      0.95      0.91     98805\nweighted avg       1.00      1.00      1.00     98805"
  },
  {
    "objectID": "notebooks/Last/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "href": "notebooks/Last/week_04_ml_models.html#imbalance-handling-class_weight-vs-smote-for-rare-classes-r2l-u2r-analogues",
    "title": "Week 04 — Classical ML Models",
    "section": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)",
    "text": "3. Imbalance handling: class_weight vs SMOTE for rare classes (R2L / U2R analogues)\nNSL-KDD includes very rare R2L and U2R attack families. Instead of assuming I already know which integer labels correspond to these families, I follow a data-driven approach:\n\nIdentify the two rarest classes in y_train.\nTreat them as “R2L/U2R-like” minority classes.\nCompare:\n\nBaseline models using only class_weight=\"balanced\"\nModels trained on SMOTE-resampled data (train only), focusing on those rare classes.\n\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Identify the two rarest classes\nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes (treated as R2L/U2R-like):\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\nMinority classes (treated as R2L/U2R-like): [np.int32(3), np.int32(2)]\nClass counts: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\n\n\n\ndef evaluate_subset(classes_subset, y_true, y_pred):\n    import numpy as np\n    mask = np.isin(y_true, classes_subset)\n    y_true_sub = y_true[mask]\n    y_pred_sub = y_pred[mask]\n    print(\"Subset size:\", y_true_sub.shape[0])\n    return classification_summary(y_true_sub, y_pred_sub, print_report=True)\n\n\n3.1 Baseline: class_weight only\n\nprint(\"Random Forest (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf)\n\nprint(\"\\nSVM-RBF (class_weight baseline) — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_svm)\n\nRandom Forest (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.90      0.90      0.90        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.63      0.63      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\nSVM-RBF (class_weight baseline) — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       1.00      0.80      0.89        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.67      0.59      0.63       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n\n\n3.2 SMOTE + Random Forest / SVM-RBF (train only)\n\n# SMOTE strategy: upsample only the identified minority classes\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE counts:\", Counter(y_train_sm))\n\nSMOTE sampling strategy: {np.int32(3): 313166, np.int32(2): 313166}\nAfter SMOTE counts: Counter({np.int32(0): 313166, np.int32(2): 313166, np.int32(3): 313166, np.int32(4): 77822, np.int32(1): 3285})\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Reuse best params from the baseline RF grid search\nprint(\"Baseline RF best params:\", rf_gs.best_params_)\n\nrf_sm = RandomForestClassifier(\n    n_estimators=rf_gs.best_params_[\"n_estimators\"],\n    max_depth=rf_gs.best_params_[\"max_depth\"],\n    n_jobs=-1,\n    class_weight=\"balanced\",    # you can also set this to None, since SMOTE already balances\n    random_state=42,\n)\n\n# Fit on SMOTE-resampled data \nrf_sm.fit(X_train_sm, y_train_sm)\n\ny_pred_rf_sm = rf_sm.predict(X_test)\n\nprint(\"\\nRandom Forest + SMOTE — full test report\")\n_ = classification_summary(y_test, y_pred_rf_sm, print_report=True)\n\nprint(\"\\nRandom Forest + SMOTE — minority classes\")\n_ = evaluate_subset(minority_classes, y_test, y_pred_rf_sm)\n\nBaseline RF best params: {'max_depth': 20, 'n_estimators': 400}\n\nRandom Forest + SMOTE — full test report\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       1.00      0.99      1.00       822\n           2       0.99      0.98      0.99       225\n           3       0.75      0.90      0.82        10\n           4       1.00      1.00      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.95      0.98      0.96     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nRandom Forest + SMOTE — minority classes\nSubset size: 235\n              precision    recall  f1-score   support\n\n           2       1.00      0.98      0.99       225\n           3       0.82      0.90      0.86        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.98       235\n   macro avg       0.60      0.63      0.62       235\nweighted avg       0.99      0.98      0.98       235\n\n\n\n\n\n4. SVM-RBF (Baseline, Fast Subset)\n\nimport numpy as np\nfrom collections import Counter\n\ndef build_svm_subset(X, y, minority_classes, max_samples=5000, random_state=42):\n    # Build a smaller train set for SVM:\n    # - Keep ALL samples from minority_classes\n    # - Subsample majority classes so total ~ max_samples\n\n    rng = np.random.RandomState(random_state)\n    y = np.asarray(y)\n\n    # Indices for minority and majority\n    mask_min = np.isin(y, minority_classes)\n    idx_min = np.where(mask_min)[0]\n    idx_maj = np.where(~mask_min)[0]\n\n    # Always keep all minority samples\n    keep_idx = list(idx_min)\n\n    # Remaining budget for majority\n    remaining = max(0, max_samples - len(idx_min))\n    if remaining &gt; 0 and len(idx_maj) &gt; remaining:\n        subsampled_maj = rng.choice(idx_maj, size=remaining, replace=False)\n    else:\n        subsampled_maj = idx_maj\n\n    keep_idx.extend(subsampled_maj)\n    keep_idx = np.array(keep_idx)\n\n    X_small = X[keep_idx]\n    y_small = y[keep_idx]\n\n    print(\"SVM subset size:\", X_small.shape, \"class counts:\", Counter(y_small))\n\n    return X_small, y_small\n\n\n\n5. SVM-RBF with SMOTE Oversampling\n\nfrom src.models import make_fast_svm_rbf\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\n# Identify minority classes \nclass_counts_full = Counter(y_train)\nclasses_sorted = sorted(class_counts_full.items(), key=lambda kv: kv[1])\nminority_classes = [c for c, _ in classes_sorted[:2]]\nprint(\"Minority classes:\", minority_classes)\nprint(\"Class counts:\", class_counts_full)\n\n# Build smaller subset for SVM baseline (no SMOTE)\nX_train_svm, y_train_svm = build_svm_subset(\n    X_train, y_train,\n    minority_classes=minority_classes,\n    max_samples=2000,        # you can drop to 3000 or 2000 if still slow\n    random_state=42\n)\n\n# SMOTE on FULL train, then subset for SVM\ndesired = max(class_counts_full.values())\nsampling_strategy = {cls: desired for cls in minority_classes}\nprint(\"SMOTE sampling strategy:\", sampling_strategy)\n\nsmote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\nX_train_sm_full, y_train_sm_full = smote.fit_resample(X_train, y_train)\nprint(\"After SMOTE (full):\", Counter(y_train_sm_full))\n\n# Subset the SMOTE-resampled set for SVM\nX_train_sm_svm, y_train_sm_svm = build_svm_subset(\n    X_train_sm_full, y_train_sm_full,\n    minority_classes=minority_classes,\n    max_samples=5000,\n    random_state=42\n)\n\n# SVM baseline\nsvm_fast = make_fast_svm_rbf()\nsvm_fast.fit(X_train_svm, y_train_svm)\n\ny_pred_svm = svm_fast.predict(X_test)\n\nprint(\"\\nSVM-RBF baseline (fast, subset)\")\nclassification_summary(y_test, y_pred_svm, print_report=True)\nplot_confusion(y_test, y_pred_svm, title=\"SVM-RBF (fast baseline, subset)\")\n\n# SVM on SMOTE-resampled subset\nsvm_sm = make_fast_svm_rbf()\nsvm_sm.fit(X_train_sm_svm, y_train_sm_svm)\n\ny_pred_svm_sm = svm_sm.predict(X_test)\n\nprint(\"\\nSVM-RBF + SMOTE (fast, subset)\")\nclassification_summary(y_test, y_pred_svm_sm, print_report=True)\n\nprint(\"\\nMinority class performance (SVM + SMOTE)\")\nevaluate_subset(minority_classes, y_test, y_pred_svm_sm)\n\nMinority classes: [np.int32(3), np.int32(2)]\nClass counts: Counter({np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285, np.int32(2): 901, np.int32(3): 42})\nSVM subset size: (2000, 115) class counts: Counter({np.int32(2): 901, np.int32(0): 853, np.int32(4): 201, np.int32(3): 42, np.int32(1): 3})\nSMOTE sampling strategy: {np.int32(3): 313166, np.int32(2): 313166}\nAfter SMOTE (full): Counter({np.int32(0): 313166, np.int32(2): 313166, np.int32(3): 313166, np.int32(4): 77822, np.int32(1): 3285})\nSVM subset size: (1020605, 115) class counts: Counter({np.int32(2): 313166, np.int32(3): 313166, np.int32(0): 313166, np.int32(4): 77822, np.int32(1): 3285})\n\nSVM-RBF baseline (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00     78292\n           1       1.00      0.07      0.13       822\n           2       0.13      1.00      0.23       225\n           3       0.11      0.60      0.19        10\n           4       0.97      0.96      0.97     19456\n\n    accuracy                           0.98     98805\n   macro avg       0.64      0.72      0.50     98805\nweighted avg       0.99      0.98      0.98     98805\n\n\nSVM-RBF + SMOTE (fast, subset)\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     78292\n           1       0.98      0.98      0.98       822\n           2       0.70      0.98      0.82       225\n           3       0.78      0.70      0.74        10\n           4       1.00      0.99      1.00     19456\n\n    accuracy                           1.00     98805\n   macro avg       0.89      0.93      0.91     98805\nweighted avg       1.00      1.00      1.00     98805\n\n\nMinority class performance (SVM + SMOTE)\nSubset size: 235\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         0\n           2       1.00      0.98      0.99       225\n           3       1.00      0.70      0.82        10\n           4       0.00      0.00      0.00         0\n\n    accuracy                           0.97       235\n   macro avg       0.50      0.42      0.45       235\nweighted avg       1.00      0.97      0.98       235\n\n\n\n{'0': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n '2': {'precision': 0.995475113122172,\n  'recall': 0.9777777777777777,\n  'f1-score': 0.9865470852017937,\n  'support': 225.0},\n '3': {'precision': 1.0,\n  'recall': 0.7,\n  'f1-score': 0.8235294117647058,\n  'support': 10.0},\n '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0},\n 'accuracy': 0.9659574468085106,\n 'macro avg': {'precision': 0.498868778280543,\n  'recall': 0.4194444444444444,\n  'f1-score': 0.4525191242416249,\n  'support': 235.0},\n 'weighted avg': {'precision': 0.9956676614999519,\n  'recall': 0.9659574468085106,\n  'f1-score': 0.9796101629278751,\n  'support': 235.0}}\n\n\n\n\n\n\n\n\n\n\n# Save existing results for later evaluation\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\n# Use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk4_dir = reports_root / \"week_04\"\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Saving Week 04 results under:\", wk4_dir)\n\n# UNSUPERVISED / SEMI-SUPERVISED \n#\n# Use variables that ALREADY exist from earlier cells:\n#   y_test_bin     : binary true labels (0 = normal, 1 = attack)\n#   y_pred_iso     : Isolation Forest binary predictions\n#   y_pred_ocsvm   : OC-SVM binary predictions\n#   y_pred_ae      : Autoencoder binary predictions\n#   test_err       : Autoencoder reconstruction error (per test sample)\n#\n# Scores are taken from existing values:\n#   - Isolation Forest / OC-SVM: score = y_pred_* (0/1 as a crude anomaly score)\n#   - Autoencoder: score = test_err reconstruction error)\n\nunsup_frames = []\n\ndef add_binary_model_existing(name, y_true, y_pred, score):\n    df = pd.DataFrame({\n        \"model\":      name,\n        \"y_true_bin\": np.asarray(y_true).astype(int),\n        \"y_pred_bin\": np.asarray(y_pred).astype(int),\n        \"score\":      np.asarray(score).astype(float),\n    })\n    unsup_frames.append(df)\n    print(f\"Added unsupervised/semi-supervised model: {name}\")\n\n# Isolation Forest\nif \"y_test_bin\" in globals() and \"y_pred_iso\" in globals():\n    add_binary_model_existing(\n        \"Isolation Forest\",\n        y_test_bin,\n        y_pred_iso,\n        y_pred_iso,   # just reuse 0/1 prediction as score (no recomputation)\n    )\nelse:\n    print(\"Isolation Forest predictions not found; skipping.\")\n\n# OC-SVM\nif \"y_test_bin\" in globals() and \"y_pred_ocsvm\" in globals():\n    add_binary_model_existing(\n        \"OC-SVM (RBF)\",\n        y_test_bin,\n        y_pred_ocsvm,\n        y_pred_ocsvm,  # reuse 0/1 prediction as score\n    )\nelse:\n    print(\"OC-SVM predictions not found; skipping.\")\n\n# Autoencoder\nif \"y_test_bin\" in globals() and \"y_pred_ae\" in globals() and \"test_err\" in globals():\n    add_binary_model_existing(\n        \"Deep Autoencoder\",\n        y_test_bin,\n        y_pred_ae,\n        test_err,      # reconstruction error already computed\n    )\nelse:\n    print(\"Autoencoder predictions/errors not found; skipping.\")\n\n#if unsup_frames:\n#    unsup_df = pd.concat(unsup_frames, ignore_index=True)\n#    unsup_csv_path = wk4_dir / \"week04_unsupervised_predictions.csv\"\n#    unsup_parquet_path = wk4_dir / \"week04_unsupervised_predictions.parquet\"\n\n#    unsup_df.to_csv(unsup_csv_path, index=False)\n#    try:\n#        unsup_df.to_parquet(unsup_parquet_path, index=False)\n#    except Exception as e:\n#        print(\"Parquet save for unsupervised models failed (optional):\", e)\n\n#    print(\"\\nSaved unsupervised/semi-supervised results to:\")\n#    print(\" -\", unsup_csv_path)\n#    print(\" -\", unsup_parquet_path)\n#else:\n#    print(\"No unsupervised/semi-supervised models were added; nothing saved for that group.\")\n\nsup_frames = []\n\ndef add_supervised_existing(model_name, y_true, y_pred):\n    df = pd.DataFrame({\n        \"model\":  model_name,\n        \"y_true\": np.asarray(y_true),\n        \"y_pred\": np.asarray(y_pred),\n    })\n    sup_frames.append(df)\n    print(f\"Added supervised model: {model_name}\")\n\n# Random Forest baseline\nif \"y_test\" in globals() and \"y_pred_rf\" in globals():\n    add_supervised_existing(\"Random Forest (baseline)\", y_test, y_pred_rf)\nelse:\n    print(\"Random Forest baseline predictions not found; skipping.\")\n\n# SVM-RBF baseline\nif \"y_test\" in globals() and \"y_pred_svm\" in globals():\n    add_supervised_existing(\"SVM-RBF (baseline)\", y_test, y_pred_svm)\nelse:\n    print(\"SVM-RBF baseline predictions not found; skipping.\")\n\n# Random Forest + SMOTE\nif \"y_test\" in globals() and \"y_pred_rf_sm\" in globals():\n    add_supervised_existing(\"Random Forest + SMOTE\", y_test, y_pred_rf_sm)\nelse:\n    print(\"Random Forest + SMOTE predictions not found; skipping.\")\n\n# SVM-RBF + SMOTE\nif \"y_test\" in globals() and \"y_pred_svm_sm\" in globals():\n    add_supervised_existing(\"SVM-RBF + SMOTE\", y_test, y_pred_svm_sm)\nelse:\n    print(\"SVM-RBF + SMOTE predictions not found; skipping.\")\n\n#if sup_frames:\n#    sup_pred_df = pd.concat(sup_frames, ignore_index=True)\n#    sup_csv_path = wk4_dir / \"week04_supervised_predictions.csv\"\n#    sup_parquet_path = wk4_dir / \"week04_supervised_predictions.parquet\"\n\n#    sup_pred_df.to_csv(sup_csv_path, index=False)\n#    try:\n#        sup_pred_df.to_parquet(sup_parquet_path, index=False)\n#    except Exception as e:\n#        print(\"Parquet save for supervised models failed (optional):\", e)\n\n#    print(\"\\nSaved supervised results to:\")\n#    print(\" -\", sup_csv_path)\n#    print(\" -\", sup_parquet_path)\n#else:\n#    print(\"No supervised models were added; nothing saved for that group.\")\n\nSaving Week 04 results under: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nAdded unsupervised/semi-supervised model: Isolation Forest\nAdded unsupervised/semi-supervised model: OC-SVM (RBF)\nAdded unsupervised/semi-supervised model: Deep Autoencoder\nAdded supervised model: Random Forest (baseline)\nAdded supervised model: SVM-RBF (baseline)\nAdded supervised model: Random Forest + SMOTE\nAdded supervised model: SVM-RBF + SMOTE\n\n\n\n# Save Week 04 models for Week 06 explainability\nimport joblib\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\n\nmodels_dir = getattr(paths, \"models\", project_root / \"models\")\nmodels_dir.mkdir(parents=True, exist_ok=True)\n\nrf_path  = models_dir / \"week04_rf.joblib\"\nsvm_path = models_dir / \"week04_svm.joblib\"\nae_path  = models_dir / \"week04_ae.joblib\"\n\nprint(\"Saving models to:\", models_dir)\n\n# These variables already exist in your notebook:\n#   best_rf   -&gt; best RandomForest from GridSearch\n#   svm_fast  -&gt; SVM-RBF Pipeline (baseline)\n#   autoencoder -&gt; trained deep autoencoder\n\njoblib.dump(best_rf, rf_path)\nprint(\"Saved RF to:\", rf_path)\n\njoblib.dump(svm_fast, svm_path)\nprint(\"Saved SVM to:\", svm_path)\n\njoblib.dump(autoencoder, ae_path)\nprint(\"Saved Autoencoder to:\", ae_path)\n\nSaving models to: C:\\Users\\mehra\\Final_Project\\models\nSaved RF to: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSaved SVM to: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nSaved Autoencoder to: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html",
    "href": "notebooks/Last/week_06_report_explain.html",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "",
    "text": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.decomposition import PCA\n\n# Use a clean seaborn style\nsns.set(style=\"whitegrid\")\n\n# Project paths\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nfrom src.utils import Paths, set_global_seed\n\nset_global_seed(42)\npaths = Paths().ensure()\n\n# Derive a robust reports root (Week 03–06)\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\nwk6_dir = reports_root / \"week_06\"\n\nwk6_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Project root:\", project_root)\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports:\", wk3_dir)\nprint(\"Week 04 reports:\", wk4_dir)\nprint(\"Week 06 reports:\", wk6_dir)\n\nProject root: C:\\Users\\mehra\\Final_Project\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_04\nWeek 06 reports: C:\\Users\\mehra\\Final_Project\\reports\\week_06\n# Load processed arrays from Week 01\n\nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n\n# binary labels (0=normal, 1=attack) from Week 04 logic\nfrom collections import Counter\n\nclass_counts = Counter(y_train)\nnormal_label = max(class_counts, key=class_counts.get)\nprint(\"Treating label\", normal_label, \"as NORMAL (0). All others = ATTACK (1).\")\n\ndef to_binary(y, normal):\n    return np.where(y == normal, 0, 1)\n\ny_train_bin = to_binary(y_train, normal_label)\ny_test_bin  = to_binary(y_test, normal_label)\n\nprint(\"Binary train counts:\", Counter(y_train_bin))\nprint(\"Binary test  counts:\", Counter(y_test_bin))\n\nX_train: (395216, 115) X_test: (98805, 115)\nTreating label 0 as NORMAL (0). All others = ATTACK (1).\nBinary train counts: Counter({np.int64(0): 313166, np.int64(1): 82050})\nBinary test  counts: Counter({np.int64(0): 78292, np.int64(1): 20513})\n# Load Week 03 & Week 04 prediction files \n\nfrom pathlib import Path\n\ndef _load_csv_if_exists(path: Path, label: str):\n    if not path.exists():\n        print(f\"{label} not found at {path} — returning None.\")\n        return None\n    df = pd.read_csv(path)\n    print(f\"{label} loaded from {path} with shape {df.shape}\")\n    return df\n\nwk3_unsup = _load_csv_if_exists(\n    wk3_dir / \"week03_unsupervised_predictions.csv\",\n    \"Week 03 unsupervised predictions\"\n)\n\nwk4_unsup = _load_csv_if_exists(\n    wk4_dir / \"week04_unsupervised_predictions.csv\",\n    \"Week 04 unsupervised/semi-supervised predictions\"\n)\n\nwk4_sup = _load_csv_if_exists(\n    wk4_dir / \"week04_supervised_predictions.csv\",\n    \"Week 04 supervised predictions\"\n)\n\nprint(\"\\nSummary:\")\nprint(\"  Week 03 unsupervised df:\", None if wk3_unsup is None else wk3_unsup.shape)\nprint(\"  Week 04 unsupervised df:\", None if wk4_unsup is None else wk4_unsup.shape)\nprint(\"  Week 04 supervised df:\", None if wk4_sup is None else wk4_sup.shape)\n\nfrom IPython.display import display\n\nif wk3_unsup is not None:\n    print(\"\\nWeek 03 unsupervised head:\")\n    display(wk3_unsup.head())\n\nif wk4_unsup is not None:\n    print(\"\\nWeek 04 unsupervised head:\")\n    display(wk4_unsup.head())\n\nif wk4_sup is not None:\n    print(\"\\nWeek 04 supervised head:\")\n    display(wk4_sup.head())\n\nWeek 03 unsupervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv with shape (98257, 4)\nWeek 04 unsupervised/semi-supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_unsupervised_predictions.csv with shape (296415, 4)\nWeek 04 supervised predictions loaded from C:\\Users\\mehra\\Final_Project\\reports\\week_04\\week04_supervised_predictions.csv with shape (395220, 3)\n\nSummary:\n  Week 03 unsupervised df: (98257, 4)\n  Week 04 unsupervised df: (296415, 4)\n  Week 04 supervised df: (395220, 3)\n\nWeek 03 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n\n\n\n\n\n\nWeek 04 unsupervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0\n\n\n\n\n\n\n\n\nWeek 04 supervised head:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#feature-names-models",
    "href": "notebooks/Last/week_06_report_explain.html#feature-names-models",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Feature Names & Models",
    "text": "Feature Names & Models\n\n# Feature names via preprocessor\n\nfeature_names = None\ntry:\n    import joblib\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = list(pre.get_feature_names_out())\n    print(\"Loaded feature names from preprocessor:\", len(feature_names))\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    # Fall back to simple numeric names\n    feature_names = [f\"f{i}\" for i in range(X_train.shape[1])]\n    print(\"Using generic feature names:\", len(feature_names))\n\nLoaded feature names from preprocessor: 115\n\n\n\nimport joblib\n\nrf_model = None\nsvm_model = None\nae_model = None\n\nrf_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_rf.joblib\"\nsvm_path = getattr(paths, \"models\", project_root / \"models\") / \"week04_svm.joblib\"\nae_path  = getattr(paths, \"models\", project_root / \"models\") / \"week04_ae.joblib\"\n\nfor name, p in [(\"RF\", rf_path), (\"SVM\", svm_path), (\"AE\", ae_path)]:\n    print(f\"{name} expected at:\", p)\n\ntry:\n    if rf_path.exists():\n        rf_model = joblib.load(rf_path)\n        print(\"Loaded RF model.\")\n    else:\n        print(\"RF model file not found.\")\nexcept Exception as e:\n    print(\"Could not load RF model:\", e)\n\ntry:\n    if svm_path.exists():\n        svm_model = joblib.load(svm_path)\n        print(\"Loaded SVM model.\")\n    else:\n        print(\"SVM model file not found.\")\nexcept Exception as e:\n    print(\"Could not load SVM model:\", e)\n\ntry:\n    if ae_path.exists():\n        ae_model = joblib.load(ae_path)\n        print(\"Loaded Autoencoder model.\")\n    else:\n        print(\"Autoencoder model file not found.\")\nexcept Exception as e:\n    print(\"Could not load Autoencoder model:\", e)\n\nRF expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_rf.joblib\nSVM expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_svm.joblib\nAE expected at: C:\\Users\\mehra\\Final_Project\\models\\week04_ae.joblib\nLoaded RF model.\nLoaded SVM model.\nLoaded Autoencoder model."
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#import-explainability-utilities",
    "href": "notebooks/Last/week_06_report_explain.html#import-explainability-utilities",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Import explainability utilities",
    "text": "Import explainability utilities\n\nRandom Forest feature importance (Gini & permutation)\nPermutation importance for SVM-RBF\nUnsupervised visualizations (PCA + anomaly scores)\nPCA decision boundary plots\n\n\nfrom src.explain import (\n    compute_rf_feature_importance,\n    compute_permutation_importance,\n    plot_anomaly_score_distributions,\n    plot_reconstruction_error_hist,\n    compute_pca_embedding,\n    plot_pca_dbscan_like,\n    plot_pca_decision_boundaries,\n)"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#random-forest",
    "href": "notebooks/Last/week_06_report_explain.html#random-forest",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Random Forest",
    "text": "Random Forest\n\nGini-based feature importance\nPermutation importance on a test subset\n\nit shows which features matter most to the RF decision function.\n\n# Random Forest feature importance & permutation importance\n\nif rf_model is None:\n    print(\"RF model not loaded — skipping RF explainability. \"\n          \"Save your Random Forest as 'week04_rf.joblib' in paths.models to enable this section.\")\nelse:\n    # Gini importance\n    rf_imp_df = compute_rf_feature_importance(rf_model, feature_names)\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.barplot(data=rf_imp_df.head(20), x=\"importance\", y=\"feature\", ax=ax)\n    ax.set_title(\"Random Forest — Top 20 Gini Feature Importances\")\n    plt.tight_layout()\n    fig_path = wk6_dir / \"rf_gini_importance_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    \n    # Permutation importance on a test set\n    rf_perm_df = compute_permutation_importance(\n        rf_model,\n        X_test,\n        y_test,\n        feature_names,\n        n_repeats=10,\n        max_samples=3000,\n        random_state=42,\n    )\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.barplot(data=rf_perm_df.head(20), x=\"importance\", y=\"feature\", ax=ax)\n    ax.set_title(\"Random Forest — Top 20 Permutation Importances\")\n    plt.tight_layout()\n    fig_path = wk6_dir / \"rf_permutation_importance_top20.png\"\n    fig.savefig(fig_path, dpi=150)\n    print(\"Saved:\", fig_path)\n    \n    rf_imp_df.head()\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\rf_gini_importance_top20.png\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\rf_permutation_importance_top20.png"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#svm-and-autoencoder",
    "href": "notebooks/Last/week_06_report_explain.html#svm-and-autoencoder",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "SVM and Autoencoder",
    "text": "SVM and Autoencoder\n\nFor SVM-RBF, I compute permutation importance on the test set. This shows which input features the decision function relies on most, even though the kernel mapping itself remains opaque.\nFor the Autoencoder, I treat the reconstruction error distribution as the main explanation signal: which samples are hardest to reconstruct, how far into the tail an anomalous point sits, and how thresholds would trade precision vs recall.\n\n\n# SVM-RBF permutation importance \n\nif svm_model is None:\n    print(\"SVM model not loaded — skipping SVM permutation importance. \"\n          \"Save your SVM as 'week04_svm.joblib' in paths.models to enable this section.\")\nelse:\n    try:\n        svm_perm_df = compute_permutation_importance(\n            svm_model,\n            X_test,\n            y_test,\n            feature_names,\n            n_repeats=10,\n            max_samples=3000,\n            random_state=42,\n        )\n        fig, ax = plt.subplots(figsize=(6, 6))\n        sns.barplot(data=svm_perm_df.head(20), x=\"importance\", y=\"feature\", ax=ax)\n        ax.set_title(\"SVM-RBF — Top 20 Permutation Importances\")\n        plt.tight_layout()\n        fig_path = wk6_dir / \"svm_permutation_importance_top20.png\"\n        fig.savefig(fig_path, dpi=150)\n        print(\"Saved:\", fig_path)\n        \n        svm_perm_df.head()\n    except Exception as e:\n        print(\"SVM permutation importance computation failed:\", e)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\svm_permutation_importance_top20.png"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#unsupervised-models",
    "href": "notebooks/Last/week_06_report_explain.html#unsupervised-models",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Unsupervised Models",
    "text": "Unsupervised Models\nUsing the saved results from Week 03 and Week 04, I visualize:\n\nDBSCAN-style separation in PCA space (binary normal vs anomaly by DBSCAN)\nAnomaly score distributions for LOF, Isolation Forest, etc.\nReconstruction error histogram for the Autoencoder\n\n\n# PCA embedding for visualization\n\nX_test_pca = compute_pca_embedding(X_test, n_components=2, random_state=42)\nX_test_pca.shape\n\n(98805, 2)\n\n\n\n# Anomaly score distributions for unsupervised models (Week 03 + Week 04)\n\nif wk3_unsup is None and wk4_unsup is None:\n    print(\"No unsupervised score tables loaded — skipping anomaly score distributions.\")\nelse:\n    fig = plot_anomaly_score_distributions(\n        wk3_unsup=wk3_unsup,\n        wk4_unsup=wk4_unsup,\n        max_models=6,\n    )\n    fig_path = wk6_dir / \"unsupervised_anomaly_score_distributions.png\"\n    fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n    print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\unsupervised_anomaly_score_distributions.png\n\n\n\n\n\n\n\n\n\n\n# Autoencoder reconstruction error histogram (Week 04 unsupervised)\n\nif wk4_unsup is None:\n    print(\"Week 04 unsupervised df not loaded — skipping AE reconstruction error histogram.\")\nelse:\n    mask_ae = wk4_unsup[\"model\"].str.contains(\"Autoencoder\", case=False, na=False)\n    if not mask_ae.any():\n        print(\"No Autoencoder rows in Week 04 unsupervised df.\")\n    else:\n        ae_scores = wk4_unsup.loc[mask_ae, \"score\"].values\n        fig = plot_reconstruction_error_hist(ae_scores)\n        fig_path = wk6_dir / \"ae_reconstruction_error_hist.png\"\n        fig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n        print(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\ae_reconstruction_error_hist.png"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#pca-decision-boundaries-for-rf-and-svm",
    "href": "notebooks/Last/week_06_report_explain.html#pca-decision-boundaries-for-rf-and-svm",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "PCA Decision Boundaries for RF and SVM",
    "text": "PCA Decision Boundaries for RF and SVM\nTo findout how the supervised models separate classes:\n\nCompute a 2D PCA projection of X_train and X_test.\nFit lightweight proxy models (RF/SVM) on the PCA coordinates only.\nPlot their decision regions and the true labels.\n\nThis gives an interpretable view of decision boundaries in a low-dimensional space.\n\n# PCA-based decision boundary visualization\n\nfig = plot_pca_decision_boundaries(\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n    feature_names=feature_names,\n    random_state=42,\n)\n\nfig_path = wk6_dir / \"pca_decision_boundaries_rf_svm.png\"\nfig.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\nprint(\"Saved:\", fig_path)\n\nSaved: C:\\Users\\mehra\\Final_Project\\reports\\week_06\\pca_decision_boundaries_rf_svm.png"
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#transparency-vs-accuracy",
    "href": "notebooks/Last/week_06_report_explain.html#transparency-vs-accuracy",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Transparency vs Accuracy",
    "text": "Transparency vs Accuracy\nFrom the Week 05 metric tables and the Week 06 explainability plots, I can summarize:\n\nRandom Forest (RF)\n\nStrong overall macro F1 and robust detection across families.\n\nGini and permutation importances reveal a small subset of features (e.g., duration, connection counts, service type, error flags) driving most decisions.\n\nInterpretation burden is moderate: RF is an ensemble, but feature ranking plus simple partial reasoning about top features gives a clear story.\n\nSVM-RBF\n\nCompetitive or slightly better performance on some families, but the decision function is more opaque.\n\nWe rely on permutation importance as a post-hoc explanation: it highlights which inputs matter most, but does not provide per-sample attributions. This makes communication to non-experts somewhat harder than for RF.\n\nDeep Autoencoder\n\nExcellent at flagging rare or unseen patterns via reconstruction error.\n\nDirect interpretability is low; we primarily use reconstruction error histograms and anomaly-score distributions to understand how “far” a sample is from normal traffic. Feature-level attributions are deliberately not attempted here to keep the setup robust.\n\nClassical unsupervised models (DBSCAN, LOF, Isolation Forest)\n\nProvide intuitive geometric views (dense vs. sparse regions, anomaly score tails).\nHowever, binary outputs alone are not directly tied to specific feature semantics; they complement, but do not replace, supervised classifiers.\n\n\nOverall trade-off:\n- As we move from tree-based models → kernel methods → deep models, raw detection performance may improve, but so does the interpretability burden.\n- A practical IDS design will often favor a hybrid stack:\n- RF (or similar) as the main, explainable classifier with clear feature-importance stories.\n- Deep / unsupervised models as anomaly detectors with well-chosen thresholds and clear triage pathways."
  },
  {
    "objectID": "notebooks/Last/week_06_report_explain.html#real-world-ids-implications",
    "href": "notebooks/Last/week_06_report_explain.html#real-world-ids-implications",
    "title": "Week 06 — Final Analysis, Explainability & Reporting",
    "section": "Real-World IDS Implications",
    "text": "Real-World IDS Implications\n\nFalse-positive budget\n\nEven a small false-positive rate (e.g., 0.5%) can overwhelm analysts at scale.\n\nUse anomaly score distributions and calibration checks to set thresholds that keep FPs within an acceptable budget (e.g., N alerts per day).\n\nThreshold tuning\n\nFor anomaly detectors (Isolation Forest, LOF, Autoencoder, etc.), treat thresholds as policy knobs, not fixed constants.\n\nRevisit them regularly based on alert volumes and feedback from incident responders.\n\nModel maintenance & drift\n\nNetwork behavior evolves: new services, user patterns, and attack techniques appear.\n\nSchedule periodic retraining / revalidation (e.g., monthly or quarterly) and monitor simple drift indicators (feature distributions, reconstruction error shifts).\n\nRole specialization of models\n\nUse supervised models (RF/SVM) for labeled attack families and clear triage explanations based on feature importances.\n\nUse unsupervised / deep models as canaries to catch unusual behavior that does not match existing signatures.\n\nFeature-importance plots and PCA views help justify why a particular flow was flagged, supporting analyst trust and facilitating handoff to incident responders."
  },
  {
    "objectID": "notebooks/week_01_data_prep.html",
    "href": "notebooks/week_01_data_prep.html",
    "title": "Week 01 — Problem Definition & Data Preparation",
    "section": "",
    "text": "Objective. Load NSL-KDD corrected 10% file, encode categoricals, scale numerics, and persist split artifacts.\n\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n\n\n\n# Set up environment and constants. \nfrom pathlib import Path\nimport pandas as pd\nfrom src.utils import set_global_seed, Paths\nfrom src.io import load_raw_nsl_kdd, map_attack_family, save_numpy\nfrom src.prep import split_and_fit\nimport joblib\nset_global_seed(42)\npaths = Paths().ensure()\n\n\n# Load data from corrected file. \nraw_path = paths.data_raw / \"NSL-KDD.raw\"\nprint(raw_path)  # optional: confirm the resolved path\ndf = load_raw_nsl_kdd(raw_path)\ndf = map_attack_family(df)\ndisplay(df.head())\ndf.shape\n\nC:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n\n\n\n\n\n\n\nduration\nprotocol_type\nservice\nflag\nsrc_bytes\ndst_bytes\nland\nwrong_fragment\nurgent\nhot\n...\ndst_host_same_srv_rate\ndst_host_diff_srv_rate\ndst_host_same_src_port_rate\ndst_host_srv_diff_host_rate\ndst_host_serror_rate\ndst_host_srv_serror_rate\ndst_host_rerror_rate\ndst_host_srv_rerror_rate\nlabel\nfamily\n\n\n\n\n0\n0\ntcp\nhttp\nSF\n181\n5450\n0\n0\n0\n0\n...\n1.0\n0.0\n0.11\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n1\n0\ntcp\nhttp\nSF\n239\n486\n0\n0\n0\n0\n...\n1.0\n0.0\n0.05\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n2\n0\ntcp\nhttp\nSF\n235\n1337\n0\n0\n0\n0\n...\n1.0\n0.0\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n3\n0\ntcp\nhttp\nSF\n219\n1337\n0\n0\n0\n0\n...\n1.0\n0.0\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n4\n0\ntcp\nhttp\nSF\n217\n2032\n0\n0\n0\n0\n...\n1.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.0\nnormal\nnormal\n\n\n\n\n5 rows × 43 columns\n\n\n\n(494021, 43)\n\n\n\n# Check family distribution and Confirm class imbalance.\nfam_counts = df['family'].value_counts()\nfam_perc = (fam_counts / len(df)).round(4)\ndisplay(pd.DataFrame({'count': fam_counts, 'percent': fam_perc}))\n\n\n\n\n\n\n\n\ncount\npercent\n\n\nfamily\n\n\n\n\n\n\nDoS\n391458\n0.7924\n\n\nnormal\n97278\n0.1969\n\n\nProbe\n4107\n0.0083\n\n\nR2L\n1126\n0.0023\n\n\nU2R\n52\n0.0001\n\n\n\n\n\n\n\n\n# Split, encode, and scale. (Fit preprocessor on train only.)\nX_train, X_test, y_train, y_test, pre = split_and_fit(df, paths)\nX_train.shape, X_test.shape\n\n((395216, 115), (98805, 115))\n\n\n\n# Ensure y labels are integer-encoded (required for ML models)\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test  = le.transform(y_test)\n\nprint(\"Label classes:\", le.classes_)\n\nLabel classes: ['DoS' 'Probe' 'R2L' 'U2R' 'normal']\n\n\n\n# Persist artifacts. (Save arrays and preprocessor.)\nsave_numpy(paths.data_proc / 'X_train.npy', X_train)\nsave_numpy(paths.data_proc / 'X_test.npy', X_test)\nsave_numpy(paths.data_proc / 'y_train.npy', y_train)\nsave_numpy(paths.data_proc / 'y_test.npy', y_test)\njoblib.dump(pre, paths.data_proc / 'preprocessor.joblib')\nprint('Saved processed arrays and preprocessor to', paths.data_proc)\n\nSaved processed arrays and preprocessor to C:\\Users\\mehra\\Final_Project\\data\\processed\n\n\n\n# Brief summary. (Report shapes and missing checks.)\nsummary = {\n    'rows': len(df), 'cols': df.shape[1],\n    'X_train_shape': X_train.shape, 'X_test_shape': X_test.shape,\n    'y_train_counts': {k:int(v) for k,v in pd.Series(y_train).value_counts().items()},\n    'y_test_counts': {k:int(v) for k,v in pd.Series(y_test).value_counts().items()},\n    'missing_total': int(df.isna().sum().sum())\n}\nsummary\n\n{'rows': 494021,\n 'cols': 43,\n 'X_train_shape': (395216, 115),\n 'X_test_shape': (98805, 115),\n 'y_train_counts': {0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42},\n 'y_test_counts': {0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10},\n 'missing_total': 0}"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html",
    "href": "notebooks/week_03_stat_models.html",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "",
    "text": "Z-Score\nElliptic Envelope\nLocal Outlier Factor\nDBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os, sys\nsys.path.append(os.path.abspath(\"..\"))  # from notebooks/ to project root\n\nfrom src.utils import set_global_seed, Paths\n\nset_global_seed()\nprint(\"Import OK.\", Paths)\n\nImport OK. &lt;class 'src.utils.Paths'&gt;\n# Set up imports and load the raw NSL-KDD data. \nimport os, sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nfrom src.utils import Paths, set_global_seed\nfrom src.io import load_raw_nsl_kdd, map_attack_family\nfrom src.unsupervised import (\n    make_binary_subset, z_score_feature, z_score_labels,\n    run_elliptic_envelope, run_mahalanobis,lof_grid_search, run_dbscan\n)\n\npaths = Paths().ensure()\nset_global_seed(42)\n\n# Load the same raw file used in Week 01.\nraw_path = paths.data_raw / 'NSL-KDD.raw'\nprint('Loading raw data from:', raw_path)\ndf = load_raw_nsl_kdd(raw_path)\ndf = map_attack_family(df)\ndf.shape\n\nLoading raw data from: C:\\Users\\mehra\\Final_Project\\data\\raw\\NSL-KDD.raw\n\n\n(494021, 43)\n# Create a binary subset: normal vs teardrop. (Mirror Chapter_2.)\ndf_bin = make_binary_subset(df, normal_label='normal', attack_label='teardrop')\ndf_bin['Label'].value_counts()\n\nLabel\n0    97278\n1      979\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "href": "notebooks/week_03_stat_models.html#z-score-rule-on-a-single-feature",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Z-Score rule on a single feature",
    "text": "Z-Score rule on a single feature\nWe follow Chapter_2 and use the wrong_fragment feature with a threshold of |Z|&gt;2 to flag anomalies (attacks).\n\n# Compute Z-score for 'wrong_fragment' and map to labels using |Z| &gt; 2.\nz = z_score_feature(df_bin['wrong_fragment'])\ndf_bin['Z'] = z\ndf_bin['Z_Pred'] = z_score_labels(z, k=2.0)\n\ncm_z = confusion_matrix(df_bin['Label'], df_bin['Z_Pred'])\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_z, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('Z-Score rule on wrong_fragment (|Z|&gt;2)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_z\n\n\n\n\n\n\n\n\narray([[97278,     0],\n       [    0,   979]])"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "href": "notebooks/week_03_stat_models.html#elliptic-envelope-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Elliptic Envelope (Robust covariance)",
    "text": "Elliptic Envelope (Robust covariance)\nI use EllipticEnvelope with contamination=0.1 and random_state=0, dropping non-numeric / categorical columns\n\nell_res = run_elliptic_envelope(df_bin, contamination=0.1, random_state=0)\ncm_ell = ell_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_ell, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('EllipticEnvelope (contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_ell\n\n\n\n\n\n\n\n\narray([[87729,  9549],\n       [  702,   277]])"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "href": "notebooks/week_03_stat_models.html#mahalanobis-distance-with-robust-covariance",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Mahalanobis distance with robust covariance",
    "text": "Mahalanobis distance with robust covariance\nWe now fit a robust covariance model (MinCovDet) on the same numeric feature space, compute squared Mahalanobis distances, and threshold them using the chi-square quantile at alpha = 0.99. Points with distance² above the threshold are treated as anomalies.\n\n# Mahalanobis distance model. (Robust covariance + chi-square threshold.)\nfrom src.unsupervised import run_mahalanobis\n\nmahal_res = run_mahalanobis(df_bin, alpha=0.99, robust=True)\n\ncm_mahal = mahal_res.confusion\nprint(\"Chi-square threshold (alpha=0.99):\", mahal_res.threshold)\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_mahal, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Mahalanobis distance (robust covariance, α=0.99)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\ncm_mahal\n\nChi-square threshold (alpha=0.99): 63.690739751564465\n\n\n\n\n\n\n\n\n\narray([[53378, 43900],\n       [    0,   979]])"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html#local-outlier-factor-lof",
    "href": "notebooks/week_03_stat_models.html#local-outlier-factor-lof",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\nFirst, I run a single LOF model with a modest neighborhood size; then, I grid search over k to inspect accuracy, precision, and recall as a function of the neighborhood size.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\nactual = df_bin['Label'].to_numpy()\ndrop_cols = [c for c in ['Label', 'label', 'target', 'protocol_type', 'service', 'flag', 'family'] if c in df_bin.columns]\nX = df_bin.drop(columns=drop_cols)\n\nk0 = 5\nlof0 = LocalOutlierFactor(n_neighbors=k0, contamination=0.1)\npred0 = lof0.fit_predict(X)\npred0_rescored = np.where(pred0 == -1, 1, 0)\ncm_lof0 = confusion_matrix(actual, pred0_rescored)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_lof0, annot=True, fmt='d', cmap='YlGnBu')\nplt.title(f'LOF (k={k0}, contamination=0.1)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_lof0\n\n\n\n\n\n\n\n\narray([[87465,  9813],\n       [  966,    13]])\n\n\n\n# Grid search over k for LOF\nk_values = list(range(100, 1501, 100))  # 100, 200, ..., 3000\nlof_grid = lof_grid_search(df_bin, k_values)\n\nplt.figure(figsize=(8,5))\nplt.plot(lof_grid.ks, lof_grid.accuracies, label='Accuracy')\nplt.plot(lof_grid.ks, lof_grid.precisions, label='Precision')\nplt.plot(lof_grid.ks, lof_grid.recalls, label='Recall')\nplt.xlabel('k (n_neighbors)')\nplt.ylabel('Metric (%)')\nplt.title('LOF metrics vs k')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "notebooks/week_03_stat_models.html#dbscan",
    "href": "notebooks/week_03_stat_models.html#dbscan",
    "title": "Week 03 — Statistical / Classical Anomaly Detection",
    "section": "DBSCAN",
    "text": "DBSCAN\nI run DBSCAN with eps=0.2 and min_samples=5 on the same feature space, rescoring cluster labels so that noise points are treated as anomalies.\n\ndb_res = run_dbscan(df_bin, eps=0.2, min_samples=5)\ncm_db = db_res.confusion\nplt.figure(figsize=(6,5))\nsns.heatmap(cm_db, annot=True, fmt='d', cmap='YlGnBu')\nplt.title('DBSCAN (eps=0.2, min_samples=5)')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\ncm_db\n\n\n\n\n\n\n\n\narray([[11475, 85803],\n       [    0,   979]])\n\n\n\n# Save anomaly detection results for Week 05\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom src.utils import Paths\n\npaths = Paths().ensure()\n\n# Determine project root and reports directory\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nreports_root = getattr(paths, \"reports\", project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk3_dir.mkdir(parents=True, exist_ok=True)\n\nframes = []\n\n# 1) Z-SCORE MODEL \nif {\"Label\", \"Z\", \"Z_Pred\"}.issubset(df_bin.columns):\n    z_df = pd.DataFrame({\n        \"model\":      \"Z-Score (wrong_fragment, |Z|&gt;2)\",\n        \"y_true_bin\": df_bin[\"Label\"].astype(int).to_numpy(),\n        \"y_pred_bin\": df_bin[\"Z_Pred\"].astype(int).to_numpy(),\n        # Score: |Z| (higher = more anomalous)\n        \"score\":      np.abs(df_bin[\"Z\"]).to_numpy(),\n    })\n    frames.append(z_df)\n    print(\"Added Z-Score results.\")\nelse:\n    print(\"Z-score fields not found. Skipping Z-score model.\")\n\n\n# Helper function to add result objects\ndef add_res_object(name):\n    obj = globals().get(name, None)\n    if obj is None:\n        print(f\"{name} not found, skipping.\")\n        return\n    \n    needed = [\"name\", \"y_true\", \"y_pred\", \"scores\"]\n    missing = [a for a in needed if not hasattr(obj, a)]\n    if missing:\n        print(f\"{name} missing attributes {missing}, skipping.\")\n        return\n    \n    df = pd.DataFrame({\n        \"model\":      obj.name,\n        \"y_true_bin\": np.array(obj.y_true).astype(int),\n        \"y_pred_bin\": np.array(obj.y_pred).astype(int),\n        \"score\":      np.array(obj.scores).astype(float),\n    })\n    frames.append(df)\n    print(f\"Added results for {obj.name}.\")\n\n\n# Elliptic Envelope\n# Mahalanobis\n# LOF\n# DBSCAN\nadd_res_object(\"ell_res\")\nadd_res_object(\"mahal_res\")\nadd_res_object(\"lof0\")\nadd_res_object(\"db_res\")\n\n\n# Save all results\nif not frames:\n    raise ValueError(\"No Week 03 results found. Run the model cells first.\")\n\nunsup_df = pd.concat(frames, ignore_index=True)\n\ncsv_path     = wk3_dir / \"week03_unsupervised_predictions.csv\"\n\nunsup_df.to_csv(csv_path, index=False)\n\n\nprint(\"\\nSaved Week 03 anomaly detection results:\")\nprint(\" -\", csv_path)\n\nunsup_df.head()\n\nAdded Z-Score results.\nell_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nmahal_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\nlof0 missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\ndb_res missing attributes ['name', 'y_true', 'y_pred', 'scores'], skipping.\n\nSaved Week 03 anomaly detection results:\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.csv\n - C:\\Users\\mehra\\Final_Project\\reports\\week_03\\week03_unsupervised_predictions.parquet\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html",
    "href": "notebooks/week_05_model_evaluation.html",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "",
    "text": "# Imports, project root, and paths.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os, sys, time\nfrom pathlib import Path\n\nthis_dir = Path.cwd()\nproject_root = this_dir.parent if this_dir.name == \"notebooks\" else this_dir\nsys.path.append(str(project_root))\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Sklearn metrics & helpers\nfrom sklearn.metrics import (\n    confusion_matrix,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    average_precision_score,\n)\nfrom sklearn.calibration import CalibrationDisplay\n\n# Project utilities\nfrom src.utils import Paths, set_global_seed\n\n# Optional: helper for binary label mapping (used in earlier weeks)\ntry:\n    from src.eval import to_binary  # used in earlier weeks\nexcept ImportError:\n    to_binary = None\n\nset_global_seed(42)\npaths = Paths().ensure()\n\nprint(\"Project root:\", project_root)\nprint(\"Using paths:\", paths)\n\nProject root: C:\\Users\\mehra\\Final_Project\nUsing paths: Paths(root=WindowsPath('C:/Users/mehra/Final_Project'), data_raw=WindowsPath('C:/Users/mehra/Final_Project/data/raw'), data_proc=WindowsPath('C:/Users/mehra/Final_Project/data/processed'), figs=WindowsPath('C:/Users/mehra/Final_Project/notebooks/figures'), artifacts=WindowsPath('C:/Users/mehra/Final_Project/notebooks/artifacts'))\nfrom pathlib import Path as _Path\n\n_this_dir = _Path.cwd()\n_project_root = _this_dir.parent if _this_dir.name == \"notebooks\" else _this_dir\n\n# Try to use paths.reports if it exists; otherwise fallback to &lt;project_root&gt;/reports\nreports_root = getattr(paths, \"reports\", _project_root / \"reports\")\n\nwk3_dir = reports_root / \"week_03\"\nwk4_dir = reports_root / \"week_04\"\n\nwk3_dir.mkdir(parents=True, exist_ok=True)\nwk4_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Reports root:\", reports_root)\nprint(\"Week 03 reports folder:\", wk3_dir)\nprint(\"Week 04 reports folder:\", wk4_dir)\n\nReports root: C:\\Users\\mehra\\Final_Project\\reports\nWeek 03 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_03\nWeek 04 reports folder: C:\\Users\\mehra\\Final_Project\\reports\\week_04\n# Load processed train/test arrays from Week 01 \nX_train_path = paths.data_proc / \"X_train.npy\"\nX_test_path  = paths.data_proc / \"X_test.npy\"\ny_train_path = paths.data_proc / \"y_train.npy\"\ny_test_path  = paths.data_proc / \"y_test.npy\"\n\nX_train = np.load(X_train_path)\nX_test  = np.load(X_test_path)\ny_train = np.load(y_train_path)\ny_test  = np.load(y_test_path)\n\nprint(\"X_train:\", X_train.shape, \"X_test:\", X_test.shape)\nprint(\"y_train classes:\", Counter(y_train))\nprint(\"y_test  classes:\", Counter(y_test))\n\n# Optional binary labels (normal vs attack) if a helper exists.\nif to_binary is not None:\n    normal_label = \"normal\"  # adjust if your label encoding is different\n    y_train_bin = to_binary(y_train, normal_label)\n    y_test_bin  = to_binary(y_test, normal_label)\n    print(\"Binary train counts:\", Counter(y_train_bin))\n    print(\"Binary test  counts:\", Counter(y_test_bin))\nelse:\n    y_train_bin = None\n    y_test_bin = None\n    print(\"NOTE: src.eval.to_binary not found – binary labels not created automatically.\")\n\nX_train: (395216, 115) X_test: (98805, 115)\ny_train classes: Counter({0: 313166, 4: 77822, 1: 3285, 2: 901, 3: 42})\ny_test  classes: Counter({0: 78292, 4: 19456, 1: 822, 2: 225, 3: 10})\nNOTE: src.eval.to_binary not found – binary labels not created automatically.\n# Helper functions for metrics and plotting.\n\ndef multiclass_metrics(y_true, y_pred, y_proba=None, labels=None, average=\"macro\"):\n    \"\"\"Compute macro + per-class metrics for a multiclass setting.\n\n    Parameters\n    ----------\n    y_true, y_pred : array-like\n        True and predicted class labels.\n    y_proba : array-like of shape (n_samples, n_classes), optional\n        Predicted probabilities for each class (for ROC/PR).\n    labels : list, optional\n        Label order. If None, inferred from y_true.\n    average : str\n        Averaging scheme for global metrics.\n    \"\"\"\n    if labels is None:\n        labels = np.unique(y_true)\n    \n    prec, rec, f1, support = precision_recall_fscore_support(\n        y_true, y_pred, labels=labels, average=None, zero_division=0\n    )\n    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=average, zero_division=0\n    )\n    \n    per_class_df = pd.DataFrame({\n        \"family\": labels,\n        \"precision\": prec,\n        \"recall\": rec,\n        \"f1\": f1,\n        \"support\": support,\n    })\n    \n    metrics = {\n        \"precision_macro\": macro_prec,\n        \"recall_macro\": macro_rec,\n        \"f1_macro\": macro_f1,\n    }\n    \n    # ROC-AUC (OvR) and PR-AUC\n    if y_proba is not None:\n        # Build one-hot encoding from labels.\n        label_to_idx = {lab: i for i, lab in enumerate(labels)}\n        y_true_idx = np.array([label_to_idx[lab] for lab in y_true])\n        Y_true_ovr = np.eye(len(labels))[y_true_idx]\n        \n        try:\n            roc_macro_ovr = roc_auc_score(Y_true_ovr, y_proba, average=\"macro\", multi_class=\"ovr\")\n        except Exception:\n            roc_macro_ovr = np.nan\n        \n        try:\n            pr_macro_ovr = average_precision_score(Y_true_ovr, y_proba, average=\"macro\")\n        except Exception:\n            pr_macro_ovr = np.nan\n        \n        metrics[\"roc_auc_ovr_macro\"] = roc_macro_ovr\n        metrics[\"pr_auc_ovr_macro\"] = pr_macro_ovr\n    else:\n        metrics[\"roc_auc_ovr_macro\"] = np.nan\n        metrics[\"pr_auc_ovr_macro\"] = np.nan\n    \n    return metrics, per_class_df\n\n\ndef binary_threshold_metrics(y_true_bin, scores, thresholds):\n    \"\"\"Sweep thresholds over anomaly scores for binary detectors.\n\n    Assumes higher `scores` = more anomalous (1 = attack, 0 = normal).\n    Returns a DataFrame with precision/recall/F1 for each threshold.\n    \"\"\"\n    rows = []\n    for thr in thresholds:\n        y_pred_bin = (scores &gt;= thr).astype(int)\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n        )\n        rows.append({\n            \"threshold\": thr,\n            \"precision\": prec,\n            \"recall\": rec,\n            \"f1\": f1,\n        })\n    return pd.DataFrame(rows)\n\n\ndef plot_confusion(cm, labels, title):\n    fig, ax = plt.subplots(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"YlGnBu\",\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual\")\n    ax.set_title(title)\n    plt.tight_layout()\n    return fig, ax"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#load-week-04-supervised-results",
    "href": "notebooks/week_05_model_evaluation.html#load-week-04-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 supervised results",
    "text": "Load Week 04 supervised results\n\n# Load Week 04 supervised predictions.\npred_parquet = wk4_dir / \"week04_supervised_predictions.parquet\"\npred_csv     = wk4_dir / \"week04_supervised_predictions.csv\"\n\nif pred_parquet.exists():\n    sup_pred_df = pd.read_parquet(pred_parquet)\nelif pred_csv.exists():\n    sup_pred_df = pd.read_csv(pred_csv)\nelse:\n    raise FileNotFoundError(\n        f\"Could not find Week 04 prediction file. Expected one of:\\n{pred_parquet}\\n{pred_csv}\\n\"\n        \"Please save your Week 04 test predictions to one of these paths.\"\n    )\n\nprint(\"Loaded Week 04 supervised predictions:\")\ndisplay(sup_pred_df.head())\n\n# Identify probability columns and families from colnames (proba_&lt;family&gt;).\nproba_cols = [c for c in sup_pred_df.columns if c.startswith(\"proba_\")]\nfamilies_from_proba = [c.replace(\"proba_\", \"\") for c in proba_cols]\n\nLoaded Week 04 supervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true\ny_pred\n\n\n\n\n0\nRandom Forest (baseline)\n0\n0\n\n\n1\nRandom Forest (baseline)\n4\n4\n\n\n2\nRandom Forest (baseline)\n0\n0\n\n\n3\nRandom Forest (baseline)\n4\n4\n\n\n4\nRandom Forest (baseline)\n0\n0"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "href": "notebooks/week_05_model_evaluation.html#load-week-03-unsupervised-statistical-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 03 unsupervised / statistical results",
    "text": "Load Week 03 unsupervised / statistical results\n\n# Load Week 03 unsupervised predictions\nunsup_parquet = wk3_dir / \"week03_unsupervised_predictions.parquet\"\nunsup_csv     = wk3_dir / \"week03_unsupervised_predictions.csv\"\n\nunsup_df = None\nif unsup_parquet.exists():\n    unsup_df = pd.read_parquet(unsup_parquet)\nelif unsup_csv.exists():\n    unsup_df = pd.read_csv(unsup_csv)\n\nif unsup_df is not None:\n    print(\"Loaded Week 03 unsupervised predictions:\")\n    display(unsup_df.head())\nelse:\n    print(\"Week 03 unsupervised prediction file not found — \"\n          \"unsupervised models will be skipped unless you add it.\")\n\nLoaded Week 03 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n1\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n2\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n3\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\n0\n0\n0.100112"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "href": "notebooks/week_05_model_evaluation.html#load-week-04-unsupervised-semi-supervised-results",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Load Week 04 unsupervised / semi-supervised results",
    "text": "Load Week 04 unsupervised / semi-supervised results\n\n# Load Week 04 unsupervised predictions\nunsup4_parquet = wk4_dir / \"week04_unsupervised_predictions.parquet\"\nunsup4_csv     = wk4_dir / \"week04_unsupervised_predictions.csv\"\n\nunsup4_df = None\nif unsup4_parquet.exists():\n    unsup4_df = pd.read_parquet(unsup4_parquet)\nelif unsup4_csv.exists():\n    unsup4_df = pd.read_csv(unsup4_csv)\n\nif unsup4_df is not None:\n    print(\"Loaded Week 04 unsupervised predictions:\")\n    display(unsup4_df.head())\nelse:\n    print(\"Week 04 unsupervised prediction file not found — \"\n          \"these models will be skipped unless you add it.\")\n\nLoaded Week 04 unsupervised predictions:\n\n\n\n\n\n\n\n\n\nmodel\ny_true_bin\ny_pred_bin\nscore\n\n\n\n\n0\nIsolation Forest\n0\n0\n0.0\n\n\n1\nIsolation Forest\n1\n1\n1.0\n\n\n2\nIsolation Forest\n0\n0\n0.0\n\n\n3\nIsolation Forest\n1\n1\n1.0\n\n\n4\nIsolation Forest\n0\n0\n0.0"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#metric-table",
    "href": "notebooks/week_05_model_evaluation.html#metric-table",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Metric Table",
    "text": "Metric Table\n\nPrecision, Recall, F1 (macro)\nROC-AUC (OvR) and PR-AUC (macro) where probabilities/scores are available\n\nSupervised models are treated as multiclass, while unsupervised models are evaluated in the binary attack vs normal setting.\n\n# Build metric table from saved Week 03 / Week 04 outputs.\n\nrows = []\nper_class_by_model = {}  # store per-family metrics for later deep-dive\n\n# Supervised (multiclass)\nfor model_name, g in sup_pred_df.groupby(\"model\"):\n    y_true = g[\"y_true\"].to_numpy()\n    y_pred = g[\"y_pred\"].to_numpy()\n\n    # Use proba_* columns if present\n    if proba_cols:\n        y_proba = g[proba_cols].to_numpy()\n        labels_order = families_from_proba\n    else:\n        y_proba = None\n        labels_order = np.unique(y_true)\n\n    metrics, per_class = multiclass_metrics(\n        y_true,\n        y_pred,\n        y_proba=y_proba,\n        labels=labels_order,\n    )\n    per_class[\"model\"] = model_name\n    per_class_by_model[model_name] = per_class\n\n    rows.append({\n        \"model\": model_name,\n        \"type\": \"supervised\",\n        **metrics,\n    })\n\n# Unsupervised (binary) from Week 03 + Week 04 \ncombined_unsup = None\nif 'unsup_df' in globals() and unsup_df is not None:\n    combined_unsup = unsup_df.copy()\nif 'unsup4_df' in globals() and unsup4_df is not None:\n    if combined_unsup is None:\n        combined_unsup = unsup4_df.copy()\n    else:\n        combined_unsup = pd.concat([combined_unsup, unsup4_df], ignore_index=True)\n\nif combined_unsup is not None:\n    for model_name, g in combined_unsup.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        y_pred_b = g[\"y_pred_bin\"].to_numpy()\n\n        prec, rec, f1, _ = precision_recall_fscore_support(\n            y_true_b, y_pred_b, average=\"binary\", zero_division=0\n        )\n\n        scores = g[\"score\"].to_numpy() if \"score\" in g.columns else None\n        if scores is not None:\n            try:\n                roc_auc = roc_auc_score(y_true_b, scores)\n            except Exception:\n                roc_auc = np.nan\n            try:\n                pr_auc = average_precision_score(y_true_b, scores)\n            except Exception:\n                pr_auc = np.nan\n        else:\n            roc_auc = np.nan\n            pr_auc = np.nan\n\n        rows.append({\n            \"model\": model_name,\n            \"type\": \"unsupervised\",\n            \"precision_macro\": prec,\n            \"recall_macro\": rec,\n            \"f1_macro\": f1,\n            \"roc_auc_ovr_macro\": roc_auc,\n            \"pr_auc_ovr_macro\": pr_auc,\n        })\n\nif rows:\n    metric_table = pd.DataFrame(rows).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\nelse:\n    metric_table = pd.DataFrame(columns=[\n        \"model\", \"type\",\n        \"precision_macro\", \"recall_macro\", \"f1_macro\",\n        \"roc_auc_ovr_macro\", \"pr_auc_ovr_macro\"\n    ])\n\nmetric_table\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n1\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n2\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n3\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n4\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n5\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "href": "notebooks/week_05_model_evaluation.html#family-wise-deep-dive-focus-on-r2l-u2r",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Family-wise deep-dive (focus on R2L / U2R)",
    "text": "Family-wise deep-dive (focus on R2L / U2R)\n\nSelect the best supervised model by macro F1.\n\nInspect per-family precision/recall/F1.\n\nPlot its confusion matrix on the test set.\n\n\n# Select best supervised model by macro F1.\nsup_only = metric_table[metric_table[\"type\"] == \"supervised\"]\nbest_name = sup_only.sort_values(\"f1_macro\", ascending=False)[\"model\"].iloc[0]\nprint(\"Best supervised model by macro F1:\", best_name)\n\nbest_block = sup_pred_df[sup_pred_df[\"model\"] == best_name].reset_index(drop=True)\ny_true_best = best_block[\"y_true\"].to_numpy()\ny_pred_best = best_block[\"y_pred\"].to_numpy()\n\n# Recompute per-family metrics for the best model \nif best_name in per_class_by_model:\n    per_class_best = per_class_by_model[best_name]\nelse:\n    labels = np.unique(y_true_best)\n    metrics_best, per_class_best = multiclass_metrics(y_true_best, y_pred_best, labels=labels)\n\ndisplay(per_class_best.sort_values(\"f1\"))\n\nlabels_cm = np.unique(y_true_best)\ncm_best = confusion_matrix(y_true_best, y_pred_best, labels=labels_cm)\nfig, ax = plot_confusion(cm_best, labels_cm, f\"Confusion matrix – {best_name}\")\nplt.show()\n\nBest supervised model by macro F1: Random Forest (baseline)\n\n\n\n\n\n\n\n\n\nfamily\nprecision\nrecall\nf1\nsupport\nmodel\n\n\n\n\n3\n3\n0.818182\n0.900000\n0.857143\n10\nRandom Forest (baseline)\n\n\n2\n2\n0.995475\n0.977778\n0.986547\n225\nRandom Forest (baseline)\n\n\n1\n1\n0.998778\n0.993917\n0.996341\n822\nRandom Forest (baseline)\n\n\n4\n4\n0.999383\n0.999794\n0.999589\n19456\nRandom Forest (baseline)\n\n\n0\n0\n0.999987\n0.999974\n0.999981\n78292\nRandom Forest (baseline)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2L / U2R error analysis with examples\nUsing the best supervised model, we inspect misclassified R2L and U2R examples and show a few raw feature rows as concrete cases.\n\n# R2L and U2R error analysis.\n\nr2l_label = \"R2L\"  # adjust if needed\nu2r_label = \"U2R\"  # adjust if needed\n\nmask_r2l = (y_true_best == r2l_label)\nmask_u2r = (y_true_best == u2r_label)\n\nprint(\"R2L support:\", mask_r2l.sum())\nprint(\"U2R support:\", mask_u2r.sum())\n\nmis_r2l_idx = np.where(mask_r2l & (y_pred_best != r2l_label))[0]\nmis_u2r_idx = np.where(mask_u2r & (y_pred_best != u2r_label))[0]\n\nprint(\"Misclassified R2L:\", len(mis_r2l_idx))\nprint(\"Misclassified U2R:\", len(mis_u2r_idx))\n\n# Build a feature DataFrame for X_test \ntry:\n    import joblib\n    pre_path = paths.data_proc / \"preprocessor.joblib\"\n    pre = joblib.load(pre_path)\n    feature_names = pre.get_feature_names_out()\n    X_test_df = pd.DataFrame(X_test, columns=feature_names)\nexcept Exception as e:\n    print(\"Could not load preprocessor or feature names:\", e)\n    X_test_df = pd.DataFrame(X_test)\n    X_test_df[\"family\"] = y_test\n\ndef show_examples(indices, family_name, n=5):\n    if len(indices) == 0:\n        print(f\"No misclassifications found for {family_name}.\")\n        return\n    idx_sel = indices[:n]\n    ex = X_test_df.iloc[idx_sel].copy()\n    ex[\"y_true\"] = y_true_best[idx_sel]\n    ex[\"y_pred\"] = y_pred_best[idx_sel]\n    display(ex)\n\nprint(\"\\nExample misclassified R2L samples:\")\nshow_examples(mis_r2l_idx, \"R2L\")\n\nprint(\"\\nExample misclassified U2R samples:\")\nshow_examples(mis_u2r_idx, \"U2R\")\n\nR2L support: 0\nU2R support: 0\nMisclassified R2L: 0\nMisclassified U2R: 0\n\nExample misclassified R2L samples:\nNo misclassifications found for R2L.\n\nExample misclassified U2R samples:\nNo misclassifications found for U2R."
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#robustness-checks",
    "href": "notebooks/week_05_model_evaluation.html#robustness-checks",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Robustness checks",
    "text": "Robustness checks\n\n1) Threshold sensitivity for unsupervised models\nSweep a range of thresholds and inspect how binary F1 changes for each detector.\n\n# Threshold sensitivity for unsupervised detectors.\n\nif (unsup_df is not None) and (\"score\" in unsup_df.columns):\n    thresholds = np.linspace(0.1, 0.9, 9)  # generic [0.1, 0.9]; adjust if needed\n    \n    for model_name, g in unsup_df.groupby(\"model\"):\n        y_true_b = g[\"y_true_bin\"].to_numpy()\n        scores = g[\"score\"].to_numpy()\n        \n        # Normalize scores into [0, 1] for a stable threshold range.\n        s_min, s_max = scores.min(), scores.max()\n        if s_max &gt; s_min:\n            scores_norm = (scores - s_min) / (s_max - s_min)\n        else:\n            scores_norm = scores\n        \n        df_thr = binary_threshold_metrics(y_true_b, scores_norm, thresholds)\n        df_thr[\"model\"] = model_name\n        display(df_thr)\n        \n        fig, ax = plt.subplots()\n        ax.plot(df_thr[\"threshold\"], df_thr[\"f1\"], marker=\"o\")\n        ax.set_xlabel(\"Threshold (on normalized score)\")\n        ax.set_ylabel(\"F1 (binary)\")\n        ax.set_title(f\"Threshold sensitivity – {model_name}\")\n        plt.tight_layout()\n        plt.show()\nelse:\n    print(\"No unsupervised scores available – threshold robustness skipped.\")\n\n\n\n\n\n\n\n\nthreshold\nprecision\nrecall\nf1\nmodel\n\n\n\n\n0\n0.1\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n1\n0.2\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n2\n0.3\n1.0\n1.000000\n1.000000\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n3\n0.4\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n4\n0.5\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n5\n0.6\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n6\n0.7\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n7\n0.8\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)\n\n\n8\n0.9\n1.0\n0.990807\n0.995382\nZ-Score (wrong_fragment, |Z|&gt;2)"
  },
  {
    "objectID": "notebooks/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "href": "notebooks/week_05_model_evaluation.html#final-summary-ranked-shortlist",
    "title": "Week 05 — Model Evaluation & Comparison",
    "section": "Final summary & ranked shortlist",
    "text": "Final summary & ranked shortlist\nWe now combine:\n\nGlobal metrics (F1, ROC-AUC, PR-AUC) for all methods into a single summary table and then draft a ranked shortlist with trade-offs.\n\n\n# Merge metric table only (runtime columns already included in metric_table)\n\nsummary = metric_table.copy()\n\nsummary_sorted = summary.sort_values(\n    [\"type\", \"f1_macro\"],\n    ascending=[True, False]\n).reset_index(drop=True)\n\nsummary_sorted\n\n\n\n\n\n\n\n\nmodel\ntype\nprecision_macro\nrecall_macro\nf1_macro\nroc_auc_ovr_macro\npr_auc_ovr_macro\n\n\n\n\n0\nRandom Forest (baseline)\nsupervised\n0.962361\n0.974293\n0.967920\nNaN\nNaN\n\n\n1\nRandom Forest + SMOTE\nsupervised\n0.947867\n0.975182\n0.960155\nNaN\nNaN\n\n\n2\nSVM-RBF + SMOTE\nsupervised\n0.890610\n0.930098\n0.905367\nNaN\nNaN\n\n\n3\nSVM-RBF (baseline)\nsupervised\n0.642394\n0.722858\n0.501165\nNaN\nNaN\n\n\n4\nZ-Score (wrong_fragment, |Z|&gt;2)\nunsupervised\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n5\nOC-SVM (RBF)\nunsupervised\n0.779636\n1.000000\n0.876175\n0.962972\n0.779636\n\n\n6\nIsolation Forest\nunsupervised\n0.769121\n0.369132\n0.498847\n0.670050\n0.414882\n\n\n7\nDeep Autoencoder\nunsupervised\n0.906161\n0.046605\n0.088650\n0.981710\n0.878009\n\n\n\n\n\n\n\n\n# Ranked shortlist narrative based on `summary_sorted`.\n\nimport pandas as pd\n\ndef _fmt(x):\n    if pd.isna(x):\n        return \"N/A\"\n    try:\n        return f\"{x:.3f}\"\n    except Exception:\n        return str(x)\n\nlines = []\nlines.append(\"### Ranked shortlist (auto-generated)\\n\")\n\n# Top supervised models\nif 'summary_sorted' not in globals():\n    print(\"summary_sorted is not defined. Run the previous cells first.\")\nelse:\n    sup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"supervised\"] if \"type\" in summary_sorted.columns else summary_sorted.copy()\n    unsup = summary_sorted[summary_sorted.get(\"type\", \"\") == \"unsupervised\"] if \"type\" in summary_sorted.columns else summary_sorted.iloc[0:0]\n\n    if not sup.empty:\n        lines.append(\"**Supervised models (multiclass)**\\n\")\n        top_sup = sup.sort_values(\"f1_macro\", ascending=False).head(3)\n\n        for rank, (_, row) in enumerate(top_sup.iterrows(), start=1):\n            name = row[\"model\"]\n            f1m = _fmt(row.get(\"f1_macro\"))\n            prec = _fmt(row.get(\"precision_macro\"))\n            rec = _fmt(row.get(\"recall_macro\"))\n            roc = _fmt(row.get(\"roc_auc_ovr_macro\"))\n            prc = _fmt(row.get(\"pr_auc_ovr_macro\"))\n            ft = _fmt(row.get(\"fit_time_sec\")) if \"fit_time_sec\" in row else \"N/A\"\n            pt = _fmt(row.get(\"predict_time_sec\")) if \"predict_time_sec\" in row else \"N/A\"\n\n            lines.append(\n                f\"{rank}. **{name}** — macro F1 = {f1m}, \"\n                f\"precision = {prec}, recall = {rec}, \"\n                f\"ROC-AUC (OvR) = {roc}, PR-AUC = {prc}; \"\n                f\"fit time ≈ {ft}s, predict time ≈ {pt}s.\"\n            )\n\n        lines.append(\"\")\n\n    # Best unsupervised / semi-supervised model\n    if not unsup.empty:\n        best_unsup = unsup.sort_values(\"f1_macro\", ascending=False).iloc[0]\n        name = best_unsup[\"model\"]\n        f1m = _fmt(best_unsup.get(\"f1_macro\"))\n        prec = _fmt(best_unsup.get(\"precision_macro\"))\n        rec = _fmt(best_unsup.get(\"recall_macro\"))\n        roc = _fmt(best_unsup.get(\"roc_auc_ovr_macro\"))\n        prc = _fmt(best_unsup.get(\"pr_auc_ovr_macro\"))\n\n        lines.append(\"**Unsupervised / semi-supervised models (binary anomaly detection)**\\n\")\n        lines.append(\n            f\"- **{name}** — best binary macro F1 = {f1m}, \"\n            f\"precision = {prec}, recall = {rec}, \"\n            f\"ROC-AUC = {roc}, PR-AUC = {prc}. \"\n            \"Use as an anomaly tripwire to complement the supervised family classifier.\"\n        )\n    else:\n        lines.append(\"_No unsupervised results found in `summary_sorted`._\")\n\n    # Print the narrative as markdown-style text\n    print(\"\\n\".join(lines))\n\n### Ranked shortlist (auto-generated)\n\n**Supervised models (multiclass)**\n\n1. **Random Forest (baseline)** — macro F1 = 0.968, precision = 0.962, recall = 0.974, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n2. **Random Forest + SMOTE** — macro F1 = 0.960, precision = 0.948, recall = 0.975, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n3. **SVM-RBF + SMOTE** — macro F1 = 0.905, precision = 0.891, recall = 0.930, ROC-AUC (OvR) = N/A, PR-AUC = N/A; fit time ≈ N/As, predict time ≈ N/As.\n\n**Unsupervised / semi-supervised models (binary anomaly detection)**\n\n- **Z-Score (wrong_fragment, |Z|&gt;2)** — best binary macro F1 = 1.000, precision = 1.000, recall = 1.000, ROC-AUC = 1.000, PR-AUC = 1.000. Use as an anomaly tripwire to complement the supervised family classifier."
  },
  {
    "objectID": "presentation.html#slide-1-introduction-motivation",
    "href": "presentation.html#slide-1-introduction-motivation",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Slide 1 — Introduction & Motivation",
    "text": "Slide 1 — Introduction & Motivation\n\nRising network complexity\nExpanding attack surface\nNeed for adaptive detection\nLimits of signature systems\nValue of behavioral analytics\nGoal: improved threat identification\n\n\nExecutives face rising operational risks due to evolving traffic patterns. Signature systems miss new and subtle attacks. This project evaluates anomaly-based and learning-based detection to strengthen SOC capabilities."
  },
  {
    "objectID": "presentation.html#introduction-motivation",
    "href": "presentation.html#introduction-motivation",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Introduction & Motivation",
    "text": "Introduction & Motivation\n\nRising network complexity\nExpanding attack surface\nNeed for adaptive detection\nLimits of signature systems\nValue of behavioral analytics\nGoal: improved threat identification\n\n\n\nThis project is motivated by the growing complexity of modern networks and the expanding attack surface that organizations face. Traditional signature-based detection struggles to keep pace with evolving and previously unseen threats. By shifting toward adaptive, behavior-based analytics, we can identify abnormal activity earlier and more accurately. The overall objective is to strengthen threat detection capabilities and improve security decision-making at scale."
  },
  {
    "objectID": "presentation.html#slide-2-problem-statement",
    "href": "presentation.html#slide-2-problem-statement",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Slide 2 — Problem Statement",
    "text": "Slide 2 — Problem Statement\n\nQ1: Effectiveness of unsupervised detectors\nQ2: Supervised vs. statistical performance\nFocus: accuracy, precision, recall\nAssess interpretability differences\nEvaluate malicious traffic detection\nIdentify operational trade-offs\n\n\n\n\n\n\n\nNote\n\n\nWe test whether unsupervised models trained on only normal traffic can detect attacks, and how supervised ML compares to statistical anomaly methods across key performance dimensions."
  },
  {
    "objectID": "presentation.html#problem-statement",
    "href": "presentation.html#problem-statement",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nIdentify limits of current anomaly detection approaches\nAssess ability to detect malicious network behavior\nCompare unsupervised, statistical, and supervised methods\nEvaluate differences in detection effectiveness\nUnderstand variation in precision, recall, and false alerts\nDetermine most reliable model for operational deployment\n\n\n\nThe goal is to understand how different detection methods perform when identifying malicious activity in complex network environments. We evaluate traditional statistical models, modern unsupervised anomaly detectors, and supervised machine-learning approaches to determine their strengths and limitations. The focus is on how accurately these methods detect threats, how often they generate false alerts, and how interpretable their outputs are for analysts. Ultimately, the aim is to determine which approach provides the most reliable and operationally effective intrusion detection capability."
  },
  {
    "objectID": "presentation.html#background",
    "href": "presentation.html#background",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Background",
    "text": "Background\n\nExpanding cyberattack landscape\nImportance of anomaly detection\nMix of statistical and ML tools**\nChallenge of imbalanced attacks\nNeed for explainability\nRelevance to modern SOCs\n\n\n\nCyberattacks continue to grow in scale and sophistication, making traditional rule-based detection insufficient. Anomaly detection is increasingly important because it can surface unusual or emerging behaviors that signatures fail to capture. Our evaluation spans both statistical techniques and machine-learning models, each offering different strengths. A major challenge in this domain is the imbalance between normal traffic and rare attack events, which can distort model performance. Explainability is also essential so analysts can trust and validate detection outputs. These considerations directly impact how Security Operations Centers operate and prioritize alerts."
  },
  {
    "objectID": "presentation.html#dataset-overview",
    "href": "presentation.html#dataset-overview",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\nNSL-KDD standard dataset\n41 engineered network features\nBalanced training/test sets\nMultiple attack categories\nSupports binary and multi-class tasks\nWidely used for IDS research\n\n\n\nThe NSL-KDD dataset is a widely used benchmark for intrusion detection research and is designed to address some of the limitations in the original KDD’99 dataset, such as redundant records. It contains 41 engineered network features and multiple attack categories, enabling both binary and multi-class evaluation. For this project, we used the 10% version of the dataset, which provides a representative but computationally manageable subset of the full collection. The data comes from the University of New Brunswick’s Canadian Institute for Cybersecurity repository, which maintains standardized, research-grade network security datasets."
  },
  {
    "objectID": "presentation.html#methodology",
    "href": "presentation.html#methodology",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Methodology",
    "text": "Methodology\n\nStandard pipeline: EDA → preprocessing → modeling → evaluation\nData cleaning, encoding, and scaling\nUnsupervised: Z-score, IQR, Elliptic, Mahalanobis\nAdvanced: Isolation Forest, LOF, Autoencoder\nSupervised: RF, SVM, LogReg\nEvaluated using accuracy, precision, recall, F1, AUC\n\n\n\nOur workflow follows a standard machine learning pipeline beginning with exploratory data analysis, followed by preprocessing steps such as cleaning, encoding, and scaling. We implemented a range of models, starting with statistical and unsupervised anomaly detectors, then moving to more advanced techniques like Isolation Forest, Local Outlier Factor, and Autoencoders. Supervised models, including Random Forest, SVM, and Logistic Regression, were trained using the labeled portions of the dataset. All models were evaluated using consistent metrics—accuracy, precision, recall, F1, and AUC—to ensure a fair comparison across approaches."
  },
  {
    "objectID": "presentation.html#unsupervised-results-statistical",
    "href": "presentation.html#unsupervised-results-statistical",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Unsupervised Results: Statistical",
    "text": "Unsupervised Results: Statistical\n\nZ-score detects extreme deviations\nIQR captures distribution outliers\nElliptic Envelope robust covariance\nMahalanobis multivariate distance\nHigh false-positive variability\nLimited subtle attack detection\n\n\n\nThese traditional statistical methods provide fast and lightweight anomaly detection but have clear limitations. Z-score and IQR rely on simple distribution thresholds, so they mainly detect very large deviations and miss more nuanced behaviors. Elliptic Envelope and Mahalanobis distance incorporate covariance structure, offering a more sophisticated view of normal traffic, but they still struggle with the complexity and variability of real attack patterns. Across all of these methods, we observed high false-positive rates and inconsistent performance, especially when the anomalies were subtle or closely resembled normal traffic. This highlights the need for more advanced models in operational environments."
  },
  {
    "objectID": "presentation.html#unsupervised-results-advanced",
    "href": "presentation.html#unsupervised-results-advanced",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Unsupervised Results: Advanced",
    "text": "Unsupervised Results: Advanced\n\nIsolation Forest strong separation\nLOF local-density anomalies\nAutoencoder learns latent structure\nOutperforms statistical baselines\nLower recall for stealth attacks\nNeeds threshold tuning\n\n\n\nThese advanced unsupervised models deliver noticeably better performance than the simpler statistical methods. Isolation Forest is effective at separating anomalies by isolating rare behaviors, while LOF focuses on local density differences to detect unusual patterns in small regions of the feature space. The Autoencoder, which learns compressed representations of normal traffic, performed the strongest among the unsupervised approaches and was able to capture complex relationships between features. However, all these models still showed limitations, particularly with low-signal or stealthy attacks that closely resemble normal activity. Their performance is also highly sensitive to threshold settings, meaning operational tuning is essential for reliable detection."
  },
  {
    "objectID": "presentation.html#supervised-results-models",
    "href": "presentation.html#supervised-results-models",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Supervised Results: Models",
    "text": "Supervised Results: Models\n\nRandom Forest best performance\nHigh precision and recall\nSVM-RBF competitive results**\nLogistic baseline comparison\nClear attack class separation\nStable across test folds\n\n\n\nSupervised learning clearly provided the strongest results in this evaluation. Random Forest delivered the best overall performance with consistently high precision and recall, making it the most reliable model for distinguishing normal and malicious traffic. The SVM with an RBF kernel also performed competitively, especially on nonlinear patterns, while Logistic Regression served as a baseline for comparison. These models showed clear separation between attack classes and demonstrated stable behavior across multiple test folds, reinforcing their suitability for operational intrusion detection when labeled data is available."
  },
  {
    "objectID": "presentation.html#supervised-results-explainability",
    "href": "presentation.html#supervised-results-explainability",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Supervised Results: Explainability",
    "text": "Supervised Results: Explainability\n\nRandom Forest feature importance clarity\nIdentify top predictive fields\nSVM decision boundary insights\nEnhanced analyst trust\nSupports security auditability\nReduces black-box risk\n\n\n\nA key advantage of the supervised models—especially Random Forest and SVM—is their interpretability. Random Forest provides clear feature-importance rankings, helping analysts understand which network attributes contribute most to attack detection. SVM models offer insights into decision boundaries, clarifying how the classifier separates malicious and normal behavior. These interpretability tools improve analyst trust, support auditing and compliance requirements, and reduce the risks associated with deploying opaque, black-box detection systems. For an operational SOC, this level of transparency is essential for validating alerts and refining detection strategies."
  },
  {
    "objectID": "presentation.html#comparison",
    "href": "presentation.html#comparison",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Comparison",
    "text": "Comparison\n\nSupervised models outperform unsupervised\nUnsupervised useful for unknown attacks\nSupervised highest accuracy\nAutoencoder best unsupervised\nStatistical weakest coverage\nRF strongest overall detector\n\n\n\n\n\n\n\nNote\n\n\nThe combined evaluation shows supervised learning clearly ahead when labels exist, but unsupervised models remain valuable for zero-day threats."
  },
  {
    "objectID": "presentation.html#discussion",
    "href": "presentation.html#discussion",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Discussion",
    "text": "Discussion\nHow effectively can unsupervised anomaly detection identify malicious traffic?\n\nTrained only on normal traffic\nDetects high-variance anomalies\nMisses subtle or stealth attacks\nUseful for unknown threats\nRequires threshold tuning\nModerately effective overall\n\n\n\nUnsupervised anomaly detection methods can identify attacks that significantly deviate from normal behavior, especially high-variance or clearly abnormal patterns. However, they struggle with low-signal attacks that resemble legitimate traffic, leading to missed detections. Their strength lies in detecting unknown or emerging threats where labeled data is unavailable, but their performance is inconsistent without careful threshold calibration. Overall, unsupervised methods are moderately effective but insufficient as a standalone intrusion detection strategy."
  },
  {
    "objectID": "presentation.html#overall-trade-off",
    "href": "presentation.html#overall-trade-off",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Overall Trade-Off",
    "text": "Overall Trade-Off\n\nSupervised: high accuracy, low flexibility\nUnsupervised: high adaptability, lower precision\nStatistical: fast, low insight\nSupervised requires maintenance\nUnsupervised requires threshold tuning\nTrade-off: precision vs generalization\n\n\n\nEach modeling approach comes with strengths and trade-offs. Supervised models deliver high accuracy and strong predictive power, but they depend on labeled data and require ongoing maintenance as network behavior evolves. Unsupervised models are more flexible and adaptable, making them valuable for detecting unfamiliar threats, but they generally produce lower precision and require careful threshold tuning. Statistical methods are fast and simple to deploy, yet they offer limited insight and struggle with complex attack patterns. Ultimately, the decision comes down to balancing precision against generalization, depending on the organization’s operational needs and risk tolerance."
  },
  {
    "objectID": "presentation.html#real-world-implications",
    "href": "presentation.html#real-world-implications",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Real-World Implications",
    "text": "Real-World Implications\n\nManage false positives proactively\nAdjust alert thresholds regularly\nPlan periodic model retraining\nMonitor concept drift indicators\nCombine supervised + unsupervised\nStrengthen SOC decision support\n\n\n\n\n\nThese findings translate into several practical considerations for real-world intrusion detection. First, managing false positives is essential to prevent analyst fatigue and ensure SOC efficiency. Alert thresholds should be reviewed and adjusted regularly, especially for unsupervised models that are sensitive to behavioral shifts. Models must be retrained periodically to stay aligned with evolving network patterns and avoid degradation from concept drift. A hybrid strategy—combining supervised and unsupervised techniques—can offer both strong accuracy and adaptability. Altogether, these practices enhance SOC decision-making and support more resilient and proactive cybersecurity operations."
  },
  {
    "objectID": "presentation.html#introduction-motivation-background-gradientlinear-gradientto-bottom-283b95-17b2c3",
    "href": "presentation.html#introduction-motivation-background-gradientlinear-gradientto-bottom-283b95-17b2c3",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Introduction & Motivation {background-gradient=“linear-gradient(to bottom, #283b95, 17b2c3)”}",
    "text": "Introduction & Motivation {background-gradient=“linear-gradient(to bottom, #283b95, 17b2c3)”}\n\nRising network complexity\nExpanding attack surface\nNeed for adaptive detection\nLimits of signature systems\nValue of behavioral analytics\nGoal: improved threat identification\n\n\n\nNotes:\nExecutives face rising operational risks due to evolving traffic patterns. Signature systems miss new and subtle attacks. . . . This project evaluates anomaly-based and learning-based detection to strengthen SOC capabilities."
  },
  {
    "objectID": "presentation.html#problem-statement-transitionfade-transition-speeddefault",
    "href": "presentation.html#problem-statement-transitionfade-transition-speeddefault",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Problem Statement {transition=“fade” transition-speed=“default”}}",
    "text": "Problem Statement {transition=“fade” transition-speed=“default”}}\n\nQ1: Effectiveness of unsupervised detectors\nQ2: Supervised vs. statistical performance\nFocus: accuracy, precision, recall\nAssess interpretability differences\nEvaluate malicious traffic detection\nIdentify operational trade-offs\n\n\n\n\n\n\n\nNote\n\n\nWe test whether unsupervised models trained on only normal traffic can detect attacks, and how supervised ML compares to statistical anomaly methods across key performance dimensions."
  },
  {
    "objectID": "presentation.html#models-comparison",
    "href": "presentation.html#models-comparison",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Models Comparison",
    "text": "Models Comparison\n\nSupervised models outperform unsupervised\nUnsupervised useful for unknown attacks\nSupervised highest accuracy\nAutoencoder best unsupervised\nStatistical weakest coverage\nRF strongest overall detector\n\n\n\nWhen comparing all modeling approaches, supervised methods clearly performed the best overall, offering the highest accuracy and the most reliable detection across attack categories. Unsupervised models still play an important role, particularly for detecting new or previously unseen attack patterns, but their precision and consistency are lower. Among the unsupervised models, the Autoencoder showed the strongest capability, while simple statistical techniques provided the weakest coverage and struggled with complex behaviors. Random Forest emerged as the strongest individual model in the entire evaluation, combining accuracy, stability, and interpretability."
  },
  {
    "objectID": "presentation.html#metric-table",
    "href": "presentation.html#metric-table",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Metric Table",
    "text": "Metric Table\nModel Performance Summary\n\n\n\n\n\n\n\n\nmodel\n\n\nprecision_macro\n\n\nrecall_macro\n\n\nf1_macro\n\n\nroc_auc_ovr_macro\n\n\npr_auc_ovr_macro\n\n\n\n\n\n\n0\n\n\nRandom Forest (baseline)\n\n\n0.962361\n\n\n0.974293\n\n\n0.967920\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\nRandom Forest + SMOTE\n\n\n0.947867\n\n\n0.975182\n\n\n0.960155\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\nSVM-RBF + SMOTE\n\n\n0.890610\n\n\n0.930098\n\n\n0.905367\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\nSVM-RBF (baseline)\n\n\n0.642394\n\n\n0.722858\n\n\n0.501165\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nZ-Score\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n\n\n5\n\n\nOC-SVM (RBF)\n\n\n0.779636\n\n\n1.000000\n\n\n0.876175\n\n\n0.962972\n\n\n0.779636\n\n\n\n\n6\n\n\nIsolation Forest\n\n\n0.769121\n\n\n0.369132\n\n\n0.498847\n\n\n0.670050\n\n\n0.414882\n\n\n\n\n7\n\n\nDeep Autoencoder\n\n\n0.906161\n\n\n0.046605\n\n\n0.088650\n\n\n0.981710\n\n\n0.878009\n\n\n\n\n\n\n\nThis table compares supervised, statistical, and unsupervised models using macro-level performance metrics."
  },
  {
    "objectID": "presentation.html#how-effectively-can-unsupervised-anomaly-detection-identify-malicious-traffic",
    "href": "presentation.html#how-effectively-can-unsupervised-anomaly-detection-identify-malicious-traffic",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "How effectively can unsupervised anomaly detection identify malicious traffic?",
    "text": "How effectively can unsupervised anomaly detection identify malicious traffic?\n\nTrained only on normal traffic\nDetects high-variance anomalies\nMisses subtle or stealth attacks\nUseful for unknown threats\nRequires threshold tuning\nModerately effective overall\n\n\nUnsupervised anomaly detection methods can identify attacks that significantly deviate from normal behavior, especially high-variance or clearly abnormal patterns. However, they struggle with low-signal attacks that resemble legitimate traffic, leading to missed detections. Their strength lies in detecting unknown or emerging threats where labeled data is unavailable, but their performance is inconsistent without careful threshold calibration. Overall, unsupervised methods are moderately effective but insufficient as a standalone intrusion detection strategy."
  },
  {
    "objectID": "presentation.html#how-do-supervised-ml-models-compare-with-statistical-anomaly-detectors",
    "href": "presentation.html#how-do-supervised-ml-models-compare-with-statistical-anomaly-detectors",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "How do supervised ML models compare with statistical anomaly detectors?",
    "text": "How do supervised ML models compare with statistical anomaly detectors?\n\nSupervised achieve higher accuracy\nStronger precision and recall\nBetter class separation\nStatistical models detect only simple outliers\nSupervised offer clearer interpretability\nMore reliable for operational IDS\n\n\nSupervised machine learning models significantly outperform statistical anomaly detection methods across all major evaluation metrics. They deliver higher accuracy, stronger precision and recall, and more consistent detection of diverse attack types. Statistical methods, such as Z-score, IQR, and Mahalanobis distance, catch only basic or extreme outliers and fail to capture complex attack behaviors. Supervised models also provide clearer interpretability through feature importance and decision boundary analysis, making them far easier for analysts to validate and operationalize. Overall, supervised learning offers far superior intrusion detection performance and reliability in real-world environments."
  },
  {
    "objectID": "presentation.html#discussion-1",
    "href": "presentation.html#discussion-1",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Discussion",
    "text": "Discussion\nHow do supervised ML models compare with statistical anomaly detectors?\n\nSupervised achieve higher accuracy\nStronger precision and recall\nBetter class separation\nStatistical models detect only simple outliers\nSupervised offer clearer interpretability\nMore reliable for operational IDS\n\n\n\nSupervised machine learning models significantly outperform statistical anomaly detection methods across all major evaluation metrics. They deliver higher accuracy, stronger precision and recall, and more consistent detection of diverse attack types. Statistical methods, such as Z-score, IQR, and Mahalanobis distance, catch only basic or extreme outliers and fail to capture complex attack behaviors. Supervised models also provide clearer interpretability through feature importance and decision boundary analysis, making them far easier for analysts to validate and operationalize. Overall, supervised learning offers far superior intrusion detection performance and reliability in real-world environments."
  },
  {
    "objectID": "presentation.html#discussion-2",
    "href": "presentation.html#discussion-2",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Discussion",
    "text": "Discussion\n\nUnsupervised moderately effective\nDetects only high-variance anomalies\nStruggles with low-signal attacks\nSupervised significantly stronger\nHigher precision and recall\nBetter overall interpretability\n\n\n\nOverall, the results show that unsupervised methods are moderately effective but mainly detect anomalies that are large or highly unusual. They tend to miss low-signal or stealthy attacks that blend in with normal traffic. In contrast, supervised models performed significantly better across all key metrics, including precision and recall, and provided clearer interpretability for analysts. This makes them more dependable for operational use when labeled data is available. The comparison highlights that while unsupervised models add value for discovering unknown threats, supervised learning remains the stronger and more reliable foundation for intrusion detection."
  },
  {
    "objectID": "presentation.html#discussion-transitionfade-in-slide-out-transition-speedslow.-scrollable",
    "href": "presentation.html#discussion-transitionfade-in-slide-out-transition-speedslow.-scrollable",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Discussion {transition=“fade-in slide-out” transition-speed=“slow”. scrollable}",
    "text": "Discussion {transition=“fade-in slide-out” transition-speed=“slow”. scrollable}\nHow do supervised ML models compare with statistical anomaly detectors?\n\nSupervised achieve higher accuracy\nStronger precision and recall\nBetter class separation\nStatistical models detect only simple outliers\nSupervised offer clearer interpretability\nMore reliable for operational IDS\n\n\n\nSupervised machine learning models significantly outperform statistical anomaly detection methods across all major evaluation metrics. They deliver higher accuracy, stronger precision and recall, and more consistent detection of diverse attack types. Statistical methods, such as Z-score, IQR, and Mahalanobis distance, catch only basic or extreme outliers and fail to capture complex attack behaviors. Supervised models also provide clearer interpretability through feature importance and decision boundary analysis, making them far easier for analysts to validate and operationalize. Overall, supervised learning offers far superior intrusion detection performance and reliability in real-world environments."
  },
  {
    "objectID": "presentation.html#model-performance-summary",
    "href": "presentation.html#model-performance-summary",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Model Performance Summary",
    "text": "Model Performance Summary\n\n\n\n\n\n\n\n\nmodel\n\n\nprecision_macro\n\n\nrecall_macro\n\n\nf1_macro\n\n\nroc_auc_ovr_macro\n\n\npr_auc_ovr_macro\n\n\n\n\n\n\n0\n\n\nRandom Forest (baseline)\n\n\n0.962361\n\n\n0.974293\n\n\n0.967920\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\nRandom Forest + SMOTE\n\n\n0.947867\n\n\n0.975182\n\n\n0.960155\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\nSVM-RBF + SMOTE\n\n\n0.890610\n\n\n0.930098\n\n\n0.905367\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\nSVM-RBF (baseline)\n\n\n0.642394\n\n\n0.722858\n\n\n0.501165\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nZ-Score\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n1.000000\n\n\n\n\n5\n\n\nOC-SVM (RBF)\n\n\n0.779636\n\n\n1.000000\n\n\n0.876175\n\n\n0.962972\n\n\n0.779636\n\n\n\n\n6\n\n\nIsolation Forest\n\n\n0.769121\n\n\n0.369132\n\n\n0.498847\n\n\n0.670050\n\n\n0.414882\n\n\n\n\n7\n\n\nDeep Autoencoder\n\n\n0.906161\n\n\n0.046605\n\n\n0.088650\n\n\n0.981710\n\n\n0.878009"
  }
]