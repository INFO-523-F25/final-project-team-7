[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "The NSL-KDD dataset is an enhanced version of the KDD Cup 1999 intrusion detection benchmark.  It removes redundant records and provides a more balanced sample distribution for fair model evaluation.  Each record represents a network connection with 41 features describing various characteristics such as protocol type, service, traffic flags, and packet statistics. The target label identifies whether a connection is normal or belongs to one of four attack families:  - DoS (Denial of Service): Overwhelms system resources (e.g., neptune, smurf, teardrop)  - Probe: Scans for vulnerabilities or open ports (e.g., satan, nmap, ipsweep)  - R2L (Remote to Local): Attempts to gain access remotely (e.g., guess_passwd, imap)  - U2R (User to Root): Escalates privileges locally (e.g., rootkit, buffer_overflow) \n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nIn the context of intrusion detection, the dataset encompasses four main families of \nattack behaviors: Denial of Service (DoS), Probe, Remote to Local (R2L),\nand User to Root (U2R): \n\n- normal: 19.69% (n = 97278)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset?  It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "The NSL-KDD dataset is an enhanced version of the KDD Cup 1999 intrusion detection benchmark.  It removes redundant records and provides a more balanced sample distribution for fair model evaluation.  Each record represents a network connection with 41 features describing various characteristics such as protocol type, service, traffic flags, and packet statistics. The target label identifies whether a connection is normal or belongs to one of four attack families:  - DoS (Denial of Service): Overwhelms system resources (e.g., neptune, smurf, teardrop)  - Probe: Scans for vulnerabilities or open ports (e.g., satan, nmap, ipsweep)  - R2L (Remote to Local): Attempts to gain access remotely (e.g., guess_passwd, imap)  - U2R (User to Root): Escalates privileges locally (e.g., rootkit, buffer_overflow) \n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nIn the context of intrusion detection, the dataset encompasses four main families of \nattack behaviors: Denial of Service (DoS), Probe, Remote to Local (R2L),\nand User to Root (U2R): \n\n- normal: 19.69% (n = 97278)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset?  It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Questions",
    "text": "Questions\n\nHow effectively can unsupervised anomaly detection methods identify malicious network traffic when trained only on normal data? \nHow does the performance of supervised machine learning–based intrusion detection compare with statistical anomaly detection methods in terms of detection accuracy, precision, recall, and interpretability?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Analysis plan",
    "text": "Analysis plan\nWeek 1 – Problem Definition & Data Preparation The first week focuses on defining the objectives of anomaly detection within the NSL-KDD dataset. The dataset will be loaded, inspected, and preprocessed to ensure analytical readiness. Tasks include handling missing values, encoding categorical features, and scaling numerical variables. Deliverables: Data preparation notebook and summary report.\nWeek 2 – Exploratory Data Analysis (EDA) This phase aims to understand data distributions, feature correlations, and patterns of normal versus attack traffic. Visualization techniques will be applied to highlight key discriminative features and potential sources of anomaly. Deliverables: EDA notebook with visual insights and descriptive summaries.\nWeek 3 – Statistical Anomaly Detection Methods In this stage, classical statistical methods for anomaly detection will be implemented, including Z-Score, Elliptical Envelope, Local Outlier Factor (LOF), and DBSCAN. Their performance in identifying abnormal network behaviors will be analyzed and compared. Deliverables: Statistical anomaly detection notebook and comparative summary.\nWeek 4 – Machine Learning–Based Methods Modern learning-based detectors will be explored using Isolation Forest, One-Class SVM, and Deep Autoencoders. Models will be tuned for optimal performance, and detection boundaries will be visualized to interpret decision behavior. Deliverables: ML anomaly detection notebook and evaluation metrics.\nWeek 5 – Model Evaluation and Comparison All implemented methods will be assessed using standard performance metrics such as Precision, Recall, F1-score, ROC-AUC, and Detection Rate. Comparative analyses and ablation studies will be performed to evaluate robustness and generalization. Deliverables: Evaluation notebook and performance report.\nWeek 6 – Final Analysis & Reporting The final week synthesizes results into a comprehensive assessment of the effectiveness of different anomaly detection techniques. The findings will discuss practical implications, advantages, and limitations of each approach in real-world intrusion detection scenarios. Additionally, a reflection phase will visualize anomaly scores and latent embeddings (via PCA, t-SNE, UMAP) to assess separability between normal and attack samples. The final report will include a comparative interpretability reflection, discussing how the transparency–accuracy balance affects IDS deployment decisions. Deliverables: Final project report and presentation slides."
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "Although the NSL-KDD dataset is dated and partly synthetic—constructed from simulated network environments rather than real enterprise traffic—it remains one of the most widely used and well-documented benchmarks for intrusion detection research.\nIts design introduces several limitations that will be carefully considered in this study:\n\nTemporal relevance: The dataset reflects network behaviors and attack types common in the late 1990s, before the advent of modern threats such as advanced persistent attacks, encrypted communication channels, and large-scale botnets.\n\nSynthetic generation: The data were produced under controlled lab conditions rather than from live network capture, which means background noise, user variability, and realistic packet timing patterns are underrepresented.\n\nFeature design: Certain attributes (e.g., service, flag) may no longer map directly to contemporary protocols or operating systems, reducing direct applicability to modern network telemetry.\n\nLabel completeness: All attacks are labeled, unlike in real-world network monitoring where labels are often incomplete or delayed.\n\nDespite these constraints, NSL-KDD remains a valuable controlled testbed for benchmarking anomaly and intrusion detection algorithms because:\n\nIt provides clearly defined attack families (DoS, Probe, R2L, U2R) that facilitate comparative and reproducible experiments.\n\nIt allows systematic evaluation of class imbalance, thresholding, and interpretability trade-offs across model types.\n\nIt serves as a pedagogically consistent foundation for developing methods that can later be generalized to newer datasets such as CIC-IDS2017, UNSW-NB15, or proprietary enterprise data.\n\nMitigation strategies include:\n\nApplying robust preprocessing, resampling, and dimensional reduction to reduce dataset biases.\n\nFraming results as algorithmic performance benchmarks, not as direct indicators of deployment-ready accuracy.\n\nReflecting on how each model’s strengths and weaknesses might translate to modern, encrypted, or real-time network environments.\n\n\n\n\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\n\nlike\nthis\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 29 Oct 2025   Prob (F-statistic):           5.84e-08\nTime:                        09:16:38   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#quarto",
    "href": "presentation.html#quarto",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Quarto",
    "text": "Quarto\n\nThe presentation is created using the Quarto CLI\n## sets the start of a new slide"
  },
  {
    "objectID": "presentation.html#layouts",
    "href": "presentation.html#layouts",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Layouts",
    "text": "Layouts\nYou can use plain text\n\n\n\nor bullet points1\n\n\nor in two columns\n\n\nlike\nthis\n\nAnd add footnotes"
  },
  {
    "objectID": "presentation.html#code",
    "href": "presentation.html#code",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Code",
    "text": "Code\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.073\nModel:                            OLS   Adj. R-squared:                  0.070\nMethod:                 Least Squares   F-statistic:                     30.59\nDate:                Wed, 29 Oct 2025   Prob (F-statistic):           5.84e-08\nTime:                        09:16:38   Log-Likelihood:                -1346.4\nNo. Observations:                 392   AIC:                             2697.\nDf Residuals:                     390   BIC:                             2705.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         35.8015      2.266     15.800      0.000      31.347      40.257\nspeed       -354.7055     64.129     -5.531      0.000    -480.788    -228.623\n==============================================================================\nOmnibus:                       27.687   Durbin-Watson:                   0.589\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               18.976\nSkew:                           0.420   Prob(JB):                     7.57e-05\nKurtosis:                       2.323   Cond. No.                         169.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "presentation.html#plot-and-text",
    "href": "presentation.html#plot-and-text",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Plot and text",
    "text": "Plot and text\n\n\n\nSome text\ngoes here"
  },
  {
    "objectID": "presentation.html#tables",
    "href": "presentation.html#tables",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Tables",
    "text": "Tables\nIf you want to generate a table, make sure it is in the HTML format (instead of Markdown or other formats), e.g.,\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\n\n\n\nisland\n\n\n\nbill_length_mm\n\n\n\nbill_depth_mm\n\n\n\nflipper_length_mm\n\n\n\nbody_mass_g\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.1\n\n\n\n18.7\n\n\n\n181.0\n\n\n\n3750.0\n\n\n\nMale\n\n\n\n\n\n\n\n1\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.5\n\n\n\n17.4\n\n\n\n186.0\n\n\n\n3800.0\n\n\n\nFemale\n\n\n\n\n\n\n\n2\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n40.3\n\n\n\n18.0\n\n\n\n195.0\n\n\n\n3250.0\n\n\n\nFemale\n\n\n\n\n\n\n\n4\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n36.7\n\n\n\n19.3\n\n\n\n193.0\n\n\n\n3450.0\n\n\n\nFemale\n\n\n\n\n\n\n\n5\n\n\n\nAdelie\n\n\n\nTorgersen\n\n\n\n39.3\n\n\n\n20.6\n\n\n\n190.0\n\n\n\n3650.0\n\n\n\nMale"
  },
  {
    "objectID": "presentation.html#images",
    "href": "presentation.html#images",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Images",
    "text": "Images\n\nImage credit: Danielle Navarro, Percolate."
  },
  {
    "objectID": "presentation.html#math-expressions",
    "href": "presentation.html#math-expressions",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Math Expressions",
    "text": "Math Expressions\nYou can write LaTeX math expressions inside a pair of dollar signs, e.g. $\\alpha+\\beta$ renders \\(\\alpha + \\beta\\). You can use the display style with double dollar signs:\n$$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$$\n\\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nLimitations:\n\nThe source code of a LaTeX math expression must be in one line, unless it is inside a pair of double dollar signs, in which case the starting $$ must appear in the very beginning of a line, followed immediately by a non-space character, and the ending $$ must be at the end of a line, led by a non-space character;\nThere should not be spaces after the opening $ or before the closing $."
  },
  {
    "objectID": "presentation.html#feeling-adventurous",
    "href": "presentation.html#feeling-adventurous",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Feeling adventurous?",
    "text": "Feeling adventurous?\n\nYou are welcomed to use the default styling of the slides. In fact, that’s what I expect majority of you will do. You will differentiate yourself with the content of your presentation.\nBut some of you might want to play around with slide styling. Some solutions for this can be found at https://quarto.org/docs/presentations/revealjs."
  },
  {
    "objectID": "presentation.html#footnotes",
    "href": "presentation.html#footnotes",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd add footnotes↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was developed by Mehran Tajbakhsh For INFO 523 - Data Mining and Discovery at the University of Arizona, taught by Dr. Greg Chism. The team is comprised of the following team members.\n\nMehran Tajbakhsh: Last-year MSDS student with an MS in Computer Science specializing in Cybersecurity.."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "This project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "",
    "text": "This project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "presentation.html#plots",
    "href": "presentation.html#plots",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "proposal.html#dataset-summary",
    "href": "proposal.html#dataset-summary",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Summary",
    "text": "Dataset Summary\nThe NSL-KDD dataset is an improved successor to KDD Cup 1999, designed to mitigate key issues such as redundant records and extreme class imbalance. It comprises network connection records with 41 features (transport/application protocol, service, flag indicators, traffic statistics, etc.) and a label indicating normal or one of several attack types grouped into four categories: DoS, Probe, R2L, and U2R.\n\n\n\nThis analysis uses the 10% of NSL-KDD corrected file with 494021 rows and 43 columns.\n\nFor security analytics, labels are grouped into families:\n\n- normal: 0.00% (n = 0)\n- DoS: 79.24% (n = 391458)\n- Probe: 0.83% (n = 4107)\n- R2L: 0.23% (n = 1126)\n- U2R: 0.01% (n = 52)\n\n\n\nWhy this dataset? It’s the canonical benchmark for intrusion/anomaly detection and complements NSL-KDD, which addresses redundancy/imbalance issues. Its size, variety of features (categorical + numeric), and labeled attacks make it ideal for comparing statistical (Z-Score, Elliptical Envelope, LOF, DBSCAN) and ML detectors (Isolation Forest, OC-SVM, Deep Autoencoders)."
  },
  {
    "objectID": "proposal.html#methodology-overview",
    "href": "proposal.html#methodology-overview",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Methodology Overview",
    "text": "Methodology Overview\nThis project will implement two complementary detection approaches to compare how well statistical and learning-based methods can detect network intrusions. \n\n\n\n\n\n\n\n\n\nApproach Type\nAlgorithms\nTraining Data\nEvaluation Strategy\n\n\n\n\nUnsupervised Anomaly Detection\nZ-Score, Elliptic Envelope, Local Outlier Factor (LOF), DBSCAN\nTrained only on normal traffic (no attack labels)\nEvaluate anomaly scores using percentile-based thresholding (e.g., top 5% as anomalies); compute precision, recall, and false positive rate against true labels\n\n\nSupervised / Semi-Supervised Intrusion Detection\nRandom Forest, Support Vector Machine (SVM), Isolation Forest, Deep Autoencoder\nTrained on labeled normal and attack traffic\nEvaluate using stratified 80/20 train-test split with 5-fold cross-validation; compare Precision, Recall, F1-score, and ROC-AUC\n\n\n\n\nTable 1. Comparison of Supervised vs. Unsupervised Detection Approaches."
  },
  {
    "objectID": "proposal.html#real-world-application-context",
    "href": "proposal.html#real-world-application-context",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Real-World Application Context",
    "text": "Real-World Application Context\nThis project models an enterprise network intrusion detection system (IDS). In real-world settings, such systems continuously monitor inbound and outbound connections to detect unusual traffic patterns. Findings from this analysis can be directly extended to: - Enterprise network monitoring - Cloud service intrusion prevention - IoT network anomaly detection - Streaming data analysis using similar architectures (e.g., online learning or real-time inference)"
  },
  {
    "objectID": "proposal.html#data-preprocessing-and-feature-handling",
    "href": "proposal.html#data-preprocessing-and-feature-handling",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Data Preprocessing and Feature Handling",
    "text": "Data Preprocessing and Feature Handling\n\nFeature Encoding: Categorical features (protocol_type, service, and flag) will be transformed using One-Hot Encoding to convert them into numerical representations suitable for machine learning models.\nScaling: All numeric features will be standardized using StandardScaler to normalize feature magnitudes and support distance-based models such as SVM, LOF, and DBSCAN.\nDimensionality Reduction: Principal Component Analysis (PCA) will be applied to reduce feature dimensionality, minimize noise, and visualize decision boundaries in lower dimensions.\nLatent-Space Visualization: Beyond PCA, t-SNE and UMAP will be applied to project high-dimensional data into 2D latent spaces. These visualizations will reveal natural clustering between normal and attack traffic, and illustrate how different models separate anomalous connections.\nFeature Selection: Correlation filtering and low-variance thresholding will be used to eliminate redundant and non-informative predictors.\nTrain/Test Split: A stratified 80/20 split will ensure consistent class ratios between training and testing datasets, preserving representation of all attack families.\nCross-Validation: For supervised learning models, 5-fold cross-validation will be applied to ensure stable and generalizable results.\nClass Imbalance Handling: Severe imbalance in R2L and U2R attack classes will be mitigated using SMOTE (Synthetic Minority Oversampling Technique) and class-weight adjustments during model training."
  },
  {
    "objectID": "proposal.html#threshold-definition-for-unsupervised-models",
    "href": "proposal.html#threshold-definition-for-unsupervised-models",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Threshold Definition for Unsupervised Models:",
    "text": "Threshold Definition for Unsupervised Models:\nFor models like LOF and DBSCAN, anomaly scores will be transformed into binary outcomes using a threshold determined by the 95th percentile of the score distribution on training data. Models will be compared based on precision, recall, and false positive rate against true attack labels."
  },
  {
    "objectID": "proposal.html#model-interpretability",
    "href": "proposal.html#model-interpretability",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Model Interpretability",
    "text": "Model Interpretability\nTo ensure transparency and explainability in the results, this project will include interpretability analysis for both supervised and unsupervised models:\n\nTree-Based Models (Random Forest): Feature importance plots will be used to identify the most influential network attributes contributing to intrusion detection decisions.\nSVM and Deep Autoencoders: The SHAP (SHapley Additive exPlanations) framework will be applied to quantify how each input feature affects individual model predictions, highlighting both global and local interpretability.\nUnsupervised Models (LOF, DBSCAN, Elliptic Envelope): Model behavior will be interpreted through visualization of PCA-based decision boundaries, cluster separations, and anomaly score distributions.\nAutoencoder Reconstruction Analysis: For deep learning models, reconstruction error plots will be examined to understand how effectively normal vs. anomalous traffic patterns are captured and separated.\nCombined Insights: Comparing interpretability outputs across models will help evaluate the trade-off between detection accuracy and explainability—an important factor in real-world IDS deployment.\nInterpretability Trade-Offs: Comparative analysis will examine the balance between model transparency and detection power. While deep or ensemble methods may offer superior accuracy, their complexity can obscure decision rationale. Simpler statistical models, though less accurate, provide clearer interpretive insights valuable for security analysts. This trade-off will be explicitly discussed in the results and conclusions."
  },
  {
    "objectID": "proposal.html#evaluation-setup",
    "href": "proposal.html#evaluation-setup",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Evaluation Setup",
    "text": "Evaluation Setup\nThe evaluation process will assess the performance, robustness, and reliability of all models through a consistent and structured framework:\n\nPerformance Metrics: Model effectiveness will be quantified using Precision, Recall, F1-Score, ROC-AUC, and Detection Rate to capture both classification accuracy and anomaly detection strength.\nValidation Strategy: Results will be validated through a combination of stratified 80/20 train–test split and 5-fold cross-validation to ensure generalization and mitigate overfitting.\nFamily-Wise and Rare Attack Evaluation: Each attack family (DoS, Probe, R2L, and U2R) will be evaluated independently, with special attention to rare classes (R2L and U2R). Detection rates, false negatives, and confusion patterns for these low-frequency attacks will reveal how well unsupervised and supervised approaches generalize to scarce attack behaviors.\nThreshold Optimization (Unsupervised Models): Anomaly score thresholds (e.g., top 5% or 95th percentile) will be tuned on the training data to balance false positives and false negatives.\nBaseline Comparison: Model results will be benchmarked against published NSL-KDD baselines and recent IDS studies to contextualize performance improvements.\nVisualization: ROC curves, confusion matrices, and precision–recall plots will be used to visually compare detection quality and identify performance trade-offs among methods."
  },
  {
    "objectID": "proposal.html#dataset-limitations-and-justification",
    "href": "proposal.html#dataset-limitations-and-justification",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and does not reflect modern encrypted or high-volume network traffic, it remains widely used in academic research because:\n\nIt contains well-structured, labeled attack categories, making it ideal for benchmarking and comparing different algorithms.\nIt enables controlled, reproducible comparisons between models without introducing data privacy or confidentiality issues.\nIt provides clear ground truth labels, which are often unavailable in modern or proprietary network traffic datasets.\n\nThe goal is to develop and compare generalizable anomaly detection methods, which can later be adapted to more recent or proprietary datasets."
  },
  {
    "objectID": "proposal.html#reproducibility-and-version-control",
    "href": "proposal.html#reproducibility-and-version-control",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Reproducibility and Version Control",
    "text": "Reproducibility and Version Control\nTo ensure that all results are consistent and reproducible across runs and environments, this project follows strict versioning and organization practices:\n\nRandom Seeds: All random processes (e.g., data splits, model initialization, SMOTE) will use fixed random seeds to ensure reproducibility.\nRepository Organization: The project repository will follow a clear, modular directory structure to separate data, notebooks, and sources.\n\nproject-root/\n│\n├── data/                # Raw and processed datasets\n├── notebooks/           # Jupyter notebooks for analysis and modeling\n├── src/                 # Preprocessing and utility code\n├── requirements.txt     # Python dependencies for environment setup\n└── README.md            # Project overview and usage instructions\n\nExperiment tracking will be implemented using versioned notebooks and GitHub commits, ensuring reproducibility.\nDependencies and environment configuration will be captured in a requirements.txt file.\nAll visualizations, experiment logs, and performance metrics will be versioned and stored in the repository for transparent result verification."
  },
  {
    "objectID": "index.html#abstract-1",
    "href": "index.html#abstract-1",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Abstract",
    "text": "Abstract\nThis project designs a reproducible intrusion-detection framework that evaluates both unsupervised and supervised learning on the NSL-KDD dataset. Because the full corpus contains nearly four million connections, we employ a randomly sampled 10% subset to maintain representative class proportions while enabling efficient experimentation. Unsupervised detectors (Z-Score, Elliptic Envelope, Local Outlier Factor, DBSCAN) are trained using only normal traffic and converted to decisions via percentile-based thresholds (e.g., 95th percentile). Supervised and semi-supervised models (Random Forest, SVM, Isolation Forest, Deep Autoencoder) are trained on labeled traffic.\nPreprocessing includes one-hot encoding for categorical features (protocol_type, service, flag), standardization of numeric variables, and PCA for dimensionality reduction and visualization. Severe class imbalance—especially in R2L and U2R—is addressed with SMOTE oversampling and class-weight adjustments. Evaluation follows a stratified 80/20 train–test split with 5-fold cross-validation, reporting Precision, Recall, F1-Score, ROC-AUC, Detection Rate, and family-wise performance to expose sensitivity across attack categories. Model interpretability leverages feature importance (tree-based models), SHAP explanations (SVM/autoencoders), PCA decision views, and reconstruction-error analyses.\nAll experiments use fixed random seeds, versioned notebooks, and dependency pinning for full reproducibility. While NSL-KDD is dated relative to modern encrypted, high-throughput networks, its labeled structure remains valuable for controlled benchmarking. Outcomes include a comparative assessment of accuracy, efficiency, and explainability, along with practical guidance for deploying anomaly-detection components in enterprise and IoT contexts."
  },
  {
    "objectID": "proposal.html#dataset-limitations-and-justification-1",
    "href": "proposal.html#dataset-limitations-and-justification-1",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and does not reflect modern encrypted or high-volume network traffic, it remains widely used in academic research because:\n\nIt contains well-structured, labeled attack categories, making it ideal for benchmarking and comparing different algorithms.\nIt enables controlled, reproducible comparisons between models without introducing data privacy or confidentiality issues.\nIt provides clear ground truth labels, which are often unavailable in modern or proprietary network traffic datasets.\n\nThe goal is to develop and compare generalizable anomaly detection methods, which can later be adapted to more recent or proprietary datasets."
  },
  {
    "objectID": "presentation.html#dataset-limitations-and-justification",
    "href": "presentation.html#dataset-limitations-and-justification",
    "title": "Anomaly Detection in Network Traffic Using NSL-KDD Dataset",
    "section": "Dataset Limitations and Justification",
    "text": "Dataset Limitations and Justification\nAlthough the NSL-KDD dataset is dated and partly synthetic—constructed from simulated network environments rather than real enterprise traffic—it remains one of the most widely used and well-documented benchmarks for intrusion detection research.\nIts design introduces several limitations that will be carefully considered in this study:\n\nTemporal relevance: The dataset reflects network behaviors and attack types common in the late 1990s, before the advent of modern threats such as advanced persistent attacks, encrypted communication channels, and large-scale botnets.\n\nSynthetic generation: The data were produced under controlled lab conditions rather than from live network capture, which means background noise, user variability, and realistic packet timing patterns are underrepresented.\n\nFeature design: Certain attributes (e.g., service, flag) may no longer map directly to contemporary protocols or operating systems, reducing direct applicability to modern network telemetry.\n\nLabel completeness: All attacks are labeled, unlike in real-world network monitoring where labels are often incomplete or delayed.\n\nDespite these constraints, NSL-KDD remains a valuable controlled testbed for benchmarking anomaly and intrusion detection algorithms because:\n\nIt provides clearly defined attack families (DoS, Probe, R2L, U2R) that facilitate comparative and reproducible experiments.\n\nIt allows systematic evaluation of class imbalance, thresholding, and interpretability trade-offs across model types.\n\nIt serves as a pedagogically consistent foundation for developing methods that can later be generalized to newer datasets such as CIC-IDS2017, UNSW-NB15, or proprietary enterprise data.\n\nMitigation strategies include:\n\nApplying robust preprocessing, resampling, and dimensional reduction to reduce dataset biases.\n\nFraming results as algorithmic performance benchmarks, not as direct indicators of deployment-ready accuracy.\n\nReflecting on how each model’s strengths and weaknesses might translate to modern, encrypted, or real-time network environments."
  }
]